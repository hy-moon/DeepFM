{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys\n",
    "import pydot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from IPython.display import SVG\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import AUC\n",
    "from keras.layers import Lambda,Reshape,Input,Embedding,Dense,Flatten,Activation,concatenate,dot,add,multiply,subtract,Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.layers.advanced_activations import PReLU,ReLU\n",
    "from keras.regularizers import l2 as l2_reg\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('train_merge_data.csv', index_col=0)\n",
    "#test = pd.read_csv('test_merge_data.csv',index_col=0)\n",
    "Train = pd.read_csv('train_job/train.csv')\n",
    "Test = pd.read_csv('test_job.csv')\n",
    "tags = pd.read_csv('train_job/tags.csv')\n",
    "user_tags = pd.read_csv('train_job/user_tags.csv')\n",
    "job_tags = pd.read_csv('train_job/job_tags.csv')\n",
    "company = pd.read_csv('train_job/job_companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([Test,Train.drop(columns='applied')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "company.companySize.fillna('?',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>jobID</th>\n",
       "      <th>applied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fe292163d06253b716e9a0099b42031d</td>\n",
       "      <td>15de21c670ae7c3f6f3f1f37029303c9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6377fa90618fae77571e8dc90d98d409</td>\n",
       "      <td>55b37c5c270e5d84c793e486d798c01d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ec0888a5b04139be0dfe942c7eb4199</td>\n",
       "      <td>0fcbc61acd0479dc77e3cccc0f5ffca7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             userID                             jobID  applied\n",
       "0  fe292163d06253b716e9a0099b42031d  15de21c670ae7c3f6f3f1f37029303c9        0\n",
       "1  6377fa90618fae77571e8dc90d98d409  55b37c5c270e5d84c793e486d798c01d        0\n",
       "2  8ec0888a5b04139be0dfe942c7eb4199  0fcbc61acd0479dc77e3cccc0f5ffca7        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>jobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebaee1af0c501f22ddfe242fc16dae53</td>\n",
       "      <td>352407221afb776e3143e8a1a0577885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ab05403ac7808cbfba3da26665f7a9c</td>\n",
       "      <td>96b9bff013acedfb1d140579e2fbeb63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33349e909eba71677299d2fc97e158b7</td>\n",
       "      <td>58d4d1e7b1e97b258c9ed0b37e02d087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             userID                             jobID\n",
       "0  ebaee1af0c501f22ddfe242fc16dae53  352407221afb776e3143e8a1a0577885\n",
       "1  9ab05403ac7808cbfba3da26665f7a9c  96b9bff013acedfb1d140579e2fbeb63\n",
       "2  33349e909eba71677299d2fc97e158b7  58d4d1e7b1e97b258c9ed0b37e02d087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagID</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>602d1305678a8d5fdb372271e980da6a</td>\n",
       "      <td>Amazon Web Services(AWS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e3251075554389fe91d17a794861d47b</td>\n",
       "      <td>Tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a1d50185e7426cbb0acad1e6ca74b9aa</td>\n",
       "      <td>Docker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tagID                   keyword\n",
       "0  602d1305678a8d5fdb372271e980da6a  Amazon Web Services(AWS)\n",
       "1  e3251075554389fe91d17a794861d47b                Tensorflow\n",
       "2  a1d50185e7426cbb0acad1e6ca74b9aa                    Docker"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>tagID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e576423831e043f7928d9ac113abbe6f</td>\n",
       "      <td>82c2559140b95ccda9c6ca4a8b981f1e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e576423831e043f7928d9ac113abbe6f</td>\n",
       "      <td>2ba8698b79439589fdd2b0f7218d8b07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e576423831e043f7928d9ac113abbe6f</td>\n",
       "      <td>351b33587c5fdd93bd42ef7ac9995a28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             userID                             tagID\n",
       "0  e576423831e043f7928d9ac113abbe6f  82c2559140b95ccda9c6ca4a8b981f1e\n",
       "1  e576423831e043f7928d9ac113abbe6f  2ba8698b79439589fdd2b0f7218d8b07\n",
       "2  e576423831e043f7928d9ac113abbe6f  351b33587c5fdd93bd42ef7ac9995a28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_tags.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobID</th>\n",
       "      <th>tagID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>320722549d1751cf3f247855f937b982</td>\n",
       "      <td>d38901788c533e8286cb6400b40b386d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e744f91c29ec99f0e662c9177946c627</td>\n",
       "      <td>3948ead63a9f2944218de038d8934305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e744f91c29ec99f0e662c9177946c627</td>\n",
       "      <td>0e095e054ee94774d6a496099eb1cf6a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              jobID                             tagID\n",
       "0  320722549d1751cf3f247855f937b982  d38901788c533e8286cb6400b40b386d\n",
       "1  e744f91c29ec99f0e662c9177946c627  3948ead63a9f2944218de038d8934305\n",
       "2  e744f91c29ec99f0e662c9177946c627  0e095e054ee94774d6a496099eb1cf6a"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_tags.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobIDs = data.jobID.unique()\n",
    "userIDs = data.userID.unique()\n",
    "tagIDs = tags.tagID.unique()\n",
    "companyIDs = company.companyID.unique()\n",
    "\n",
    "jobID_encoder = LabelEncoder(); jobID_encoder.fit(jobIDs)\n",
    "userID_encoder = LabelEncoder(); userID_encoder.fit(userIDs)\n",
    "tagID_encoder = LabelEncoder(); tagID_encoder.fit(tagIDs)\n",
    "companyID_encoder = LabelEncoder(); companyID_encoder.fit(companyIDs)\n",
    "companySize_encoder = LabelEncoder()\n",
    "\n",
    "uniq_tagID = tagID_encoder.transform(tagIDs)\n",
    "\n",
    "company.jobID = jobID_encoder.transform(company.jobID)\n",
    "company.companyID = companyID_encoder.transform(company.companyID)\n",
    "company.companySize = companySize_encoder.fit_transform(company.companySize)\n",
    "\n",
    "user_tags.userID = userID_encoder.transform(user_tags.userID)\n",
    "user_tags.tagID = tagID_encoder.transform(user_tags.tagID)\n",
    "\n",
    "job_tags.jobID = jobID_encoder.transform(job_tags.jobID)\n",
    "job_tags.tagID = tagID_encoder.transform(job_tags.tagID)\n",
    "\n",
    "def get_userTags(uid):\n",
    "    return list(user_tags[user_tags.userID==uid].tagID.unique())\n",
    "def get_jobTags(jid):\n",
    "    return list(job_tags[job_tags.jobID==jid].tagID.unique())\n",
    "def get_matchTags(df):\n",
    "    return np.array(list(set(df.userTags)&set(df.jobTags)))\n",
    "def get_matchTagsPer(df):\n",
    "    # user tag중 job tag와 겹치는 tag 비율\n",
    "    return len(df['matchTags'])/len(set(df.userTags))\n",
    "\n",
    "def preprocess(df):\n",
    "    df['userID'] = userID_encoder.transform(df['userID'])\n",
    "    df['jobID'] = jobID_encoder.transform(df['jobID'])\n",
    "    df['userTags'] = df.userID.apply(lambda x:get_userTags(x))\n",
    "    df['jobTags'] = df.jobID.apply(lambda x:get_jobTags(x))\n",
    "    df['matchTags'] = df.apply(get_matchTags,axis=1)\n",
    "    df['matchTagsPer'] = df.apply(get_matchTagsPer,axis=1)\n",
    "    df['anyMatch'] = df['matchTags'].apply(lambda x: 1 if len(x)>0 else 0)\n",
    "    df = df.merge(company, how='left', on='jobID')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "Train = preprocess(Train)\n",
    "Test = preprocess(Test)\n",
    "\n",
    "max_len = 0\n",
    "for i in pd.concat([Train,Test])['matchTags'].values:\n",
    "    max_len = max(len(i),max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user, job 일치하는 tag padding\n",
    "def pad_matchTags(df):\n",
    "    pad = df['matchTags'].values\n",
    "    return sequence.pad_sequences(pad,padding='post')\n",
    "\n",
    "train_matchTags = pad_matchTags(Train)\n",
    "test_matchTags = pad_matchTags(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>jobID</th>\n",
       "      <th>applied</th>\n",
       "      <th>userTags</th>\n",
       "      <th>jobTags</th>\n",
       "      <th>matchTags</th>\n",
       "      <th>matchTagsPer</th>\n",
       "      <th>anyMatch</th>\n",
       "      <th>companyID</th>\n",
       "      <th>companySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>[141, 558, 836, 301, 142, 623, 404, 439, 878, ...</td>\n",
       "      <td>[590, 726]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>[836, 485, 501, 439, 205, 640, 282, 42, 510, 8...</td>\n",
       "      <td>[732, 224, 697]</td>\n",
       "      <td>[697]</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>[855, 595, 724, 308, 204, 271, 132]</td>\n",
       "      <td>[212, 595, 378]</td>\n",
       "      <td>[595]</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>[396, 855, 423, 709, 77, 595, 42, 204, 224, 78...</td>\n",
       "      <td>[42, 780, 439, 709, 366]</td>\n",
       "      <td>[42, 709, 439]</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>[161, 271, 809, 126, 614, 836, 696, 501, 655, ...</td>\n",
       "      <td>[224, 439, 42, 480]</td>\n",
       "      <td>[42]</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>195</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  jobID  applied                                           userTags  \\\n",
       "0     195     52        0  [141, 558, 836, 301, 142, 623, 404, 439, 878, ...   \n",
       "1      64    245        0  [836, 485, 501, 439, 205, 640, 282, 42, 510, 8...   \n",
       "2     111     35        1                [855, 595, 724, 308, 204, 271, 132]   \n",
       "3     190    177        0  [396, 855, 423, 709, 77, 595, 42, 204, 224, 78...   \n",
       "4     159    117        0  [161, 271, 809, 126, 614, 836, 696, 501, 655, ...   \n",
       "\n",
       "                    jobTags       matchTags  matchTagsPer  anyMatch  \\\n",
       "0                [590, 726]              []      0.000000         0   \n",
       "1           [732, 224, 697]           [697]      0.040000         1   \n",
       "2           [212, 595, 378]           [595]      0.142857         1   \n",
       "3  [42, 780, 439, 709, 366]  [42, 709, 439]      0.176471         1   \n",
       "4       [224, 439, 42, 480]            [42]      0.050000         1   \n",
       "\n",
       "   companyID  companySize  \n",
       "0         79            3  \n",
       "1        141            2  \n",
       "2        259            7  \n",
       "3        146            3  \n",
       "4        195            6  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['userID','jobID','companyID','companySize','matchTagsPer','anyMatch']\n",
    "CATEGORY = ['userID','jobID','companyID','anyMatch','companySize']\n",
    "MUL_CATEGORY = ['matchTags']\n",
    "CONTINUE = ['matchTagsPer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Train[COLUMNS]\n",
    "test_data = Test[COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.array(Train['applied'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nd = pd.concat([train_data,test_data])\\nfor col in CONTINUE:\\n    scaler = StandardScaler();\\n    data[col] = scaler.fit_transform(d[[col]])\\n    train_data[col] = scaler.transform(train_data[[col]])\\n    test_data[col] = scaler.transform(test_data[[col]])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Category / Continue columns encoding\n",
    "'''\n",
    "d = pd.concat([train_data,test_data])\n",
    "for col in CONTINUE:\n",
    "    scaler = StandardScaler();\n",
    "    data[col] = scaler.fit_transform(d[[col]])\n",
    "    train_data[col] = scaler.transform(train_data[[col]])\n",
    "    test_data[col] = scaler.transform(test_data[[col]])\n",
    "    \n",
    "feature_num={}\n",
    "for col in CATEGORY:\n",
    "    le = LabelEncoder(); \n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "    feature_num[col] = data[col].max()\n",
    "for col in CONTINUE:\n",
    "    scaler = StandardScaler(); \n",
    "    data[col] = scaler.fit_transform(data[[col]])\n",
    "    train[col] = scaler.transform(train[[col]])\n",
    "    test[col] = scaler.transform(test[[col]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>jobID</th>\n",
       "      <th>companyID</th>\n",
       "      <th>companySize</th>\n",
       "      <th>matchTagsPer</th>\n",
       "      <th>anyMatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195</td>\n",
       "      <td>52</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>245</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>35</td>\n",
       "      <td>259</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>177</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159</td>\n",
       "      <td>117</td>\n",
       "      <td>195</td>\n",
       "      <td>6</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  jobID  companyID  companySize  matchTagsPer  anyMatch\n",
       "0     195     52         79            3      0.000000         0\n",
       "1      64    245        141            2      0.040000         1\n",
       "2     111     35        259            7      0.142857         1\n",
       "3     190    177        146            3      0.176471         1\n",
       "4     159    117        195            6      0.050000         1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [697,   0,   0, ...,   0,   0,   0],\n",
       "       [595,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [697, 836,   0, ...,   0,   0,   0],\n",
       "       [439,   0,   0, ...,   0,   0,   0],\n",
       "       [440,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matchTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>jobID</th>\n",
       "      <th>companyID</th>\n",
       "      <th>companySize</th>\n",
       "      <th>matchTagsPer</th>\n",
       "      <th>anyMatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>157</td>\n",
       "      <td>193</td>\n",
       "      <td>3</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>420</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>252</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  jobID  companyID  companySize  matchTagsPer  anyMatch\n",
       "0     180    157        193            3      0.230769         1\n",
       "1     121    420        168            6      0.083333         1\n",
       "2      29    252        111            3      0.166667         1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[204, 141, 423, ...,   0,   0,   0],\n",
       "       [204,   0,   0, ...,   0,   0,   0],\n",
       "       [ 42,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [202, 836,   0, ...,   0,   0,   0],\n",
       "       [595,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matchTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.to_csv('preprocessed_Train.csv',index=False)\n",
    "Test.to_csv('preprocessed_Test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category = np.array(train_data[CATEGORY])\n",
    "test_category = np.array(test_data[CATEGORY])\n",
    "train_continue = np.array(train_data[CONTINUE])\n",
    "test_continue = np.array(test_data[CONTINUE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dim = {}\n",
    "d = pd.concat([Train,Test])\n",
    "for col in CATEGORY:\n",
    "    dim[col] = len(d[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data의 label값이 없어서, 임의로 train/test set 나눔\n",
    "\n",
    "x_test = Train.sample(frac=0.15,random_state=2021)\n",
    "x_train = Train.drop(x_test.index)\n",
    "\n",
    "train_label = np.array(x_train['applied'])\n",
    "train_category = np.array(x_train[CATEGORY])\n",
    "train_continue = np.array(x_train[CONTINUE])\n",
    "train_matchTags = pad_matchTags(x_train)\n",
    "\n",
    "test_label = np.array(x_test['applied'])\n",
    "test_category = np.array(x_test[CATEGORY])\n",
    "test_continue = np.array(x_test[CONTINUE])\n",
    "test_matchTags = pad_matchTags(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepFM(k=16,deep_dim=256,flag=False):\n",
    "    inputs = []\n",
    "    flattens = []\n",
    "    \n",
    "    #2nd order FM\n",
    "    for col in CATEGORY:\n",
    "        input_ = Input(shape=(1,), name='2nd_input_'+col, dtype='int32')\n",
    "        embed_ = Embedding(dim[col], k, input_length=1,W_regularizer=l2_reg(0.001), name='2nd_embed_'+col)(input_)\n",
    "        flatten_ = Flatten(name='flatten_'+col)(embed_)\n",
    "        inputs.append(input_)\n",
    "        flattens.append(flatten_)\n",
    "        \n",
    "    if flag:\n",
    "        input_ = Input(shape=(max_len,),dtype='int32')\n",
    "        embed_ = Embedding(len(uniq_tagID),k, input_length=max_len)(input_)\n",
    "        flatten_ = Lambda(lambda x: K.mean(x,axis=1))(embed_) # None * max_len * k -> None * k\n",
    "        inputs.append(input_)\n",
    "        flattens.append(flatten_)\n",
    "    \n",
    "    sum_embeds = add(flattens)\n",
    "    sum_square_embeds = multiply([sum_embeds,sum_embeds])\n",
    "    square_embeds = []\n",
    "    \n",
    "    for layer in flattens:\n",
    "        square_embeds.append(multiply([layer,layer]))\n",
    "    square_sum_embeds = add(square_embeds)\n",
    "    subtract_layer = subtract([sum_square_embeds,square_sum_embeds])\n",
    "    second_order = Lambda(lambda x: x*0.5)(subtract_layer)\n",
    "    second_order = Dropout(0.9)(second_order)\n",
    "    \n",
    "    #1st order FM\n",
    "    layers = []\n",
    "    for idx,col in enumerate(CATEGORY):\n",
    "        embed_ = Embedding(dim[col],1,input_length=1,W_regularizer=l2_reg(0.001),name='1st_embed_'+col)(inputs[idx])\n",
    "        flatten_ = Flatten(name='1st_flatten_'+col)(embed_)\n",
    "        layers.append(flatten_)\n",
    "    \n",
    "    first_order = add(layers)\n",
    "    first_odrer = BatchNormalization()(first_order)\n",
    "    first_order = Dropout(0.9)(first_order)\n",
    "    \n",
    "    #deep\n",
    "    continue_input = Input(shape=(len(CONTINUE),),name='input_continue')\n",
    "    inputs.append(continue_input)\n",
    "    continue_dense = Dense(deep_dim, use_bias=False)(continue_input)\n",
    "    \n",
    "    deep = concatenate(flattens+[continue_dense])\n",
    "    deep = ReLU()(deep)\n",
    "    #deep = PReLU()(deep)\n",
    "    deep = Dropout(rate=0.6)(deep)\n",
    "    \n",
    "    deep = Dense(deep_dim)(deep)\n",
    "    deep = ReLU()(deep)\n",
    "    #deep = PReLU()(deep)\n",
    "    deep = Dropout(rate=0.3)(deep)\n",
    "    \n",
    "    deep = Dense(deep_dim)(deep)\n",
    "    deep = ReLU()(deep)\n",
    "    #deep = PReLU()(deep)\n",
    "    deep = Dropout(rate=0.2)(deep)\n",
    "    \n",
    "    concat_input = concatenate([first_order,second_order,deep])\n",
    "    outputs = Dense(1, activation='sigmoid')(concat_input)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = optimizers.SGD(lr=0.01, decay=0, momentum=0.01, nesterov=True)\n",
    "    \n",
    "    loss = 'binary_crossentropy'\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[AUC(name='auc')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(196, 18, input_length=1, name=\"2nd_embed_userID\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(733, 18, input_length=1, name=\"2nd_embed_jobID\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(276, 18, input_length=1, name=\"2nd_embed_companyID\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(2, 18, input_length=1, name=\"2nd_embed_anyMatch\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(8, 18, input_length=1, name=\"2nd_embed_companySize\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(196, 1, input_length=1, name=\"1st_embed_userID\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(733, 1, input_length=1, name=\"1st_embed_jobID\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(276, 1, input_length=1, name=\"1st_embed_companyID\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(2, 1, input_length=1, name=\"1st_embed_anyMatch\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(8, 1, input_length=1, name=\"1st_embed_companySize\", embeddings_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "model = DeepFM(k=18,deep_dim=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "2nd_input_userID (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "2nd_input_jobID (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "2nd_input_companyID (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "2nd_input_anyMatch (InputLayer) (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "2nd_input_companySize (InputLay (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "2nd_embed_userID (Embedding)    (None, 1, 18)        3528        2nd_input_userID[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "2nd_embed_jobID (Embedding)     (None, 1, 18)        13194       2nd_input_jobID[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2nd_embed_companyID (Embedding) (None, 1, 18)        4968        2nd_input_companyID[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "2nd_embed_anyMatch (Embedding)  (None, 1, 18)        36          2nd_input_anyMatch[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "2nd_embed_companySize (Embeddin (None, 1, 18)        144         2nd_input_companySize[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 8, 18)        15966       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_continue (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_userID (Flatten)        (None, 18)           0           2nd_embed_userID[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_jobID (Flatten)         (None, 18)           0           2nd_embed_jobID[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_companyID (Flatten)     (None, 18)           0           2nd_embed_companyID[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_anyMatch (Flatten)      (None, 18)           0           2nd_embed_anyMatch[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_companySize (Flatten)   (None, 18)           0           2nd_embed_companySize[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 18)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 48)           48          input_continue[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 156)          0           flatten_userID[0][0]             \n",
      "                                                                 flatten_jobID[0][0]              \n",
      "                                                                 flatten_companyID[0][0]          \n",
      "                                                                 flatten_anyMatch[0][0]           \n",
      "                                                                 flatten_companySize[0][0]        \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 156)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 156)          0           re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 48)           7536        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 18)           0           flatten_userID[0][0]             \n",
      "                                                                 flatten_jobID[0][0]              \n",
      "                                                                 flatten_companyID[0][0]          \n",
      "                                                                 flatten_anyMatch[0][0]           \n",
      "                                                                 flatten_companySize[0][0]        \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 18)           0           flatten_userID[0][0]             \n",
      "                                                                 flatten_userID[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 18)           0           flatten_jobID[0][0]              \n",
      "                                                                 flatten_jobID[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 18)           0           flatten_companyID[0][0]          \n",
      "                                                                 flatten_companyID[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 18)           0           flatten_anyMatch[0][0]           \n",
      "                                                                 flatten_anyMatch[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 18)           0           flatten_companySize[0][0]        \n",
      "                                                                 flatten_companySize[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 18)           0           lambda_5[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re_lu_6 (ReLU)                  (None, 48)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1st_embed_userID (Embedding)    (None, 1, 1)         196         2nd_input_userID[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "1st_embed_jobID (Embedding)     (None, 1, 1)         733         2nd_input_jobID[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1st_embed_companyID (Embedding) (None, 1, 1)         276         2nd_input_companyID[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "1st_embed_anyMatch (Embedding)  (None, 1, 1)         2           2nd_input_anyMatch[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "1st_embed_companySize (Embeddin (None, 1, 1)         8           2nd_input_companySize[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 18)           0           add_7[0][0]                      \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 18)           0           multiply_16[0][0]                \n",
      "                                                                 multiply_17[0][0]                \n",
      "                                                                 multiply_18[0][0]                \n",
      "                                                                 multiply_19[0][0]                \n",
      "                                                                 multiply_20[0][0]                \n",
      "                                                                 multiply_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 48)           0           re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "1st_flatten_userID (Flatten)    (None, 1)            0           1st_embed_userID[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "1st_flatten_jobID (Flatten)     (None, 1)            0           1st_embed_jobID[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1st_flatten_companyID (Flatten) (None, 1)            0           1st_embed_companyID[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "1st_flatten_anyMatch (Flatten)  (None, 1)            0           1st_embed_anyMatch[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "1st_flatten_companySize (Flatte (None, 1)            0           1st_embed_companySize[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 18)           0           multiply_15[0][0]                \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 48)           2352        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 1)            0           1st_flatten_userID[0][0]         \n",
      "                                                                 1st_flatten_jobID[0][0]          \n",
      "                                                                 1st_flatten_companyID[0][0]      \n",
      "                                                                 1st_flatten_anyMatch[0][0]       \n",
      "                                                                 1st_flatten_companySize[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 18)           0           subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 48)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1)            0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 18)           0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 48)           0           re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 67)           0           dropout_9[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            68          concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 49,055\n",
      "Trainable params: 49,055\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4335 samples, validate on 765 samples\n",
      "Epoch 1/4000\n",
      "4335/4335 [==============================] - ETA: 10s - loss: 0.7088 - auc: 0.45 - ETA: 0s - loss: 0.6972 - auc: 0.5069 - 1s 206us/step - loss: 0.6915 - auc: 0.5023 - val_loss: 0.6701 - val_auc: 0.5436\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67009, saving model to DeepFM.h5\n",
      "Epoch 2/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.6740 - auc: 0.457 - ETA: 0s - loss: 0.6643 - auc: 0.508 - 0s 25us/step - loss: 0.6584 - auc: 0.5079 - val_loss: 0.6377 - val_auc: 0.5377\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67009 to 0.63774, saving model to DeepFM.h5\n",
      "Epoch 3/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.6491 - auc: 0.515 - ETA: 0s - loss: 0.6345 - auc: 0.474 - 0s 25us/step - loss: 0.6286 - auc: 0.4915 - val_loss: 0.6090 - val_auc: 0.5276\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63774 to 0.60904, saving model to DeepFM.h5\n",
      "Epoch 4/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.6078 - auc: 0.488 - ETA: 0s - loss: 0.6038 - auc: 0.506 - 0s 24us/step - loss: 0.6007 - auc: 0.5005 - val_loss: 0.5837 - val_auc: 0.5343\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60904 to 0.58372, saving model to DeepFM.h5\n",
      "Epoch 5/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5760 - auc: 0.449 - ETA: 0s - loss: 0.5797 - auc: 0.505 - 0s 21us/step - loss: 0.5772 - auc: 0.4999 - val_loss: 0.5616 - val_auc: 0.5320\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58372 to 0.56159, saving model to DeepFM.h5\n",
      "Epoch 6/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5673 - auc: 0.467 - ETA: 0s - loss: 0.5582 - auc: 0.504 - 0s 21us/step - loss: 0.5567 - auc: 0.5042 - val_loss: 0.5423 - val_auc: 0.5393\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56159 to 0.54228, saving model to DeepFM.h5\n",
      "Epoch 7/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5490 - auc: 0.430 - ETA: 0s - loss: 0.5398 - auc: 0.518 - 0s 21us/step - loss: 0.5377 - auc: 0.5143 - val_loss: 0.5255 - val_auc: 0.5298\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.54228 to 0.52546, saving model to DeepFM.h5\n",
      "Epoch 8/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5249 - auc: 0.490 - ETA: 0s - loss: 0.5219 - auc: 0.496 - 0s 22us/step - loss: 0.5232 - auc: 0.5007 - val_loss: 0.5110 - val_auc: 0.5191\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.52546 to 0.51098, saving model to DeepFM.h5\n",
      "Epoch 9/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5457 - auc: 0.519 - ETA: 0s - loss: 0.5153 - auc: 0.490 - 0s 21us/step - loss: 0.5097 - auc: 0.5041 - val_loss: 0.4984 - val_auc: 0.5196\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51098 to 0.49843, saving model to DeepFM.h5\n",
      "Epoch 10/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4788 - auc: 0.493 - ETA: 0s - loss: 0.4998 - auc: 0.470 - 0s 22us/step - loss: 0.5007 - auc: 0.4581 - val_loss: 0.4879 - val_auc: 0.5070\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49843 to 0.48791, saving model to DeepFM.h5\n",
      "Epoch 11/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4761 - auc: 0.459 - ETA: 0s - loss: 0.4869 - auc: 0.505 - 0s 22us/step - loss: 0.4885 - auc: 0.4815 - val_loss: 0.4788 - val_auc: 0.5306\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48791 to 0.47878, saving model to DeepFM.h5\n",
      "Epoch 12/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4977 - auc: 0.468 - ETA: 0s - loss: 0.4822 - auc: 0.484 - 0s 22us/step - loss: 0.4817 - auc: 0.4794 - val_loss: 0.4710 - val_auc: 0.5197\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47878 to 0.47096, saving model to DeepFM.h5\n",
      "Epoch 13/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4876 - auc: 0.428 - ETA: 0s - loss: 0.4740 - auc: 0.490 - 0s 25us/step - loss: 0.4733 - auc: 0.4961 - val_loss: 0.4642 - val_auc: 0.5375\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47096 to 0.46421, saving model to DeepFM.h5\n",
      "Epoch 14/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4907 - auc: 0.464 - ETA: 0s - loss: 0.4706 - auc: 0.496 - 0s 23us/step - loss: 0.4673 - auc: 0.5007 - val_loss: 0.4585 - val_auc: 0.5297\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.46421 to 0.45848, saving model to DeepFM.h5\n",
      "Epoch 15/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4814 - auc: 0.440 - ETA: 0s - loss: 0.4637 - auc: 0.491 - 0s 22us/step - loss: 0.4621 - auc: 0.4993 - val_loss: 0.4536 - val_auc: 0.5121\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.45848 to 0.45364, saving model to DeepFM.h5\n",
      "Epoch 16/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4337 - auc: 0.493 - ETA: 0s - loss: 0.4594 - auc: 0.486 - 0s 22us/step - loss: 0.4586 - auc: 0.4878 - val_loss: 0.4496 - val_auc: 0.5067\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.45364 to 0.44958, saving model to DeepFM.h5\n",
      "Epoch 17/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4722 - auc: 0.604 - ETA: 0s - loss: 0.4624 - auc: 0.505 - 0s 23us/step - loss: 0.4527 - auc: 0.5138 - val_loss: 0.4461 - val_auc: 0.5243\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.44958 to 0.44606, saving model to DeepFM.h5\n",
      "Epoch 18/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3880 - auc: 0.484 - ETA: 0s - loss: 0.4500 - auc: 0.477 - 0s 21us/step - loss: 0.4507 - auc: 0.4914 - val_loss: 0.4431 - val_auc: 0.5146\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.44606 to 0.44315, saving model to DeepFM.h5\n",
      "Epoch 19/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3976 - auc: 0.534 - ETA: 0s - loss: 0.4477 - auc: 0.480 - 0s 25us/step - loss: 0.4476 - auc: 0.4965 - val_loss: 0.4407 - val_auc: 0.5206\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.44315 to 0.44070, saving model to DeepFM.h5\n",
      "Epoch 20/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4890 - auc: 0.502 - ETA: 0s - loss: 0.4460 - auc: 0.497 - 0s 24us/step - loss: 0.4454 - auc: 0.5048 - val_loss: 0.4386 - val_auc: 0.5145\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.44070 to 0.43863, saving model to DeepFM.h5\n",
      "Epoch 21/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4105 - auc: 0.468 - ETA: 0s - loss: 0.4411 - auc: 0.518 - 0s 25us/step - loss: 0.4433 - auc: 0.5043 - val_loss: 0.4369 - val_auc: 0.5112\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.43863 to 0.43690, saving model to DeepFM.h5\n",
      "Epoch 22/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4452 - auc: 0.526 - ETA: 0s - loss: 0.4401 - auc: 0.483 - 0s 26us/step - loss: 0.4440 - auc: 0.4827 - val_loss: 0.4355 - val_auc: 0.5190\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.43690 to 0.43548, saving model to DeepFM.h5\n",
      "Epoch 23/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4839 - auc: 0.362 - ETA: 0s - loss: 0.4353 - auc: 0.487 - 0s 27us/step - loss: 0.4406 - auc: 0.5026 - val_loss: 0.4343 - val_auc: 0.5305\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.43548 to 0.43427, saving model to DeepFM.h5\n",
      "Epoch 24/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4619 - auc: 0.529 - ETA: 0s - loss: 0.4461 - auc: 0.504 - 0s 23us/step - loss: 0.4390 - auc: 0.5032 - val_loss: 0.4332 - val_auc: 0.5486\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.43427 to 0.43323, saving model to DeepFM.h5\n",
      "Epoch 25/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4756 - auc: 0.451 - ETA: 0s - loss: 0.4373 - auc: 0.510 - 0s 21us/step - loss: 0.4393 - auc: 0.4937 - val_loss: 0.4324 - val_auc: 0.5403\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.43323 to 0.43240, saving model to DeepFM.h5\n",
      "Epoch 26/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.525 - ETA: 0s - loss: 0.4368 - auc: 0.493 - 0s 23us/step - loss: 0.4383 - auc: 0.4965 - val_loss: 0.4317 - val_auc: 0.5239\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.43240 to 0.43169, saving model to DeepFM.h5\n",
      "Epoch 27/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4633 - auc: 0.450 - ETA: 0s - loss: 0.4340 - auc: 0.484 - 0s 21us/step - loss: 0.4384 - auc: 0.4846 - val_loss: 0.4311 - val_auc: 0.5412\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.43169 to 0.43110, saving model to DeepFM.h5\n",
      "Epoch 28/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4902 - auc: 0.475 - ETA: 0s - loss: 0.4313 - auc: 0.486 - 0s 23us/step - loss: 0.4369 - auc: 0.4962 - val_loss: 0.4306 - val_auc: 0.5350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: val_loss improved from 0.43110 to 0.43059, saving model to DeepFM.h5\n",
      "Epoch 29/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4072 - auc: 0.493 - ETA: 0s - loss: 0.4271 - auc: 0.503 - 0s 21us/step - loss: 0.4369 - auc: 0.4990 - val_loss: 0.4302 - val_auc: 0.5493\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.43059 to 0.43016, saving model to DeepFM.h5\n",
      "Epoch 30/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4356 - auc: 0.573 - ETA: 0s - loss: 0.4285 - auc: 0.513 - 0s 22us/step - loss: 0.4371 - auc: 0.4815 - val_loss: 0.4298 - val_auc: 0.5439\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.43016 to 0.42980, saving model to DeepFM.h5\n",
      "Epoch 31/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4652 - auc: 0.482 - ETA: 0s - loss: 0.4346 - auc: 0.529 - 0s 22us/step - loss: 0.4344 - auc: 0.5131 - val_loss: 0.4295 - val_auc: 0.5279\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.42980 to 0.42946, saving model to DeepFM.h5\n",
      "Epoch 32/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4524 - auc: 0.518 - ETA: 0s - loss: 0.4389 - auc: 0.500 - 0s 22us/step - loss: 0.4355 - auc: 0.5029 - val_loss: 0.4292 - val_auc: 0.5504\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.42946 to 0.42919, saving model to DeepFM.h5\n",
      "Epoch 33/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4023 - auc: 0.475 - ETA: 0s - loss: 0.4393 - auc: 0.501 - 0s 23us/step - loss: 0.4351 - auc: 0.4962 - val_loss: 0.4290 - val_auc: 0.5500\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.42919 to 0.42895, saving model to DeepFM.h5\n",
      "Epoch 34/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4310 - auc: 0.558 - ETA: 0s - loss: 0.4296 - auc: 0.501 - 0s 23us/step - loss: 0.4347 - auc: 0.5035 - val_loss: 0.4288 - val_auc: 0.5080\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.42895 to 0.42876, saving model to DeepFM.h5\n",
      "Epoch 35/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3880 - auc: 0.477 - ETA: 0s - loss: 0.4196 - auc: 0.498 - 0s 23us/step - loss: 0.4342 - auc: 0.4989 - val_loss: 0.4286 - val_auc: 0.5576\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.42876 to 0.42858, saving model to DeepFM.h5\n",
      "Epoch 36/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4060 - auc: 0.479 - ETA: 0s - loss: 0.4478 - auc: 0.506 - 0s 23us/step - loss: 0.4340 - auc: 0.5081 - val_loss: 0.4284 - val_auc: 0.5623\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.42858 to 0.42843, saving model to DeepFM.h5\n",
      "Epoch 37/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5237 - auc: 0.432 - ETA: 0s - loss: 0.4244 - auc: 0.494 - 0s 24us/step - loss: 0.4347 - auc: 0.4937 - val_loss: 0.4283 - val_auc: 0.5732\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.42843 to 0.42830, saving model to DeepFM.h5\n",
      "Epoch 38/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4095 - auc: 0.557 - ETA: 0s - loss: 0.4415 - auc: 0.531 - 0s 23us/step - loss: 0.4308 - auc: 0.5440 - val_loss: 0.4282 - val_auc: 0.5414\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.42830 to 0.42816, saving model to DeepFM.h5\n",
      "Epoch 39/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4252 - auc: 0.546 - ETA: 0s - loss: 0.4367 - auc: 0.505 - 0s 20us/step - loss: 0.4334 - auc: 0.5102 - val_loss: 0.4281 - val_auc: 0.5307\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.42816 to 0.42805, saving model to DeepFM.h5\n",
      "Epoch 40/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4667 - auc: 0.532 - ETA: 0s - loss: 0.4363 - auc: 0.489 - 0s 22us/step - loss: 0.4352 - auc: 0.4926 - val_loss: 0.4280 - val_auc: 0.5560\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.42805 to 0.42797, saving model to DeepFM.h5\n",
      "Epoch 41/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3930 - auc: 0.462 - ETA: 0s - loss: 0.4274 - auc: 0.506 - 0s 22us/step - loss: 0.4339 - auc: 0.4989 - val_loss: 0.4279 - val_auc: 0.5880\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.42797 to 0.42788, saving model to DeepFM.h5\n",
      "Epoch 42/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4599 - auc: 0.572 - ETA: 0s - loss: 0.4349 - auc: 0.505 - 0s 22us/step - loss: 0.4340 - auc: 0.4971 - val_loss: 0.4278 - val_auc: 0.5703\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.42788 to 0.42781, saving model to DeepFM.h5\n",
      "Epoch 43/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4346 - auc: 0.492 - ETA: 0s - loss: 0.4288 - auc: 0.500 - 0s 21us/step - loss: 0.4334 - auc: 0.5017 - val_loss: 0.4277 - val_auc: 0.5693\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.42781 to 0.42774, saving model to DeepFM.h5\n",
      "Epoch 44/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4991 - auc: 0.541 - ETA: 0s - loss: 0.4454 - auc: 0.527 - 0s 23us/step - loss: 0.4330 - auc: 0.5101 - val_loss: 0.4277 - val_auc: 0.5531\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.42774 to 0.42767, saving model to DeepFM.h5\n",
      "Epoch 45/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4144 - auc: 0.507 - ETA: 0s - loss: 0.4209 - auc: 0.539 - 0s 22us/step - loss: 0.4308 - auc: 0.5272 - val_loss: 0.4276 - val_auc: 0.5691\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.42767 to 0.42761, saving model to DeepFM.h5\n",
      "Epoch 46/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4450 - auc: 0.595 - ETA: 0s - loss: 0.4366 - auc: 0.475 - 0s 22us/step - loss: 0.4340 - auc: 0.4947 - val_loss: 0.4275 - val_auc: 0.5741\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.42761 to 0.42755, saving model to DeepFM.h5\n",
      "Epoch 47/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3864 - auc: 0.482 - ETA: 0s - loss: 0.4230 - auc: 0.510 - 0s 22us/step - loss: 0.4317 - auc: 0.5214 - val_loss: 0.4275 - val_auc: 0.5861\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.42755 to 0.42749, saving model to DeepFM.h5\n",
      "Epoch 48/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4785 - auc: 0.471 - ETA: 0s - loss: 0.4399 - auc: 0.507 - 0s 21us/step - loss: 0.4329 - auc: 0.5060 - val_loss: 0.4274 - val_auc: 0.5646\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.42749 to 0.42744, saving model to DeepFM.h5\n",
      "Epoch 49/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4572 - auc: 0.434 - ETA: 0s - loss: 0.4331 - auc: 0.489 - 0s 23us/step - loss: 0.4342 - auc: 0.4949 - val_loss: 0.4274 - val_auc: 0.5569\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.42744 to 0.42740, saving model to DeepFM.h5\n",
      "Epoch 50/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3616 - auc: 0.475 - ETA: 0s - loss: 0.4385 - auc: 0.481 - 0s 22us/step - loss: 0.4347 - auc: 0.4837 - val_loss: 0.4274 - val_auc: 0.5633\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.42740 to 0.42736, saving model to DeepFM.h5\n",
      "Epoch 51/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4093 - auc: 0.406 - ETA: 0s - loss: 0.4460 - auc: 0.511 - 0s 21us/step - loss: 0.4325 - auc: 0.5082 - val_loss: 0.4273 - val_auc: 0.5748\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.42736 to 0.42731, saving model to DeepFM.h5\n",
      "Epoch 52/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.459 - ETA: 0s - loss: 0.4398 - auc: 0.509 - 0s 23us/step - loss: 0.4317 - auc: 0.5201 - val_loss: 0.4273 - val_auc: 0.5660\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.42731 to 0.42726, saving model to DeepFM.h5\n",
      "Epoch 53/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4776 - auc: 0.563 - ETA: 0s - loss: 0.4328 - auc: 0.525 - 0s 22us/step - loss: 0.4318 - auc: 0.5148 - val_loss: 0.4272 - val_auc: 0.5660\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.42726 to 0.42721, saving model to DeepFM.h5\n",
      "Epoch 54/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3856 - auc: 0.522 - ETA: 0s - loss: 0.4256 - auc: 0.501 - 0s 22us/step - loss: 0.4336 - auc: 0.5006 - val_loss: 0.4272 - val_auc: 0.5656\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.42721 to 0.42717, saving model to DeepFM.h5\n",
      "Epoch 55/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4182 - auc: 0.515 - ETA: 0s - loss: 0.4468 - auc: 0.516 - 0s 23us/step - loss: 0.4321 - auc: 0.5109 - val_loss: 0.4271 - val_auc: 0.5717\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.42717 to 0.42713, saving model to DeepFM.h5\n",
      "Epoch 56/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3808 - auc: 0.559 - ETA: 0s - loss: 0.4431 - auc: 0.507 - 0s 24us/step - loss: 0.4311 - auc: 0.5197 - val_loss: 0.4271 - val_auc: 0.5719\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.42713 to 0.42708, saving model to DeepFM.h5\n",
      "Epoch 57/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4738 - auc: 0.459 - ETA: 0s - loss: 0.4323 - auc: 0.486 - 0s 21us/step - loss: 0.4332 - auc: 0.5031 - val_loss: 0.4270 - val_auc: 0.5716\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.42708 to 0.42704, saving model to DeepFM.h5\n",
      "Epoch 58/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5008 - auc: 0.522 - ETA: 0s - loss: 0.4226 - auc: 0.521 - 0s 22us/step - loss: 0.4310 - auc: 0.5191 - val_loss: 0.4270 - val_auc: 0.5908\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.42704 to 0.42700, saving model to DeepFM.h5\n",
      "Epoch 59/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4360 - auc: 0.558 - ETA: 0s - loss: 0.4281 - auc: 0.525 - 0s 23us/step - loss: 0.4307 - auc: 0.5256 - val_loss: 0.4270 - val_auc: 0.5822\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.42700 to 0.42696, saving model to DeepFM.h5\n",
      "Epoch 60/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4338 - auc: 0.510 - ETA: 0s - loss: 0.4294 - auc: 0.513 - 0s 22us/step - loss: 0.4336 - auc: 0.4972 - val_loss: 0.4269 - val_auc: 0.5858\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.42696 to 0.42692, saving model to DeepFM.h5\n",
      "Epoch 61/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4581 - auc: 0.487 - ETA: 0s - loss: 0.4439 - auc: 0.517 - 0s 23us/step - loss: 0.4326 - auc: 0.5061 - val_loss: 0.4269 - val_auc: 0.5937\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.42692 to 0.42688, saving model to DeepFM.h5\n",
      "Epoch 62/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4655 - auc: 0.542 - ETA: 0s - loss: 0.4379 - auc: 0.490 - 0s 22us/step - loss: 0.4338 - auc: 0.4956 - val_loss: 0.4268 - val_auc: 0.5990\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.42688 to 0.42684, saving model to DeepFM.h5\n",
      "Epoch 63/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4464 - auc: 0.443 - ETA: 0s - loss: 0.4451 - auc: 0.477 - 0s 21us/step - loss: 0.4344 - auc: 0.4854 - val_loss: 0.4268 - val_auc: 0.6010\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.42684 to 0.42680, saving model to DeepFM.h5\n",
      "Epoch 64/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4117 - auc: 0.600 - ETA: 0s - loss: 0.4343 - auc: 0.499 - 0s 23us/step - loss: 0.4333 - auc: 0.4964 - val_loss: 0.4268 - val_auc: 0.6023\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.42680 to 0.42676, saving model to DeepFM.h5\n",
      "Epoch 65/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3983 - auc: 0.450 - ETA: 0s - loss: 0.4333 - auc: 0.525 - 0s 21us/step - loss: 0.4319 - auc: 0.5147 - val_loss: 0.4267 - val_auc: 0.6028\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.42676 to 0.42672, saving model to DeepFM.h5\n",
      "Epoch 66/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4727 - auc: 0.535 - ETA: 0s - loss: 0.4340 - auc: 0.510 - 0s 21us/step - loss: 0.4324 - auc: 0.5099 - val_loss: 0.4267 - val_auc: 0.6092\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.42672 to 0.42668, saving model to DeepFM.h5\n",
      "Epoch 67/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4742 - auc: 0.508 - ETA: 0s - loss: 0.4322 - auc: 0.538 - 0s 21us/step - loss: 0.4291 - auc: 0.5435 - val_loss: 0.4266 - val_auc: 0.6167\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.42668 to 0.42664, saving model to DeepFM.h5\n",
      "Epoch 68/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3854 - auc: 0.602 - ETA: 0s - loss: 0.4344 - auc: 0.520 - 0s 23us/step - loss: 0.4312 - auc: 0.5217 - val_loss: 0.4266 - val_auc: 0.6135\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.42664 to 0.42660, saving model to DeepFM.h5\n",
      "Epoch 69/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4198 - auc: 0.505 - ETA: 0s - loss: 0.4353 - auc: 0.487 - 0s 21us/step - loss: 0.4328 - auc: 0.4983 - val_loss: 0.4266 - val_auc: 0.6149\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.42660 to 0.42657, saving model to DeepFM.h5\n",
      "Epoch 70/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3979 - auc: 0.520 - ETA: 0s - loss: 0.4241 - auc: 0.538 - 0s 22us/step - loss: 0.4295 - auc: 0.5383 - val_loss: 0.4265 - val_auc: 0.6240\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.42657 to 0.42652, saving model to DeepFM.h5\n",
      "Epoch 71/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4392 - auc: 0.534 - ETA: 0s - loss: 0.4378 - auc: 0.495 - 0s 22us/step - loss: 0.4327 - auc: 0.4997 - val_loss: 0.4265 - val_auc: 0.6230\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.42652 to 0.42648, saving model to DeepFM.h5\n",
      "Epoch 72/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4024 - auc: 0.475 - ETA: 0s - loss: 0.4257 - auc: 0.517 - 0s 23us/step - loss: 0.4315 - auc: 0.5111 - val_loss: 0.4264 - val_auc: 0.6197\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.42648 to 0.42644, saving model to DeepFM.h5\n",
      "Epoch 73/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.517 - ETA: 0s - loss: 0.4327 - auc: 0.519 - 0s 23us/step - loss: 0.4317 - auc: 0.5116 - val_loss: 0.4264 - val_auc: 0.6150\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.42644 to 0.42640, saving model to DeepFM.h5\n",
      "Epoch 74/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4382 - auc: 0.451 - ETA: 0s - loss: 0.4299 - auc: 0.515 - 0s 23us/step - loss: 0.4316 - auc: 0.5155 - val_loss: 0.4264 - val_auc: 0.6158\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.42640 to 0.42636, saving model to DeepFM.h5\n",
      "Epoch 75/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4488 - auc: 0.562 - ETA: 0s - loss: 0.4394 - auc: 0.532 - 0s 24us/step - loss: 0.4313 - auc: 0.5165 - val_loss: 0.4263 - val_auc: 0.6179\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.42636 to 0.42633, saving model to DeepFM.h5\n",
      "Epoch 76/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4337 - auc: 0.502 - ETA: 0s - loss: 0.4146 - auc: 0.512 - 0s 24us/step - loss: 0.4303 - auc: 0.5224 - val_loss: 0.4263 - val_auc: 0.6261\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.42633 to 0.42628, saving model to DeepFM.h5\n",
      "Epoch 77/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3992 - auc: 0.557 - ETA: 0s - loss: 0.4220 - auc: 0.537 - 0s 25us/step - loss: 0.4293 - auc: 0.5349 - val_loss: 0.4262 - val_auc: 0.6276\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.42628 to 0.42625, saving model to DeepFM.h5\n",
      "Epoch 78/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4236 - auc: 0.515 - ETA: 0s - loss: 0.4295 - auc: 0.531 - 0s 26us/step - loss: 0.4307 - auc: 0.5228 - val_loss: 0.4262 - val_auc: 0.6252\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.42625 to 0.42621, saving model to DeepFM.h5\n",
      "Epoch 79/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4013 - auc: 0.570 - ETA: 0s - loss: 0.4345 - auc: 0.514 - 0s 21us/step - loss: 0.4315 - auc: 0.5135 - val_loss: 0.4262 - val_auc: 0.6312\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.42621 to 0.42617, saving model to DeepFM.h5\n",
      "Epoch 80/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4522 - auc: 0.607 - ETA: 0s - loss: 0.4313 - auc: 0.529 - 0s 22us/step - loss: 0.4303 - auc: 0.5220 - val_loss: 0.4261 - val_auc: 0.6232\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.42617 to 0.42614, saving model to DeepFM.h5\n",
      "Epoch 81/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4040 - auc: 0.529 - ETA: 0s - loss: 0.4220 - auc: 0.545 - 0s 25us/step - loss: 0.4302 - auc: 0.5276 - val_loss: 0.4261 - val_auc: 0.6198\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.42614 to 0.42610, saving model to DeepFM.h5\n",
      "Epoch 82/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4021 - auc: 0.555 - ETA: 0s - loss: 0.4260 - auc: 0.533 - 0s 25us/step - loss: 0.4310 - auc: 0.5174 - val_loss: 0.4261 - val_auc: 0.6258\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.42610 to 0.42606, saving model to DeepFM.h5\n",
      "Epoch 83/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.483 - ETA: 0s - loss: 0.4197 - auc: 0.508 - 0s 25us/step - loss: 0.4315 - auc: 0.5107 - val_loss: 0.4260 - val_auc: 0.6254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss improved from 0.42606 to 0.42603, saving model to DeepFM.h5\n",
      "Epoch 84/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4316 - auc: 0.599 - ETA: 0s - loss: 0.4372 - auc: 0.538 - 0s 22us/step - loss: 0.4302 - auc: 0.5272 - val_loss: 0.4260 - val_auc: 0.6192\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.42603 to 0.42600, saving model to DeepFM.h5\n",
      "Epoch 85/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4408 - auc: 0.596 - ETA: 0s - loss: 0.4365 - auc: 0.531 - 0s 21us/step - loss: 0.4294 - auc: 0.5392 - val_loss: 0.4260 - val_auc: 0.6180\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.42600 to 0.42596, saving model to DeepFM.h5\n",
      "Epoch 86/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4059 - auc: 0.482 - ETA: 0s - loss: 0.4417 - auc: 0.497 - 0s 22us/step - loss: 0.4324 - auc: 0.4989 - val_loss: 0.4259 - val_auc: 0.6159\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.42596 to 0.42594, saving model to DeepFM.h5\n",
      "Epoch 87/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4238 - auc: 0.523 - ETA: 0s - loss: 0.4228 - auc: 0.543 - 0s 23us/step - loss: 0.4283 - auc: 0.5485 - val_loss: 0.4259 - val_auc: 0.6096\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.42594 to 0.42590, saving model to DeepFM.h5\n",
      "Epoch 88/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5030 - auc: 0.439 - ETA: 0s - loss: 0.4208 - auc: 0.501 - 0s 24us/step - loss: 0.4326 - auc: 0.4942 - val_loss: 0.4259 - val_auc: 0.6197\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.42590 to 0.42587, saving model to DeepFM.h5\n",
      "Epoch 89/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4047 - auc: 0.517 - ETA: 0s - loss: 0.4450 - auc: 0.505 - 0s 26us/step - loss: 0.4307 - auc: 0.5193 - val_loss: 0.4258 - val_auc: 0.6159\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.42587 to 0.42583, saving model to DeepFM.h5\n",
      "Epoch 90/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3964 - auc: 0.520 - ETA: 0s - loss: 0.4196 - auc: 0.508 - 0s 24us/step - loss: 0.4309 - auc: 0.5152 - val_loss: 0.4258 - val_auc: 0.6137\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.42583 to 0.42579, saving model to DeepFM.h5\n",
      "Epoch 91/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5009 - auc: 0.449 - ETA: 0s - loss: 0.4375 - auc: 0.506 - 0s 27us/step - loss: 0.4312 - auc: 0.5092 - val_loss: 0.4258 - val_auc: 0.6156\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.42579 to 0.42576, saving model to DeepFM.h5\n",
      "Epoch 92/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4100 - auc: 0.531 - ETA: 0s - loss: 0.4184 - auc: 0.520 - 0s 28us/step - loss: 0.4310 - auc: 0.5193 - val_loss: 0.4257 - val_auc: 0.6160\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.42576 to 0.42573, saving model to DeepFM.h5\n",
      "Epoch 93/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4399 - auc: 0.511 - ETA: 0s - loss: 0.4365 - auc: 0.511 - 0s 26us/step - loss: 0.4318 - auc: 0.5096 - val_loss: 0.4257 - val_auc: 0.6102\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.42573 to 0.42569, saving model to DeepFM.h5\n",
      "Epoch 94/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4083 - auc: 0.475 - ETA: 0s - loss: 0.4316 - auc: 0.519 - 0s 27us/step - loss: 0.4292 - auc: 0.5348 - val_loss: 0.4257 - val_auc: 0.6164\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.42569 to 0.42566, saving model to DeepFM.h5\n",
      "Epoch 95/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4382 - auc: 0.477 - ETA: 0s - loss: 0.4184 - auc: 0.499 - 0s 23us/step - loss: 0.4321 - auc: 0.5047 - val_loss: 0.4256 - val_auc: 0.6105\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.42566 to 0.42563, saving model to DeepFM.h5\n",
      "Epoch 96/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3946 - auc: 0.557 - ETA: 0s - loss: 0.4213 - auc: 0.528 - 0s 24us/step - loss: 0.4312 - auc: 0.5141 - val_loss: 0.4256 - val_auc: 0.6104\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.42563 to 0.42560, saving model to DeepFM.h5\n",
      "Epoch 97/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4063 - auc: 0.487 - ETA: 0s - loss: 0.4318 - auc: 0.507 - 0s 26us/step - loss: 0.4321 - auc: 0.5038 - val_loss: 0.4256 - val_auc: 0.6127\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.42560 to 0.42558, saving model to DeepFM.h5\n",
      "Epoch 98/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4326 - auc: 0.589 - ETA: 0s - loss: 0.4282 - auc: 0.528 - 0s 26us/step - loss: 0.4302 - auc: 0.5244 - val_loss: 0.4255 - val_auc: 0.6144\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.42558 to 0.42555, saving model to DeepFM.h5\n",
      "Epoch 99/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3997 - auc: 0.592 - ETA: 0s - loss: 0.4194 - auc: 0.542 - 0s 25us/step - loss: 0.4296 - auc: 0.5317 - val_loss: 0.4255 - val_auc: 0.6136\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.42555 to 0.42551, saving model to DeepFM.h5\n",
      "Epoch 100/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3373 - auc: 0.524 - ETA: 0s - loss: 0.4267 - auc: 0.538 - 0s 24us/step - loss: 0.4282 - auc: 0.5470 - val_loss: 0.4255 - val_auc: 0.6225\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.42551 to 0.42548, saving model to DeepFM.h5\n",
      "Epoch 101/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4546 - auc: 0.480 - ETA: 0s - loss: 0.4285 - auc: 0.501 - 0s 22us/step - loss: 0.4318 - auc: 0.5046 - val_loss: 0.4254 - val_auc: 0.6144\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.42548 to 0.42544, saving model to DeepFM.h5\n",
      "Epoch 102/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4277 - auc: 0.568 - ETA: 0s - loss: 0.4249 - auc: 0.532 - 0s 22us/step - loss: 0.4298 - auc: 0.5271 - val_loss: 0.4254 - val_auc: 0.6117\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.42544 to 0.42541, saving model to DeepFM.h5\n",
      "Epoch 103/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4847 - auc: 0.552 - ETA: 0s - loss: 0.4311 - auc: 0.510 - 0s 22us/step - loss: 0.4314 - auc: 0.5098 - val_loss: 0.4254 - val_auc: 0.6143\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.42541 to 0.42538, saving model to DeepFM.h5\n",
      "Epoch 104/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4245 - auc: 0.601 - ETA: 0s - loss: 0.4329 - auc: 0.516 - 0s 23us/step - loss: 0.4306 - auc: 0.5186 - val_loss: 0.4253 - val_auc: 0.6182\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.42538 to 0.42535, saving model to DeepFM.h5\n",
      "Epoch 105/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4424 - auc: 0.562 - ETA: 0s - loss: 0.4197 - auc: 0.540 - 0s 25us/step - loss: 0.4301 - auc: 0.5244 - val_loss: 0.4253 - val_auc: 0.6202\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.42535 to 0.42531, saving model to DeepFM.h5\n",
      "Epoch 106/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3897 - auc: 0.532 - ETA: 0s - loss: 0.4212 - auc: 0.541 - 0s 25us/step - loss: 0.4284 - auc: 0.5421 - val_loss: 0.4253 - val_auc: 0.6218\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.42531 to 0.42528, saving model to DeepFM.h5\n",
      "Epoch 107/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4373 - auc: 0.547 - ETA: 0s - loss: 0.4265 - auc: 0.527 - 0s 26us/step - loss: 0.4295 - auc: 0.5322 - val_loss: 0.4252 - val_auc: 0.6229\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.42528 to 0.42525, saving model to DeepFM.h5\n",
      "Epoch 108/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4967 - auc: 0.543 - ETA: 0s - loss: 0.4329 - auc: 0.547 - 0s 22us/step - loss: 0.4288 - auc: 0.5429 - val_loss: 0.4252 - val_auc: 0.6234\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.42525 to 0.42521, saving model to DeepFM.h5\n",
      "Epoch 109/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5196 - auc: 0.545 - ETA: 0s - loss: 0.4251 - auc: 0.522 - 0s 22us/step - loss: 0.4290 - auc: 0.5308 - val_loss: 0.4252 - val_auc: 0.6283\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.42521 to 0.42518, saving model to DeepFM.h5\n",
      "Epoch 110/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4224 - auc: 0.467 - ETA: 0s - loss: 0.4512 - auc: 0.519 - 0s 24us/step - loss: 0.4313 - auc: 0.5108 - val_loss: 0.4251 - val_auc: 0.6259\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.42518 to 0.42514, saving model to DeepFM.h5\n",
      "Epoch 111/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4461 - auc: 0.511 - ETA: 0s - loss: 0.4431 - auc: 0.509 - 0s 23us/step - loss: 0.4308 - auc: 0.5166 - val_loss: 0.4251 - val_auc: 0.6309\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.42514 to 0.42511, saving model to DeepFM.h5\n",
      "Epoch 112/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3974 - auc: 0.522 - ETA: 0s - loss: 0.4285 - auc: 0.533 - 0s 22us/step - loss: 0.4291 - auc: 0.5369 - val_loss: 0.4251 - val_auc: 0.6289\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.42511 to 0.42508, saving model to DeepFM.h5\n",
      "Epoch 113/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3884 - auc: 0.432 - ETA: 0s - loss: 0.4263 - auc: 0.538 - 0s 22us/step - loss: 0.4291 - auc: 0.5335 - val_loss: 0.4250 - val_auc: 0.6377\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.42508 to 0.42504, saving model to DeepFM.h5\n",
      "Epoch 114/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4658 - auc: 0.453 - ETA: 0s - loss: 0.4182 - auc: 0.535 - 0s 23us/step - loss: 0.4303 - auc: 0.5181 - val_loss: 0.4250 - val_auc: 0.6346\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.42504 to 0.42501, saving model to DeepFM.h5\n",
      "Epoch 115/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4203 - auc: 0.498 - ETA: 0s - loss: 0.4266 - auc: 0.525 - 0s 25us/step - loss: 0.4303 - auc: 0.5186 - val_loss: 0.4250 - val_auc: 0.6429\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.42501 to 0.42497, saving model to DeepFM.h5\n",
      "Epoch 116/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3554 - auc: 0.528 - ETA: 0s - loss: 0.4157 - auc: 0.546 - 0s 24us/step - loss: 0.4288 - auc: 0.5400 - val_loss: 0.4249 - val_auc: 0.6419\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.42497 to 0.42493, saving model to DeepFM.h5\n",
      "Epoch 117/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5111 - auc: 0.547 - ETA: 0s - loss: 0.4416 - auc: 0.531 - 0s 25us/step - loss: 0.4305 - auc: 0.5140 - val_loss: 0.4249 - val_auc: 0.6412\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.42493 to 0.42491, saving model to DeepFM.h5\n",
      "Epoch 118/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4788 - auc: 0.528 - ETA: 0s - loss: 0.4239 - auc: 0.520 - 0s 24us/step - loss: 0.4308 - auc: 0.5116 - val_loss: 0.4249 - val_auc: 0.6421\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.42491 to 0.42488, saving model to DeepFM.h5\n",
      "Epoch 119/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4260 - auc: 0.416 - ETA: 0s - loss: 0.4220 - auc: 0.526 - 0s 24us/step - loss: 0.4293 - auc: 0.5308 - val_loss: 0.4248 - val_auc: 0.6442\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.42488 to 0.42484, saving model to DeepFM.h5\n",
      "Epoch 120/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4112 - auc: 0.529 - ETA: 0s - loss: 0.4271 - auc: 0.559 - 0s 22us/step - loss: 0.4292 - auc: 0.5340 - val_loss: 0.4248 - val_auc: 0.6451\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.42484 to 0.42480, saving model to DeepFM.h5\n",
      "Epoch 121/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4935 - auc: 0.498 - ETA: 0s - loss: 0.4388 - auc: 0.519 - 0s 24us/step - loss: 0.4302 - auc: 0.5134 - val_loss: 0.4248 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.42480 to 0.42477, saving model to DeepFM.h5\n",
      "Epoch 122/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4778 - auc: 0.541 - ETA: 0s - loss: 0.4460 - auc: 0.516 - 0s 26us/step - loss: 0.4305 - auc: 0.5183 - val_loss: 0.4247 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.42477 to 0.42473, saving model to DeepFM.h5\n",
      "Epoch 123/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4260 - auc: 0.507 - ETA: 0s - loss: 0.4142 - auc: 0.515 - 0s 26us/step - loss: 0.4300 - auc: 0.5220 - val_loss: 0.4247 - val_auc: 0.6492\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.42473 to 0.42470, saving model to DeepFM.h5\n",
      "Epoch 124/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4689 - auc: 0.468 - ETA: 0s - loss: 0.4257 - auc: 0.523 - 0s 22us/step - loss: 0.4294 - auc: 0.5272 - val_loss: 0.4247 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.42470 to 0.42467, saving model to DeepFM.h5\n",
      "Epoch 125/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4673 - auc: 0.505 - ETA: 0s - loss: 0.4332 - auc: 0.495 - 0s 25us/step - loss: 0.4306 - auc: 0.5138 - val_loss: 0.4246 - val_auc: 0.6496\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.42467 to 0.42463, saving model to DeepFM.h5\n",
      "Epoch 126/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.465 - ETA: 0s - loss: 0.4276 - auc: 0.527 - 0s 22us/step - loss: 0.4298 - auc: 0.5258 - val_loss: 0.4246 - val_auc: 0.6491\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.42463 to 0.42461, saving model to DeepFM.h5\n",
      "Epoch 127/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4462 - auc: 0.429 - ETA: 0s - loss: 0.4272 - auc: 0.520 - 0s 21us/step - loss: 0.4297 - auc: 0.5279 - val_loss: 0.4246 - val_auc: 0.6483\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.42461 to 0.42458, saving model to DeepFM.h5\n",
      "Epoch 128/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3796 - auc: 0.581 - ETA: 0s - loss: 0.4368 - auc: 0.547 - 0s 23us/step - loss: 0.4291 - auc: 0.5323 - val_loss: 0.4245 - val_auc: 0.6508\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.42458 to 0.42454, saving model to DeepFM.h5\n",
      "Epoch 129/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4043 - auc: 0.610 - ETA: 0s - loss: 0.4334 - auc: 0.549 - 0s 23us/step - loss: 0.4291 - auc: 0.5317 - val_loss: 0.4245 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.42454 to 0.42451, saving model to DeepFM.h5\n",
      "Epoch 130/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4887 - auc: 0.542 - ETA: 0s - loss: 0.4352 - auc: 0.520 - 0s 23us/step - loss: 0.4307 - auc: 0.5079 - val_loss: 0.4245 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.42451 to 0.42448, saving model to DeepFM.h5\n",
      "Epoch 131/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4098 - auc: 0.515 - ETA: 0s - loss: 0.4356 - auc: 0.541 - 0s 24us/step - loss: 0.4280 - auc: 0.5417 - val_loss: 0.4244 - val_auc: 0.6537\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.42448 to 0.42444, saving model to DeepFM.h5\n",
      "Epoch 132/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4187 - auc: 0.596 - ETA: 0s - loss: 0.4141 - auc: 0.515 - 0s 26us/step - loss: 0.4302 - auc: 0.5150 - val_loss: 0.4244 - val_auc: 0.6465\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.42444 to 0.42441, saving model to DeepFM.h5\n",
      "Epoch 133/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4353 - auc: 0.569 - ETA: 0s - loss: 0.4298 - auc: 0.539 - 0s 29us/step - loss: 0.4290 - auc: 0.5290 - val_loss: 0.4244 - val_auc: 0.6461\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.42441 to 0.42438, saving model to DeepFM.h5\n",
      "Epoch 134/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3870 - auc: 0.568 - ETA: 0s - loss: 0.4318 - auc: 0.555 - 0s 24us/step - loss: 0.4277 - auc: 0.5468 - val_loss: 0.4243 - val_auc: 0.6440\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.42438 to 0.42434, saving model to DeepFM.h5\n",
      "Epoch 135/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4109 - auc: 0.505 - ETA: 0s - loss: 0.4337 - auc: 0.521 - 0s 26us/step - loss: 0.4291 - auc: 0.5309 - val_loss: 0.4243 - val_auc: 0.6446\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.42434 to 0.42430, saving model to DeepFM.h5\n",
      "Epoch 136/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3825 - auc: 0.536 - ETA: 0s - loss: 0.4222 - auc: 0.547 - 0s 23us/step - loss: 0.4287 - auc: 0.5363 - val_loss: 0.4243 - val_auc: 0.6423\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.42430 to 0.42427, saving model to DeepFM.h5\n",
      "Epoch 137/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4004 - auc: 0.633 - ETA: 0s - loss: 0.4398 - auc: 0.534 - 0s 22us/step - loss: 0.4285 - auc: 0.5317 - val_loss: 0.4242 - val_auc: 0.6482\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.42427 to 0.42424, saving model to DeepFM.h5\n",
      "Epoch 138/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4639 - auc: 0.537 - ETA: 0s - loss: 0.4312 - auc: 0.543 - 0s 22us/step - loss: 0.4264 - auc: 0.5566 - val_loss: 0.4242 - val_auc: 0.6456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00138: val_loss improved from 0.42424 to 0.42421, saving model to DeepFM.h5\n",
      "Epoch 139/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4337 - auc: 0.514 - ETA: 0s - loss: 0.4341 - auc: 0.528 - 0s 21us/step - loss: 0.4290 - auc: 0.5312 - val_loss: 0.4242 - val_auc: 0.6475\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.42421 to 0.42417, saving model to DeepFM.h5\n",
      "Epoch 140/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4631 - auc: 0.551 - ETA: 0s - loss: 0.4319 - auc: 0.532 - 0s 20us/step - loss: 0.4291 - auc: 0.5277 - val_loss: 0.4241 - val_auc: 0.6421\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.42417 to 0.42414, saving model to DeepFM.h5\n",
      "Epoch 141/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3940 - auc: 0.494 - ETA: 0s - loss: 0.4312 - auc: 0.537 - 0s 22us/step - loss: 0.4275 - auc: 0.5452 - val_loss: 0.4241 - val_auc: 0.6446\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.42414 to 0.42411, saving model to DeepFM.h5\n",
      "Epoch 142/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4381 - auc: 0.511 - ETA: 0s - loss: 0.4259 - auc: 0.556 - 0s 22us/step - loss: 0.4279 - auc: 0.5443 - val_loss: 0.4241 - val_auc: 0.6466\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.42411 to 0.42408, saving model to DeepFM.h5\n",
      "Epoch 143/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3599 - auc: 0.552 - ETA: 0s - loss: 0.4314 - auc: 0.549 - 0s 23us/step - loss: 0.4269 - auc: 0.5446 - val_loss: 0.4240 - val_auc: 0.6468\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.42408 to 0.42403, saving model to DeepFM.h5\n",
      "Epoch 144/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4310 - auc: 0.521 - ETA: 0s - loss: 0.4281 - auc: 0.529 - 0s 22us/step - loss: 0.4282 - auc: 0.5385 - val_loss: 0.4240 - val_auc: 0.6463\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.42403 to 0.42400, saving model to DeepFM.h5\n",
      "Epoch 145/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4481 - auc: 0.489 - ETA: 0s - loss: 0.4269 - auc: 0.522 - 0s 22us/step - loss: 0.4287 - auc: 0.5282 - val_loss: 0.4240 - val_auc: 0.6398\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.42400 to 0.42397, saving model to DeepFM.h5\n",
      "Epoch 146/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3766 - auc: 0.538 - ETA: 0s - loss: 0.4323 - auc: 0.523 - 0s 21us/step - loss: 0.4294 - auc: 0.5283 - val_loss: 0.4239 - val_auc: 0.6424\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.42397 to 0.42394, saving model to DeepFM.h5\n",
      "Epoch 147/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4597 - auc: 0.494 - ETA: 0s - loss: 0.4286 - auc: 0.522 - 0s 23us/step - loss: 0.4290 - auc: 0.5256 - val_loss: 0.4239 - val_auc: 0.6412\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.42394 to 0.42391, saving model to DeepFM.h5\n",
      "Epoch 148/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4119 - auc: 0.497 - ETA: 0s - loss: 0.4361 - auc: 0.519 - 0s 22us/step - loss: 0.4288 - auc: 0.5275 - val_loss: 0.4239 - val_auc: 0.6411\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.42391 to 0.42387, saving model to DeepFM.h5\n",
      "Epoch 149/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4116 - auc: 0.546 - ETA: 0s - loss: 0.4247 - auc: 0.569 - 0s 22us/step - loss: 0.4265 - auc: 0.5636 - val_loss: 0.4238 - val_auc: 0.6424\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.42387 to 0.42384, saving model to DeepFM.h5\n",
      "Epoch 150/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3775 - auc: 0.500 - ETA: 0s - loss: 0.4123 - auc: 0.544 - 0s 22us/step - loss: 0.4282 - auc: 0.5341 - val_loss: 0.4238 - val_auc: 0.6454\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.42384 to 0.42381, saving model to DeepFM.h5\n",
      "Epoch 151/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3904 - auc: 0.517 - ETA: 0s - loss: 0.4358 - auc: 0.523 - 0s 23us/step - loss: 0.4287 - auc: 0.5275 - val_loss: 0.4238 - val_auc: 0.6450\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.42381 to 0.42379, saving model to DeepFM.h5\n",
      "Epoch 152/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3923 - auc: 0.488 - ETA: 0s - loss: 0.4200 - auc: 0.533 - 0s 22us/step - loss: 0.4289 - auc: 0.5316 - val_loss: 0.4238 - val_auc: 0.6441\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.42379 to 0.42376, saving model to DeepFM.h5\n",
      "Epoch 153/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4719 - auc: 0.590 - ETA: 0s - loss: 0.4250 - auc: 0.557 - 0s 23us/step - loss: 0.4276 - auc: 0.5411 - val_loss: 0.4237 - val_auc: 0.6420\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.42376 to 0.42372, saving model to DeepFM.h5\n",
      "Epoch 154/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4454 - auc: 0.497 - ETA: 0s - loss: 0.4256 - auc: 0.550 - 0s 22us/step - loss: 0.4272 - auc: 0.5472 - val_loss: 0.4237 - val_auc: 0.6444\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.42372 to 0.42369, saving model to DeepFM.h5\n",
      "Epoch 155/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4716 - auc: 0.512 - ETA: 0s - loss: 0.4399 - auc: 0.531 - 0s 22us/step - loss: 0.4278 - auc: 0.5386 - val_loss: 0.4237 - val_auc: 0.6433\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.42369 to 0.42366, saving model to DeepFM.h5\n",
      "Epoch 156/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4430 - auc: 0.554 - ETA: 0s - loss: 0.4384 - auc: 0.541 - 0s 21us/step - loss: 0.4277 - auc: 0.5417 - val_loss: 0.4236 - val_auc: 0.6443\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.42366 to 0.42362, saving model to DeepFM.h5\n",
      "Epoch 157/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3686 - auc: 0.525 - ETA: 0s - loss: 0.4136 - auc: 0.556 - 0s 22us/step - loss: 0.4262 - auc: 0.5586 - val_loss: 0.4236 - val_auc: 0.6460\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.42362 to 0.42359, saving model to DeepFM.h5\n",
      "Epoch 158/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3907 - auc: 0.491 - ETA: 0s - loss: 0.4145 - auc: 0.536 - 0s 21us/step - loss: 0.4284 - auc: 0.5347 - val_loss: 0.4236 - val_auc: 0.6444\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.42359 to 0.42356, saving model to DeepFM.h5\n",
      "Epoch 159/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3789 - auc: 0.484 - ETA: 0s - loss: 0.4314 - auc: 0.522 - 0s 22us/step - loss: 0.4292 - auc: 0.5284 - val_loss: 0.4235 - val_auc: 0.6430\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.42356 to 0.42353, saving model to DeepFM.h5\n",
      "Epoch 160/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4053 - auc: 0.584 - ETA: 0s - loss: 0.4282 - auc: 0.534 - 0s 23us/step - loss: 0.4285 - auc: 0.5324 - val_loss: 0.4235 - val_auc: 0.6432\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.42353 to 0.42350, saving model to DeepFM.h5\n",
      "Epoch 161/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4428 - auc: 0.556 - ETA: 0s - loss: 0.4372 - auc: 0.536 - 0s 23us/step - loss: 0.4279 - auc: 0.5405 - val_loss: 0.4235 - val_auc: 0.6424\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.42350 to 0.42347, saving model to DeepFM.h5\n",
      "Epoch 162/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3922 - auc: 0.583 - ETA: 0s - loss: 0.4385 - auc: 0.516 - 0s 23us/step - loss: 0.4286 - auc: 0.5286 - val_loss: 0.4234 - val_auc: 0.6440\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.42347 to 0.42343, saving model to DeepFM.h5\n",
      "Epoch 163/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4662 - auc: 0.521 - ETA: 0s - loss: 0.4170 - auc: 0.547 - 0s 21us/step - loss: 0.4267 - auc: 0.5459 - val_loss: 0.4234 - val_auc: 0.6452\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.42343 to 0.42340, saving model to DeepFM.h5\n",
      "Epoch 164/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4794 - auc: 0.443 - ETA: 0s - loss: 0.4331 - auc: 0.552 - 0s 22us/step - loss: 0.4268 - auc: 0.5526 - val_loss: 0.4234 - val_auc: 0.6449\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.42340 to 0.42337, saving model to DeepFM.h5\n",
      "Epoch 165/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3813 - auc: 0.633 - ETA: 0s - loss: 0.4323 - auc: 0.538 - 0s 22us/step - loss: 0.4282 - auc: 0.5344 - val_loss: 0.4233 - val_auc: 0.6450\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.42337 to 0.42335, saving model to DeepFM.h5\n",
      "Epoch 166/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4648 - auc: 0.525 - ETA: 0s - loss: 0.4337 - auc: 0.536 - 0s 22us/step - loss: 0.4282 - auc: 0.5340 - val_loss: 0.4233 - val_auc: 0.6498\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.42335 to 0.42332, saving model to DeepFM.h5\n",
      "Epoch 167/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4127 - auc: 0.515 - ETA: 0s - loss: 0.4303 - auc: 0.553 - 0s 22us/step - loss: 0.4264 - auc: 0.5567 - val_loss: 0.4233 - val_auc: 0.6506\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.42332 to 0.42329, saving model to DeepFM.h5\n",
      "Epoch 168/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4501 - auc: 0.553 - ETA: 0s - loss: 0.4291 - auc: 0.530 - 0s 22us/step - loss: 0.4288 - auc: 0.5302 - val_loss: 0.4233 - val_auc: 0.6466\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.42329 to 0.42326, saving model to DeepFM.h5\n",
      "Epoch 169/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3790 - auc: 0.564 - ETA: 0s - loss: 0.4144 - auc: 0.542 - 0s 22us/step - loss: 0.4271 - auc: 0.5470 - val_loss: 0.4232 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.42326 to 0.42323, saving model to DeepFM.h5\n",
      "Epoch 170/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4800 - auc: 0.576 - ETA: 0s - loss: 0.4342 - auc: 0.537 - 0s 22us/step - loss: 0.4276 - auc: 0.5322 - val_loss: 0.4232 - val_auc: 0.6470\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.42323 to 0.42320, saving model to DeepFM.h5\n",
      "Epoch 171/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4625 - auc: 0.473 - ETA: 0s - loss: 0.4307 - auc: 0.516 - 0s 22us/step - loss: 0.4293 - auc: 0.5277 - val_loss: 0.4232 - val_auc: 0.6468\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.42320 to 0.42318, saving model to DeepFM.h5\n",
      "Epoch 172/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.431 - ETA: 0s - loss: 0.4263 - auc: 0.518 - 0s 22us/step - loss: 0.4293 - auc: 0.5214 - val_loss: 0.4231 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.42318 to 0.42315, saving model to DeepFM.h5\n",
      "Epoch 173/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4701 - auc: 0.394 - ETA: 0s - loss: 0.4316 - auc: 0.545 - 0s 24us/step - loss: 0.4270 - auc: 0.5435 - val_loss: 0.4231 - val_auc: 0.6476\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.42315 to 0.42312, saving model to DeepFM.h5\n",
      "Epoch 174/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4286 - auc: 0.531 - ETA: 0s - loss: 0.4269 - auc: 0.534 - 0s 23us/step - loss: 0.4279 - auc: 0.5397 - val_loss: 0.4231 - val_auc: 0.6493\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.42312 to 0.42309, saving model to DeepFM.h5\n",
      "Epoch 175/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4887 - auc: 0.501 - ETA: 0s - loss: 0.4331 - auc: 0.540 - 0s 21us/step - loss: 0.4274 - auc: 0.5438 - val_loss: 0.4231 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.42309 to 0.42306, saving model to DeepFM.h5\n",
      "Epoch 176/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4541 - auc: 0.488 - ETA: 0s - loss: 0.4353 - auc: 0.535 - 0s 21us/step - loss: 0.4279 - auc: 0.5397 - val_loss: 0.4230 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.42306 to 0.42303, saving model to DeepFM.h5\n",
      "Epoch 177/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4607 - auc: 0.558 - ETA: 0s - loss: 0.4312 - auc: 0.547 - 0s 22us/step - loss: 0.4266 - auc: 0.5442 - val_loss: 0.4230 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.42303 to 0.42300, saving model to DeepFM.h5\n",
      "Epoch 178/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4745 - auc: 0.521 - ETA: 0s - loss: 0.4322 - auc: 0.547 - 0s 22us/step - loss: 0.4255 - auc: 0.5594 - val_loss: 0.4230 - val_auc: 0.6488\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.42300 to 0.42297, saving model to DeepFM.h5\n",
      "Epoch 179/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4171 - auc: 0.596 - ETA: 0s - loss: 0.4251 - auc: 0.550 - 0s 22us/step - loss: 0.4268 - auc: 0.5481 - val_loss: 0.4229 - val_auc: 0.6492\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.42297 to 0.42294, saving model to DeepFM.h5\n",
      "Epoch 180/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3950 - auc: 0.534 - ETA: 0s - loss: 0.4233 - auc: 0.548 - 0s 21us/step - loss: 0.4276 - auc: 0.5417 - val_loss: 0.4229 - val_auc: 0.6484\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.42294 to 0.42291, saving model to DeepFM.h5\n",
      "Epoch 181/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4001 - auc: 0.644 - ETA: 0s - loss: 0.4299 - auc: 0.542 - 0s 22us/step - loss: 0.4270 - auc: 0.5446 - val_loss: 0.4229 - val_auc: 0.6501\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.42291 to 0.42289, saving model to DeepFM.h5\n",
      "Epoch 182/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4329 - auc: 0.569 - ETA: 0s - loss: 0.4286 - auc: 0.538 - 0s 23us/step - loss: 0.4269 - auc: 0.5527 - val_loss: 0.4229 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.42289 to 0.42286, saving model to DeepFM.h5\n",
      "Epoch 183/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3859 - auc: 0.557 - ETA: 0s - loss: 0.4195 - auc: 0.535 - 0s 23us/step - loss: 0.4263 - auc: 0.5530 - val_loss: 0.4228 - val_auc: 0.6521\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.42286 to 0.42283, saving model to DeepFM.h5\n",
      "Epoch 184/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4527 - auc: 0.583 - ETA: 0s - loss: 0.4228 - auc: 0.554 - 0s 22us/step - loss: 0.4274 - auc: 0.5411 - val_loss: 0.4228 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.42283 to 0.42280, saving model to DeepFM.h5\n",
      "Epoch 185/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4261 - auc: 0.568 - ETA: 0s - loss: 0.4283 - auc: 0.548 - 0s 22us/step - loss: 0.4267 - auc: 0.5458 - val_loss: 0.4228 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.42280 to 0.42277, saving model to DeepFM.h5\n",
      "Epoch 186/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4304 - auc: 0.490 - ETA: 0s - loss: 0.4298 - auc: 0.544 - 0s 22us/step - loss: 0.4272 - auc: 0.5419 - val_loss: 0.4227 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.42277 to 0.42274, saving model to DeepFM.h5\n",
      "Epoch 187/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4330 - auc: 0.634 - ETA: 0s - loss: 0.4157 - auc: 0.548 - 0s 23us/step - loss: 0.4269 - auc: 0.5456 - val_loss: 0.4227 - val_auc: 0.6498\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.42274 to 0.42271, saving model to DeepFM.h5\n",
      "Epoch 188/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3918 - auc: 0.589 - ETA: 0s - loss: 0.4392 - auc: 0.544 - 0s 23us/step - loss: 0.4284 - auc: 0.5306 - val_loss: 0.4227 - val_auc: 0.6489\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.42271 to 0.42268, saving model to DeepFM.h5\n",
      "Epoch 189/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3790 - auc: 0.568 - ETA: 0s - loss: 0.4254 - auc: 0.550 - 0s 23us/step - loss: 0.4273 - auc: 0.5401 - val_loss: 0.4227 - val_auc: 0.6520\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.42268 to 0.42265, saving model to DeepFM.h5\n",
      "Epoch 190/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4338 - auc: 0.566 - ETA: 0s - loss: 0.4369 - auc: 0.524 - 0s 23us/step - loss: 0.4278 - auc: 0.5361 - val_loss: 0.4226 - val_auc: 0.6503\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.42265 to 0.42263, saving model to DeepFM.h5\n",
      "Epoch 191/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4615 - auc: 0.476 - ETA: 0s - loss: 0.4320 - auc: 0.565 - 0s 23us/step - loss: 0.4257 - auc: 0.5584 - val_loss: 0.4226 - val_auc: 0.6472\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.42263 to 0.42260, saving model to DeepFM.h5\n",
      "Epoch 192/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4585 - auc: 0.515 - ETA: 0s - loss: 0.4232 - auc: 0.531 - 0s 23us/step - loss: 0.4272 - auc: 0.5476 - val_loss: 0.4226 - val_auc: 0.6476\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.42260 to 0.42257, saving model to DeepFM.h5\n",
      "Epoch 193/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4018 - auc: 0.536 - ETA: 0s - loss: 0.4060 - auc: 0.543 - 0s 25us/step - loss: 0.4272 - auc: 0.5441 - val_loss: 0.4225 - val_auc: 0.6473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00193: val_loss improved from 0.42257 to 0.42254, saving model to DeepFM.h5\n",
      "Epoch 194/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4217 - auc: 0.542 - ETA: 0s - loss: 0.4298 - auc: 0.549 - 0s 22us/step - loss: 0.4260 - auc: 0.5531 - val_loss: 0.4225 - val_auc: 0.6504\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.42254 to 0.42251, saving model to DeepFM.h5\n",
      "Epoch 195/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3939 - auc: 0.566 - ETA: 0s - loss: 0.4133 - auc: 0.559 - 0s 23us/step - loss: 0.4270 - auc: 0.5491 - val_loss: 0.4225 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.42251 to 0.42248, saving model to DeepFM.h5\n",
      "Epoch 196/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3928 - auc: 0.502 - ETA: 0s - loss: 0.4295 - auc: 0.509 - 0s 23us/step - loss: 0.4284 - auc: 0.5300 - val_loss: 0.4225 - val_auc: 0.6523\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.42248 to 0.42245, saving model to DeepFM.h5\n",
      "Epoch 197/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4228 - auc: 0.613 - ETA: 0s - loss: 0.4298 - auc: 0.559 - 0s 22us/step - loss: 0.4260 - auc: 0.5570 - val_loss: 0.4224 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.42245 to 0.42243, saving model to DeepFM.h5\n",
      "Epoch 198/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3782 - auc: 0.538 - ETA: 0s - loss: 0.4271 - auc: 0.550 - 0s 22us/step - loss: 0.4280 - auc: 0.5365 - val_loss: 0.4224 - val_auc: 0.6514\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.42243 to 0.42240, saving model to DeepFM.h5\n",
      "Epoch 199/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3868 - auc: 0.543 - ETA: 0s - loss: 0.4249 - auc: 0.564 - 0s 22us/step - loss: 0.4257 - auc: 0.5620 - val_loss: 0.4224 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.42240 to 0.42238, saving model to DeepFM.h5\n",
      "Epoch 200/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4986 - auc: 0.593 - ETA: 0s - loss: 0.4257 - auc: 0.551 - 0s 23us/step - loss: 0.4267 - auc: 0.5503 - val_loss: 0.4223 - val_auc: 0.6535\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.42238 to 0.42235, saving model to DeepFM.h5\n",
      "Epoch 201/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4347 - auc: 0.626 - ETA: 0s - loss: 0.4224 - auc: 0.578 - 0s 23us/step - loss: 0.4249 - auc: 0.5678 - val_loss: 0.4223 - val_auc: 0.6558\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.42235 to 0.42232, saving model to DeepFM.h5\n",
      "Epoch 202/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.562 - ETA: 0s - loss: 0.4304 - auc: 0.555 - 0s 21us/step - loss: 0.4263 - auc: 0.5441 - val_loss: 0.4223 - val_auc: 0.6539\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.42232 to 0.42229, saving model to DeepFM.h5\n",
      "Epoch 203/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4158 - auc: 0.524 - ETA: 0s - loss: 0.4269 - auc: 0.538 - 0s 21us/step - loss: 0.4272 - auc: 0.5413 - val_loss: 0.4223 - val_auc: 0.6535\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.42229 to 0.42226, saving model to DeepFM.h5\n",
      "Epoch 204/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4674 - auc: 0.585 - ETA: 0s - loss: 0.4274 - auc: 0.565 - 0s 24us/step - loss: 0.4261 - auc: 0.5539 - val_loss: 0.4222 - val_auc: 0.6543\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.42226 to 0.42223, saving model to DeepFM.h5\n",
      "Epoch 205/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4273 - auc: 0.473 - ETA: 0s - loss: 0.4256 - auc: 0.522 - 0s 25us/step - loss: 0.4291 - auc: 0.5237 - val_loss: 0.4222 - val_auc: 0.6521\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.42223 to 0.42220, saving model to DeepFM.h5\n",
      "Epoch 206/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3643 - auc: 0.533 - ETA: 0s - loss: 0.4238 - auc: 0.549 - 0s 24us/step - loss: 0.4257 - auc: 0.5555 - val_loss: 0.4222 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.42220 to 0.42216, saving model to DeepFM.h5\n",
      "Epoch 207/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4430 - auc: 0.549 - ETA: 0s - loss: 0.4236 - auc: 0.564 - 0s 25us/step - loss: 0.4262 - auc: 0.5510 - val_loss: 0.4221 - val_auc: 0.6527\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.42216 to 0.42214, saving model to DeepFM.h5\n",
      "Epoch 208/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3975 - auc: 0.489 - ETA: 0s - loss: 0.4296 - auc: 0.557 - 0s 26us/step - loss: 0.4265 - auc: 0.5435 - val_loss: 0.4221 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.42214 to 0.42211, saving model to DeepFM.h5\n",
      "Epoch 209/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4816 - auc: 0.524 - ETA: 0s - loss: 0.4285 - auc: 0.537 - 0s 25us/step - loss: 0.4276 - auc: 0.5364 - val_loss: 0.4221 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.42211 to 0.42209, saving model to DeepFM.h5\n",
      "Epoch 210/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4665 - auc: 0.565 - ETA: 0s - loss: 0.4161 - auc: 0.534 - 0s 25us/step - loss: 0.4259 - auc: 0.5498 - val_loss: 0.4221 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.42209 to 0.42206, saving model to DeepFM.h5\n",
      "Epoch 211/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4382 - auc: 0.508 - ETA: 0s - loss: 0.4125 - auc: 0.554 - 0s 24us/step - loss: 0.4258 - auc: 0.5564 - val_loss: 0.4220 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.42206 to 0.42202, saving model to DeepFM.h5\n",
      "Epoch 212/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3876 - auc: 0.438 - ETA: 0s - loss: 0.4223 - auc: 0.535 - 0s 28us/step - loss: 0.4264 - auc: 0.5465 - val_loss: 0.4220 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.42202 to 0.42199, saving model to DeepFM.h5\n",
      "Epoch 213/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4905 - auc: 0.534 - ETA: 0s - loss: 0.4277 - auc: 0.566 - 0s 24us/step - loss: 0.4253 - auc: 0.5596 - val_loss: 0.4220 - val_auc: 0.6469\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.42199 to 0.42196, saving model to DeepFM.h5\n",
      "Epoch 214/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3748 - auc: 0.503 - ETA: 0s - loss: 0.4135 - auc: 0.556 - 0s 23us/step - loss: 0.4253 - auc: 0.5588 - val_loss: 0.4219 - val_auc: 0.6483\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.42196 to 0.42192, saving model to DeepFM.h5\n",
      "Epoch 215/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4497 - auc: 0.532 - ETA: 0s - loss: 0.4121 - auc: 0.565 - 0s 21us/step - loss: 0.4260 - auc: 0.5515 - val_loss: 0.4219 - val_auc: 0.6489\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.42192 to 0.42189, saving model to DeepFM.h5\n",
      "Epoch 216/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3868 - auc: 0.550 - ETA: 0s - loss: 0.4218 - auc: 0.555 - 0s 22us/step - loss: 0.4265 - auc: 0.5508 - val_loss: 0.4219 - val_auc: 0.6487\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.42189 to 0.42186, saving model to DeepFM.h5\n",
      "Epoch 217/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4753 - auc: 0.478 - ETA: 0s - loss: 0.4394 - auc: 0.512 - 0s 21us/step - loss: 0.4290 - auc: 0.5192 - val_loss: 0.4218 - val_auc: 0.6457\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.42186 to 0.42183, saving model to DeepFM.h5\n",
      "Epoch 218/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4496 - auc: 0.526 - ETA: 0s - loss: 0.4291 - auc: 0.546 - 0s 22us/step - loss: 0.4270 - auc: 0.5403 - val_loss: 0.4218 - val_auc: 0.6458\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.42183 to 0.42180, saving model to DeepFM.h5\n",
      "Epoch 219/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4267 - auc: 0.575 - ETA: 0s - loss: 0.4302 - auc: 0.559 - 0s 22us/step - loss: 0.4266 - auc: 0.5481 - val_loss: 0.4218 - val_auc: 0.6454\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.42180 to 0.42177, saving model to DeepFM.h5\n",
      "Epoch 220/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4011 - auc: 0.527 - ETA: 0s - loss: 0.4269 - auc: 0.536 - 0s 22us/step - loss: 0.4271 - auc: 0.5357 - val_loss: 0.4217 - val_auc: 0.6482\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.42177 to 0.42175, saving model to DeepFM.h5\n",
      "Epoch 221/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3617 - auc: 0.539 - ETA: 0s - loss: 0.4135 - auc: 0.563 - 0s 22us/step - loss: 0.4260 - auc: 0.5493 - val_loss: 0.4217 - val_auc: 0.6473\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.42175 to 0.42172, saving model to DeepFM.h5\n",
      "Epoch 222/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4409 - auc: 0.548 - ETA: 0s - loss: 0.4233 - auc: 0.568 - 0s 22us/step - loss: 0.4256 - auc: 0.5626 - val_loss: 0.4217 - val_auc: 0.6471\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.42172 to 0.42169, saving model to DeepFM.h5\n",
      "Epoch 223/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3988 - auc: 0.565 - ETA: 0s - loss: 0.4256 - auc: 0.556 - 0s 23us/step - loss: 0.4260 - auc: 0.5481 - val_loss: 0.4217 - val_auc: 0.6474\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.42169 to 0.42166, saving model to DeepFM.h5\n",
      "Epoch 224/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4511 - auc: 0.586 - ETA: 0s - loss: 0.4180 - auc: 0.550 - 0s 23us/step - loss: 0.4265 - auc: 0.5437 - val_loss: 0.4216 - val_auc: 0.6445\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.42166 to 0.42164, saving model to DeepFM.h5\n",
      "Epoch 225/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4175 - auc: 0.612 - ETA: 0s - loss: 0.4208 - auc: 0.560 - 0s 22us/step - loss: 0.4251 - auc: 0.5605 - val_loss: 0.4216 - val_auc: 0.6451\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.42164 to 0.42160, saving model to DeepFM.h5\n",
      "Epoch 226/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4267 - auc: 0.558 - ETA: 0s - loss: 0.4261 - auc: 0.560 - 0s 23us/step - loss: 0.4245 - auc: 0.5707 - val_loss: 0.4216 - val_auc: 0.6453\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.42160 to 0.42157, saving model to DeepFM.h5\n",
      "Epoch 227/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3923 - auc: 0.475 - ETA: 0s - loss: 0.4215 - auc: 0.569 - 0s 23us/step - loss: 0.4252 - auc: 0.5606 - val_loss: 0.4215 - val_auc: 0.6462\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.42157 to 0.42154, saving model to DeepFM.h5\n",
      "Epoch 228/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4276 - auc: 0.535 - ETA: 0s - loss: 0.4193 - auc: 0.575 - 0s 22us/step - loss: 0.4239 - auc: 0.5684 - val_loss: 0.4215 - val_auc: 0.6461\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.42154 to 0.42151, saving model to DeepFM.h5\n",
      "Epoch 229/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3546 - auc: 0.522 - ETA: 0s - loss: 0.4273 - auc: 0.548 - 0s 22us/step - loss: 0.4263 - auc: 0.5463 - val_loss: 0.4215 - val_auc: 0.6490\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.42151 to 0.42148, saving model to DeepFM.h5\n",
      "Epoch 230/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4156 - auc: 0.672 - ETA: 0s - loss: 0.4268 - auc: 0.559 - 0s 21us/step - loss: 0.4247 - auc: 0.5672 - val_loss: 0.4214 - val_auc: 0.6462\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.42148 to 0.42145, saving model to DeepFM.h5\n",
      "Epoch 231/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4297 - auc: 0.518 - ETA: 0s - loss: 0.4232 - auc: 0.558 - 0s 23us/step - loss: 0.4255 - auc: 0.5580 - val_loss: 0.4214 - val_auc: 0.6470\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.42145 to 0.42142, saving model to DeepFM.h5\n",
      "Epoch 232/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3929 - auc: 0.573 - ETA: 0s - loss: 0.4281 - auc: 0.589 - 0s 24us/step - loss: 0.4241 - auc: 0.5704 - val_loss: 0.4214 - val_auc: 0.6476\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.42142 to 0.42139, saving model to DeepFM.h5\n",
      "Epoch 233/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3469 - auc: 0.546 - ETA: 0s - loss: 0.4239 - auc: 0.551 - 0s 22us/step - loss: 0.4257 - auc: 0.5508 - val_loss: 0.4214 - val_auc: 0.6471\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.42139 to 0.42136, saving model to DeepFM.h5\n",
      "Epoch 234/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3809 - auc: 0.635 - ETA: 0s - loss: 0.4169 - auc: 0.569 - 0s 21us/step - loss: 0.4246 - auc: 0.5662 - val_loss: 0.4213 - val_auc: 0.6472\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.42136 to 0.42133, saving model to DeepFM.h5\n",
      "Epoch 235/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4705 - auc: 0.535 - ETA: 0s - loss: 0.4278 - auc: 0.547 - 0s 22us/step - loss: 0.4268 - auc: 0.5399 - val_loss: 0.4213 - val_auc: 0.6452\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.42133 to 0.42129, saving model to DeepFM.h5\n",
      "Epoch 236/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4770 - auc: 0.464 - ETA: 0s - loss: 0.4197 - auc: 0.540 - 0s 21us/step - loss: 0.4268 - auc: 0.5449 - val_loss: 0.4213 - val_auc: 0.6459\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.42129 to 0.42128, saving model to DeepFM.h5\n",
      "Epoch 237/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4253 - auc: 0.480 - ETA: 0s - loss: 0.4174 - auc: 0.553 - 0s 23us/step - loss: 0.4243 - auc: 0.5698 - val_loss: 0.4212 - val_auc: 0.6457\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.42128 to 0.42124, saving model to DeepFM.h5\n",
      "Epoch 238/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4280 - auc: 0.537 - ETA: 0s - loss: 0.4394 - auc: 0.548 - 0s 23us/step - loss: 0.4260 - auc: 0.5492 - val_loss: 0.4212 - val_auc: 0.6486\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.42124 to 0.42121, saving model to DeepFM.h5\n",
      "Epoch 239/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4136 - auc: 0.533 - ETA: 0s - loss: 0.4211 - auc: 0.563 - 0s 21us/step - loss: 0.4250 - auc: 0.5557 - val_loss: 0.4212 - val_auc: 0.6478\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.42121 to 0.42118, saving model to DeepFM.h5\n",
      "Epoch 240/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4823 - auc: 0.543 - ETA: 0s - loss: 0.4281 - auc: 0.544 - 0s 22us/step - loss: 0.4247 - auc: 0.5658 - val_loss: 0.4211 - val_auc: 0.6465\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.42118 to 0.42115, saving model to DeepFM.h5\n",
      "Epoch 241/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4748 - auc: 0.505 - ETA: 0s - loss: 0.4194 - auc: 0.557 - 0s 21us/step - loss: 0.4266 - auc: 0.5463 - val_loss: 0.4211 - val_auc: 0.6485\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.42115 to 0.42113, saving model to DeepFM.h5\n",
      "Epoch 242/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4487 - auc: 0.609 - ETA: 0s - loss: 0.4128 - auc: 0.561 - 0s 22us/step - loss: 0.4247 - auc: 0.5621 - val_loss: 0.4211 - val_auc: 0.6487\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.42113 to 0.42109, saving model to DeepFM.h5\n",
      "Epoch 243/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4119 - auc: 0.562 - ETA: 0s - loss: 0.4285 - auc: 0.547 - 0s 22us/step - loss: 0.4270 - auc: 0.5373 - val_loss: 0.4211 - val_auc: 0.6480\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.42109 to 0.42107, saving model to DeepFM.h5\n",
      "Epoch 244/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3836 - auc: 0.592 - ETA: 0s - loss: 0.4288 - auc: 0.574 - 0s 22us/step - loss: 0.4234 - auc: 0.5801 - val_loss: 0.4210 - val_auc: 0.6477\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.42107 to 0.42104, saving model to DeepFM.h5\n",
      "Epoch 245/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4491 - auc: 0.464 - ETA: 0s - loss: 0.4215 - auc: 0.527 - 0s 22us/step - loss: 0.4258 - auc: 0.5480 - val_loss: 0.4210 - val_auc: 0.6477\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.42104 to 0.42100, saving model to DeepFM.h5\n",
      "Epoch 246/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3839 - auc: 0.476 - ETA: 0s - loss: 0.4245 - auc: 0.560 - 0s 22us/step - loss: 0.4250 - auc: 0.5630 - val_loss: 0.4210 - val_auc: 0.6506\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.42100 to 0.42097, saving model to DeepFM.h5\n",
      "Epoch 247/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4219 - auc: 0.524 - ETA: 0s - loss: 0.4228 - auc: 0.555 - 0s 22us/step - loss: 0.4270 - auc: 0.5438 - val_loss: 0.4210 - val_auc: 0.6491\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.42097 to 0.42095, saving model to DeepFM.h5\n",
      "Epoch 248/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3912 - auc: 0.581 - ETA: 0s - loss: 0.4307 - auc: 0.542 - 0s 22us/step - loss: 0.4257 - auc: 0.5496 - val_loss: 0.4209 - val_auc: 0.6487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00248: val_loss improved from 0.42095 to 0.42092, saving model to DeepFM.h5\n",
      "Epoch 249/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4044 - auc: 0.650 - ETA: 0s - loss: 0.4297 - auc: 0.559 - 0s 25us/step - loss: 0.4242 - auc: 0.5696 - val_loss: 0.4209 - val_auc: 0.6510\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.42092 to 0.42089, saving model to DeepFM.h5\n",
      "Epoch 250/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3924 - auc: 0.567 - ETA: 0s - loss: 0.4112 - auc: 0.552 - 0s 24us/step - loss: 0.4253 - auc: 0.5549 - val_loss: 0.4209 - val_auc: 0.6503\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.42089 to 0.42085, saving model to DeepFM.h5\n",
      "Epoch 251/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3814 - auc: 0.513 - ETA: 0s - loss: 0.4256 - auc: 0.587 - 0s 25us/step - loss: 0.4231 - auc: 0.5812 - val_loss: 0.4208 - val_auc: 0.6494\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.42085 to 0.42081, saving model to DeepFM.h5\n",
      "Epoch 252/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4409 - auc: 0.562 - ETA: 0s - loss: 0.4329 - auc: 0.560 - 0s 28us/step - loss: 0.4246 - auc: 0.5664 - val_loss: 0.4208 - val_auc: 0.6505\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.42081 to 0.42078, saving model to DeepFM.h5\n",
      "Epoch 253/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4449 - auc: 0.569 - ETA: 0s - loss: 0.4251 - auc: 0.516 - 0s 25us/step - loss: 0.4263 - auc: 0.5405 - val_loss: 0.4208 - val_auc: 0.6493\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.42078 to 0.42075, saving model to DeepFM.h5\n",
      "Epoch 254/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4103 - auc: 0.538 - ETA: 0s - loss: 0.4207 - auc: 0.546 - 0s 24us/step - loss: 0.4246 - auc: 0.5658 - val_loss: 0.4207 - val_auc: 0.6497\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.42075 to 0.42072, saving model to DeepFM.h5\n",
      "Epoch 255/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4317 - auc: 0.643 - ETA: 0s - loss: 0.4258 - auc: 0.579 - 0s 24us/step - loss: 0.4254 - auc: 0.5597 - val_loss: 0.4207 - val_auc: 0.6502\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.42072 to 0.42070, saving model to DeepFM.h5\n",
      "Epoch 256/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3683 - auc: 0.591 - ETA: 0s - loss: 0.4272 - auc: 0.571 - 0s 23us/step - loss: 0.4250 - auc: 0.5707 - val_loss: 0.4207 - val_auc: 0.6488\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.42070 to 0.42067, saving model to DeepFM.h5\n",
      "Epoch 257/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4191 - auc: 0.573 - ETA: 0s - loss: 0.4306 - auc: 0.545 - 0s 22us/step - loss: 0.4258 - auc: 0.5518 - val_loss: 0.4206 - val_auc: 0.6501\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.42067 to 0.42064, saving model to DeepFM.h5\n",
      "Epoch 258/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5209 - auc: 0.566 - ETA: 0s - loss: 0.4281 - auc: 0.564 - 0s 21us/step - loss: 0.4251 - auc: 0.5538 - val_loss: 0.4206 - val_auc: 0.6485\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.42064 to 0.42061, saving model to DeepFM.h5\n",
      "Epoch 259/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4198 - auc: 0.643 - ETA: 0s - loss: 0.4246 - auc: 0.579 - 0s 22us/step - loss: 0.4236 - auc: 0.5704 - val_loss: 0.4206 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.42061 to 0.42057, saving model to DeepFM.h5\n",
      "Epoch 260/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4039 - auc: 0.666 - ETA: 0s - loss: 0.4227 - auc: 0.542 - 0s 22us/step - loss: 0.4263 - auc: 0.5458 - val_loss: 0.4205 - val_auc: 0.6489\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.42057 to 0.42054, saving model to DeepFM.h5\n",
      "Epoch 261/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4947 - auc: 0.568 - ETA: 0s - loss: 0.4277 - auc: 0.574 - 0s 21us/step - loss: 0.4234 - auc: 0.5776 - val_loss: 0.4205 - val_auc: 0.6489\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.42054 to 0.42050, saving model to DeepFM.h5\n",
      "Epoch 262/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4346 - auc: 0.491 - ETA: 0s - loss: 0.4349 - auc: 0.523 - 0s 22us/step - loss: 0.4267 - auc: 0.5411 - val_loss: 0.4205 - val_auc: 0.6496\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.42050 to 0.42048, saving model to DeepFM.h5\n",
      "Epoch 263/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4242 - auc: 0.595 - ETA: 0s - loss: 0.4188 - auc: 0.566 - 0s 23us/step - loss: 0.4251 - auc: 0.5576 - val_loss: 0.4204 - val_auc: 0.6460\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.42048 to 0.42045, saving model to DeepFM.h5\n",
      "Epoch 264/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4668 - auc: 0.550 - ETA: 0s - loss: 0.4306 - auc: 0.551 - 0s 23us/step - loss: 0.4251 - auc: 0.5611 - val_loss: 0.4204 - val_auc: 0.6474\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.42045 to 0.42042, saving model to DeepFM.h5\n",
      "Epoch 265/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4604 - auc: 0.484 - ETA: 0s - loss: 0.4195 - auc: 0.569 - 0s 22us/step - loss: 0.4234 - auc: 0.5705 - val_loss: 0.4204 - val_auc: 0.6508\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.42042 to 0.42039, saving model to DeepFM.h5\n",
      "Epoch 266/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3951 - auc: 0.629 - ETA: 0s - loss: 0.4341 - auc: 0.565 - 0s 22us/step - loss: 0.4249 - auc: 0.5601 - val_loss: 0.4204 - val_auc: 0.6498\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.42039 to 0.42037, saving model to DeepFM.h5\n",
      "Epoch 267/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3767 - auc: 0.583 - ETA: 0s - loss: 0.4377 - auc: 0.547 - 0s 22us/step - loss: 0.4250 - auc: 0.5488 - val_loss: 0.4203 - val_auc: 0.6487\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.42037 to 0.42033, saving model to DeepFM.h5\n",
      "Epoch 268/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4505 - auc: 0.571 - ETA: 0s - loss: 0.4358 - auc: 0.550 - 0s 23us/step - loss: 0.4254 - auc: 0.5498 - val_loss: 0.4203 - val_auc: 0.6506\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.42033 to 0.42030, saving model to DeepFM.h5\n",
      "Epoch 269/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4212 - auc: 0.616 - ETA: 0s - loss: 0.4315 - auc: 0.561 - 0s 21us/step - loss: 0.4256 - auc: 0.5471 - val_loss: 0.4203 - val_auc: 0.6484\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.42030 to 0.42027, saving model to DeepFM.h5\n",
      "Epoch 270/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4173 - auc: 0.607 - ETA: 0s - loss: 0.4250 - auc: 0.559 - 0s 22us/step - loss: 0.4259 - auc: 0.5460 - val_loss: 0.4202 - val_auc: 0.6435\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.42027 to 0.42023, saving model to DeepFM.h5\n",
      "Epoch 271/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3979 - auc: 0.584 - ETA: 0s - loss: 0.4240 - auc: 0.565 - 0s 22us/step - loss: 0.4242 - auc: 0.5688 - val_loss: 0.4202 - val_auc: 0.6459\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.42023 to 0.42021, saving model to DeepFM.h5\n",
      "Epoch 272/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.587 - ETA: 0s - loss: 0.4307 - auc: 0.581 - 0s 21us/step - loss: 0.4233 - auc: 0.5802 - val_loss: 0.4202 - val_auc: 0.6479\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.42021 to 0.42018, saving model to DeepFM.h5\n",
      "Epoch 273/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4214 - auc: 0.517 - ETA: 0s - loss: 0.4222 - auc: 0.583 - 0s 21us/step - loss: 0.4238 - auc: 0.5754 - val_loss: 0.4201 - val_auc: 0.6488\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.42018 to 0.42015, saving model to DeepFM.h5\n",
      "Epoch 274/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3785 - auc: 0.569 - ETA: 0s - loss: 0.4166 - auc: 0.575 - 0s 23us/step - loss: 0.4223 - auc: 0.5792 - val_loss: 0.4201 - val_auc: 0.6480\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.42015 to 0.42012, saving model to DeepFM.h5\n",
      "Epoch 275/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3989 - auc: 0.466 - ETA: 0s - loss: 0.4219 - auc: 0.553 - 0s 21us/step - loss: 0.4233 - auc: 0.5674 - val_loss: 0.4201 - val_auc: 0.6453\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.42012 to 0.42007, saving model to DeepFM.h5\n",
      "Epoch 276/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4092 - auc: 0.514 - ETA: 0s - loss: 0.4322 - auc: 0.537 - 0s 24us/step - loss: 0.4265 - auc: 0.5392 - val_loss: 0.4200 - val_auc: 0.6451\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.42007 to 0.42004, saving model to DeepFM.h5\n",
      "Epoch 277/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4501 - auc: 0.572 - ETA: 0s - loss: 0.4195 - auc: 0.565 - 0s 22us/step - loss: 0.4234 - auc: 0.5669 - val_loss: 0.4200 - val_auc: 0.6463\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.42004 to 0.42000, saving model to DeepFM.h5\n",
      "Epoch 278/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4627 - auc: 0.635 - ETA: 0s - loss: 0.4368 - auc: 0.552 - 0s 21us/step - loss: 0.4246 - auc: 0.5568 - val_loss: 0.4200 - val_auc: 0.6462\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.42000 to 0.41997, saving model to DeepFM.h5\n",
      "Epoch 279/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4358 - auc: 0.604 - ETA: 0s - loss: 0.4378 - auc: 0.571 - 0s 23us/step - loss: 0.4246 - auc: 0.5668 - val_loss: 0.4199 - val_auc: 0.6452\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.41997 to 0.41994, saving model to DeepFM.h5\n",
      "Epoch 280/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4630 - auc: 0.586 - ETA: 0s - loss: 0.4242 - auc: 0.553 - 0s 21us/step - loss: 0.4248 - auc: 0.5630 - val_loss: 0.4199 - val_auc: 0.6443\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.41994 to 0.41992, saving model to DeepFM.h5\n",
      "Epoch 281/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4142 - auc: 0.566 - ETA: 0s - loss: 0.4250 - auc: 0.560 - 0s 22us/step - loss: 0.4244 - auc: 0.5649 - val_loss: 0.4199 - val_auc: 0.6462\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.41992 to 0.41989, saving model to DeepFM.h5\n",
      "Epoch 282/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4316 - auc: 0.525 - ETA: 0s - loss: 0.4314 - auc: 0.541 - 0s 24us/step - loss: 0.4253 - auc: 0.5552 - val_loss: 0.4199 - val_auc: 0.6445\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.41989 to 0.41987, saving model to DeepFM.h5\n",
      "Epoch 283/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4213 - auc: 0.549 - ETA: 0s - loss: 0.4138 - auc: 0.583 - 0s 23us/step - loss: 0.4229 - auc: 0.5745 - val_loss: 0.4198 - val_auc: 0.6466\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.41987 to 0.41983, saving model to DeepFM.h5\n",
      "Epoch 284/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4358 - auc: 0.606 - ETA: 0s - loss: 0.4127 - auc: 0.588 - 0s 22us/step - loss: 0.4219 - auc: 0.5871 - val_loss: 0.4198 - val_auc: 0.6470\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.41983 to 0.41978, saving model to DeepFM.h5\n",
      "Epoch 285/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4357 - auc: 0.603 - ETA: 0s - loss: 0.4190 - auc: 0.563 - 0s 22us/step - loss: 0.4230 - auc: 0.5727 - val_loss: 0.4197 - val_auc: 0.6456\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.41978 to 0.41974, saving model to DeepFM.h5\n",
      "Epoch 286/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4121 - auc: 0.463 - ETA: 0s - loss: 0.4266 - auc: 0.553 - 0s 23us/step - loss: 0.4255 - auc: 0.5556 - val_loss: 0.4197 - val_auc: 0.6491\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.41974 to 0.41971, saving model to DeepFM.h5\n",
      "Epoch 287/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4179 - auc: 0.553 - ETA: 0s - loss: 0.4304 - auc: 0.564 - 0s 22us/step - loss: 0.4245 - auc: 0.5628 - val_loss: 0.4197 - val_auc: 0.6464\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.41971 to 0.41969, saving model to DeepFM.h5\n",
      "Epoch 288/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4541 - auc: 0.465 - ETA: 0s - loss: 0.4296 - auc: 0.556 - 0s 24us/step - loss: 0.4247 - auc: 0.5561 - val_loss: 0.4197 - val_auc: 0.6466\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.41969 to 0.41966, saving model to DeepFM.h5\n",
      "Epoch 289/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4813 - auc: 0.605 - ETA: 0s - loss: 0.4288 - auc: 0.558 - 0s 22us/step - loss: 0.4245 - auc: 0.5607 - val_loss: 0.4196 - val_auc: 0.6472\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.41966 to 0.41963, saving model to DeepFM.h5\n",
      "Epoch 290/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4469 - auc: 0.618 - ETA: 0s - loss: 0.4214 - auc: 0.610 - 0s 23us/step - loss: 0.4216 - auc: 0.6029 - val_loss: 0.4196 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.41963 to 0.41961, saving model to DeepFM.h5\n",
      "Epoch 291/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3885 - auc: 0.615 - ETA: 0s - loss: 0.4268 - auc: 0.566 - 0s 26us/step - loss: 0.4243 - auc: 0.5716 - val_loss: 0.4196 - val_auc: 0.6495\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.41961 to 0.41959, saving model to DeepFM.h5\n",
      "Epoch 292/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4188 - auc: 0.575 - ETA: 0s - loss: 0.4267 - auc: 0.573 - 0s 25us/step - loss: 0.4238 - auc: 0.5711 - val_loss: 0.4196 - val_auc: 0.6493\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.41959 to 0.41956, saving model to DeepFM.h5\n",
      "Epoch 293/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4663 - auc: 0.557 - ETA: 0s - loss: 0.4080 - auc: 0.572 - 0s 23us/step - loss: 0.4241 - auc: 0.5639 - val_loss: 0.4195 - val_auc: 0.6477\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.41956 to 0.41953, saving model to DeepFM.h5\n",
      "Epoch 294/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4523 - auc: 0.567 - ETA: 0s - loss: 0.4316 - auc: 0.556 - 0s 23us/step - loss: 0.4248 - auc: 0.5496 - val_loss: 0.4195 - val_auc: 0.6468\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.41953 to 0.41951, saving model to DeepFM.h5\n",
      "Epoch 295/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4313 - auc: 0.635 - ETA: 0s - loss: 0.4170 - auc: 0.576 - 0s 22us/step - loss: 0.4219 - auc: 0.5868 - val_loss: 0.4195 - val_auc: 0.6481\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.41951 to 0.41947, saving model to DeepFM.h5\n",
      "Epoch 296/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4318 - auc: 0.611 - ETA: 0s - loss: 0.4251 - auc: 0.553 - 0s 22us/step - loss: 0.4248 - auc: 0.5588 - val_loss: 0.4194 - val_auc: 0.6476\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.41947 to 0.41944, saving model to DeepFM.h5\n",
      "Epoch 297/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4311 - auc: 0.566 - ETA: 0s - loss: 0.4229 - auc: 0.566 - 0s 22us/step - loss: 0.4234 - auc: 0.5684 - val_loss: 0.4194 - val_auc: 0.6487\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.41944 to 0.41941, saving model to DeepFM.h5\n",
      "Epoch 298/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4001 - auc: 0.579 - ETA: 0s - loss: 0.4163 - auc: 0.566 - 0s 23us/step - loss: 0.4236 - auc: 0.5696 - val_loss: 0.4194 - val_auc: 0.6501\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.41941 to 0.41938, saving model to DeepFM.h5\n",
      "Epoch 299/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3238 - auc: 0.580 - ETA: 0s - loss: 0.4236 - auc: 0.564 - 0s 21us/step - loss: 0.4237 - auc: 0.5647 - val_loss: 0.4194 - val_auc: 0.6492\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.41938 to 0.41935, saving model to DeepFM.h5\n",
      "Epoch 300/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4129 - auc: 0.602 - ETA: 0s - loss: 0.4317 - auc: 0.539 - 0s 22us/step - loss: 0.4254 - auc: 0.5450 - val_loss: 0.4193 - val_auc: 0.6543\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.41935 to 0.41932, saving model to DeepFM.h5\n",
      "Epoch 301/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4326 - auc: 0.548 - ETA: 0s - loss: 0.4182 - auc: 0.574 - 0s 23us/step - loss: 0.4217 - auc: 0.5833 - val_loss: 0.4193 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.41932 to 0.41928, saving model to DeepFM.h5\n",
      "Epoch 302/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4021 - auc: 0.592 - ETA: 0s - loss: 0.4297 - auc: 0.579 - 0s 21us/step - loss: 0.4233 - auc: 0.5701 - val_loss: 0.4193 - val_auc: 0.6496\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.41928 to 0.41926, saving model to DeepFM.h5\n",
      "Epoch 303/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4194 - auc: 0.532 - ETA: 0s - loss: 0.4257 - auc: 0.566 - 0s 22us/step - loss: 0.4229 - auc: 0.5715 - val_loss: 0.4192 - val_auc: 0.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00303: val_loss improved from 0.41926 to 0.41923, saving model to DeepFM.h5\n",
      "Epoch 304/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3741 - auc: 0.508 - ETA: 0s - loss: 0.4186 - auc: 0.579 - 0s 21us/step - loss: 0.4230 - auc: 0.5718 - val_loss: 0.4192 - val_auc: 0.6508\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.41923 to 0.41919, saving model to DeepFM.h5\n",
      "Epoch 305/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4767 - auc: 0.596 - ETA: 0s - loss: 0.4186 - auc: 0.567 - 0s 23us/step - loss: 0.4235 - auc: 0.5643 - val_loss: 0.4192 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.41919 to 0.41915, saving model to DeepFM.h5\n",
      "Epoch 306/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5160 - auc: 0.547 - ETA: 0s - loss: 0.4177 - auc: 0.577 - 0s 23us/step - loss: 0.4228 - auc: 0.5777 - val_loss: 0.4191 - val_auc: 0.6521\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.41915 to 0.41912, saving model to DeepFM.h5\n",
      "Epoch 307/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3762 - auc: 0.594 - ETA: 0s - loss: 0.4063 - auc: 0.561 - 0s 22us/step - loss: 0.4227 - auc: 0.5711 - val_loss: 0.4191 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.41912 to 0.41909, saving model to DeepFM.h5\n",
      "Epoch 308/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3612 - auc: 0.671 - ETA: 0s - loss: 0.4238 - auc: 0.584 - 0s 21us/step - loss: 0.4215 - auc: 0.5852 - val_loss: 0.4191 - val_auc: 0.6500\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.41909 to 0.41906, saving model to DeepFM.h5\n",
      "Epoch 309/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4806 - auc: 0.613 - ETA: 0s - loss: 0.4221 - auc: 0.563 - 0s 21us/step - loss: 0.4235 - auc: 0.5669 - val_loss: 0.4190 - val_auc: 0.6536\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.41906 to 0.41903, saving model to DeepFM.h5\n",
      "Epoch 310/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4837 - auc: 0.582 - ETA: 0s - loss: 0.4264 - auc: 0.556 - 0s 21us/step - loss: 0.4241 - auc: 0.5624 - val_loss: 0.4190 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.41903 to 0.41900, saving model to DeepFM.h5\n",
      "Epoch 311/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4191 - auc: 0.547 - ETA: 0s - loss: 0.4220 - auc: 0.577 - 0s 22us/step - loss: 0.4218 - auc: 0.5838 - val_loss: 0.4190 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.41900 to 0.41897, saving model to DeepFM.h5\n",
      "Epoch 312/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4703 - auc: 0.530 - ETA: 0s - loss: 0.4272 - auc: 0.584 - 0s 21us/step - loss: 0.4221 - auc: 0.5854 - val_loss: 0.4189 - val_auc: 0.6534\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.41897 to 0.41894, saving model to DeepFM.h5\n",
      "Epoch 313/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3956 - auc: 0.508 - ETA: 0s - loss: 0.4162 - auc: 0.552 - 0s 23us/step - loss: 0.4234 - auc: 0.5646 - val_loss: 0.4189 - val_auc: 0.6541\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.41894 to 0.41891, saving model to DeepFM.h5\n",
      "Epoch 314/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4218 - auc: 0.617 - ETA: 0s - loss: 0.4185 - auc: 0.594 - 0s 22us/step - loss: 0.4231 - auc: 0.5777 - val_loss: 0.4189 - val_auc: 0.6495\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.41891 to 0.41887, saving model to DeepFM.h5\n",
      "Epoch 315/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4527 - auc: 0.501 - ETA: 0s - loss: 0.4202 - auc: 0.591 - 0s 23us/step - loss: 0.4224 - auc: 0.5806 - val_loss: 0.4188 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.41887 to 0.41884, saving model to DeepFM.h5\n",
      "Epoch 316/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4904 - auc: 0.578 - ETA: 0s - loss: 0.4129 - auc: 0.568 - 0s 22us/step - loss: 0.4219 - auc: 0.5852 - val_loss: 0.4188 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.41884 to 0.41881, saving model to DeepFM.h5\n",
      "Epoch 317/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4270 - auc: 0.459 - ETA: 0s - loss: 0.4206 - auc: 0.572 - 0s 23us/step - loss: 0.4223 - auc: 0.5734 - val_loss: 0.4188 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.41881 to 0.41877, saving model to DeepFM.h5\n",
      "Epoch 318/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4306 - auc: 0.572 - ETA: 0s - loss: 0.4158 - auc: 0.607 - 0s 23us/step - loss: 0.4212 - auc: 0.5968 - val_loss: 0.4187 - val_auc: 0.6530\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.41877 to 0.41875, saving model to DeepFM.h5\n",
      "Epoch 319/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4977 - auc: 0.579 - ETA: 0s - loss: 0.4251 - auc: 0.582 - 0s 23us/step - loss: 0.4219 - auc: 0.5845 - val_loss: 0.4187 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.41875 to 0.41872, saving model to DeepFM.h5\n",
      "Epoch 320/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.574 - ETA: 0s - loss: 0.4199 - auc: 0.574 - 0s 23us/step - loss: 0.4229 - auc: 0.5727 - val_loss: 0.4187 - val_auc: 0.6518\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.41872 to 0.41868, saving model to DeepFM.h5\n",
      "Epoch 321/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3588 - auc: 0.633 - ETA: 0s - loss: 0.4138 - auc: 0.581 - 0s 21us/step - loss: 0.4232 - auc: 0.5650 - val_loss: 0.4186 - val_auc: 0.6551\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.41868 to 0.41864, saving model to DeepFM.h5\n",
      "Epoch 322/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3875 - auc: 0.597 - ETA: 0s - loss: 0.4164 - auc: 0.582 - 0s 21us/step - loss: 0.4212 - auc: 0.5898 - val_loss: 0.4186 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.41864 to 0.41860, saving model to DeepFM.h5\n",
      "Epoch 323/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3983 - auc: 0.582 - ETA: 0s - loss: 0.4282 - auc: 0.572 - 0s 21us/step - loss: 0.4230 - auc: 0.5779 - val_loss: 0.4186 - val_auc: 0.6561\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.41860 to 0.41858, saving model to DeepFM.h5\n",
      "Epoch 324/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3396 - auc: 0.690 - ETA: 0s - loss: 0.4235 - auc: 0.591 - 0s 22us/step - loss: 0.4226 - auc: 0.5771 - val_loss: 0.4185 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.41858 to 0.41854, saving model to DeepFM.h5\n",
      "Epoch 325/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4239 - auc: 0.638 - ETA: 0s - loss: 0.4244 - auc: 0.606 - 0s 23us/step - loss: 0.4196 - auc: 0.6064 - val_loss: 0.4185 - val_auc: 0.6571\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.41854 to 0.41851, saving model to DeepFM.h5\n",
      "Epoch 326/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3719 - auc: 0.630 - ETA: 0s - loss: 0.4357 - auc: 0.575 - 0s 23us/step - loss: 0.4223 - auc: 0.5796 - val_loss: 0.4185 - val_auc: 0.6574\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.41851 to 0.41848, saving model to DeepFM.h5\n",
      "Epoch 327/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4041 - auc: 0.464 - ETA: 0s - loss: 0.4148 - auc: 0.562 - 0s 22us/step - loss: 0.4239 - auc: 0.5590 - val_loss: 0.4184 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.41848 to 0.41844, saving model to DeepFM.h5\n",
      "Epoch 328/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3490 - auc: 0.546 - ETA: 0s - loss: 0.4206 - auc: 0.577 - 0s 24us/step - loss: 0.4227 - auc: 0.5700 - val_loss: 0.4184 - val_auc: 0.6505\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.41844 to 0.41840, saving model to DeepFM.h5\n",
      "Epoch 329/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4230 - auc: 0.558 - ETA: 0s - loss: 0.4298 - auc: 0.566 - 0s 21us/step - loss: 0.4231 - auc: 0.5675 - val_loss: 0.4184 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.41840 to 0.41838, saving model to DeepFM.h5\n",
      "Epoch 330/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4482 - auc: 0.579 - ETA: 0s - loss: 0.4244 - auc: 0.565 - 0s 22us/step - loss: 0.4216 - auc: 0.5797 - val_loss: 0.4184 - val_auc: 0.6558\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.41838 to 0.41835, saving model to DeepFM.h5\n",
      "Epoch 331/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4375 - auc: 0.652 - ETA: 0s - loss: 0.4244 - auc: 0.572 - 0s 22us/step - loss: 0.4230 - auc: 0.5744 - val_loss: 0.4183 - val_auc: 0.6558\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.41835 to 0.41833, saving model to DeepFM.h5\n",
      "Epoch 332/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3955 - auc: 0.653 - ETA: 0s - loss: 0.4149 - auc: 0.593 - 0s 21us/step - loss: 0.4213 - auc: 0.5917 - val_loss: 0.4183 - val_auc: 0.6547\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.41833 to 0.41829, saving model to DeepFM.h5\n",
      "Epoch 333/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4283 - auc: 0.507 - ETA: 0s - loss: 0.4236 - auc: 0.594 - 0s 23us/step - loss: 0.4205 - auc: 0.5985 - val_loss: 0.4183 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.41829 to 0.41826, saving model to DeepFM.h5\n",
      "Epoch 334/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4529 - auc: 0.578 - ETA: 0s - loss: 0.4130 - auc: 0.584 - 0s 21us/step - loss: 0.4216 - auc: 0.5846 - val_loss: 0.4182 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.41826 to 0.41824, saving model to DeepFM.h5\n",
      "Epoch 335/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3647 - auc: 0.552 - ETA: 0s - loss: 0.4195 - auc: 0.580 - 0s 21us/step - loss: 0.4222 - auc: 0.5766 - val_loss: 0.4182 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.41824 to 0.41821, saving model to DeepFM.h5\n",
      "Epoch 336/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4866 - auc: 0.559 - ETA: 0s - loss: 0.4335 - auc: 0.583 - 0s 23us/step - loss: 0.4219 - auc: 0.5792 - val_loss: 0.4182 - val_auc: 0.6523\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.41821 to 0.41817, saving model to DeepFM.h5\n",
      "Epoch 337/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5152 - auc: 0.569 - ETA: 0s - loss: 0.4280 - auc: 0.595 - 0s 20us/step - loss: 0.4215 - auc: 0.5889 - val_loss: 0.4181 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.41817 to 0.41814, saving model to DeepFM.h5\n",
      "Epoch 338/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4068 - auc: 0.626 - ETA: 0s - loss: 0.4276 - auc: 0.588 - 0s 22us/step - loss: 0.4214 - auc: 0.5828 - val_loss: 0.4181 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.41814 to 0.41811, saving model to DeepFM.h5\n",
      "Epoch 339/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3746 - auc: 0.452 - ETA: 0s - loss: 0.4229 - auc: 0.562 - 0s 23us/step - loss: 0.4231 - auc: 0.5672 - val_loss: 0.4181 - val_auc: 0.6537\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.41811 to 0.41808, saving model to DeepFM.h5\n",
      "Epoch 340/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3640 - auc: 0.552 - ETA: 0s - loss: 0.4196 - auc: 0.576 - 0s 21us/step - loss: 0.4231 - auc: 0.5738 - val_loss: 0.4181 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.41808 to 0.41806, saving model to DeepFM.h5\n",
      "Epoch 341/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4845 - auc: 0.614 - ETA: 0s - loss: 0.4291 - auc: 0.580 - 0s 21us/step - loss: 0.4223 - auc: 0.5807 - val_loss: 0.4180 - val_auc: 0.6532\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.41806 to 0.41805, saving model to DeepFM.h5\n",
      "Epoch 342/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4478 - auc: 0.467 - ETA: 0s - loss: 0.4211 - auc: 0.565 - 0s 21us/step - loss: 0.4240 - auc: 0.5605 - val_loss: 0.4180 - val_auc: 0.6569\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.41805 to 0.41803, saving model to DeepFM.h5\n",
      "Epoch 343/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4833 - auc: 0.591 - ETA: 0s - loss: 0.4157 - auc: 0.584 - 0s 21us/step - loss: 0.4217 - auc: 0.5866 - val_loss: 0.4180 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.41803 to 0.41800, saving model to DeepFM.h5\n",
      "Epoch 344/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4388 - auc: 0.564 - ETA: 0s - loss: 0.4274 - auc: 0.592 - 0s 23us/step - loss: 0.4209 - auc: 0.5850 - val_loss: 0.4180 - val_auc: 0.6512\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.41800 to 0.41797, saving model to DeepFM.h5\n",
      "Epoch 345/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3310 - auc: 0.673 - ETA: 0s - loss: 0.4185 - auc: 0.595 - 0s 22us/step - loss: 0.4215 - auc: 0.5868 - val_loss: 0.4179 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.41797 to 0.41792, saving model to DeepFM.h5\n",
      "Epoch 346/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3800 - auc: 0.549 - ETA: 0s - loss: 0.4238 - auc: 0.592 - 0s 22us/step - loss: 0.4214 - auc: 0.5909 - val_loss: 0.4179 - val_auc: 0.6510\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.41792 to 0.41790, saving model to DeepFM.h5\n",
      "Epoch 347/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4154 - auc: 0.502 - ETA: 0s - loss: 0.4274 - auc: 0.563 - 0s 22us/step - loss: 0.4231 - auc: 0.5718 - val_loss: 0.4179 - val_auc: 0.6541\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.41790 to 0.41787, saving model to DeepFM.h5\n",
      "Epoch 348/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3920 - auc: 0.537 - ETA: 0s - loss: 0.4193 - auc: 0.595 - 0s 22us/step - loss: 0.4221 - auc: 0.5715 - val_loss: 0.4178 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.41787 to 0.41784, saving model to DeepFM.h5\n",
      "Epoch 349/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4420 - auc: 0.591 - ETA: 0s - loss: 0.4237 - auc: 0.595 - 0s 21us/step - loss: 0.4211 - auc: 0.5848 - val_loss: 0.4178 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.41784 to 0.41781, saving model to DeepFM.h5\n",
      "Epoch 350/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3359 - auc: 0.697 - ETA: 0s - loss: 0.4298 - auc: 0.551 - 0s 21us/step - loss: 0.4222 - auc: 0.5720 - val_loss: 0.4178 - val_auc: 0.6567\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.41781 to 0.41778, saving model to DeepFM.h5\n",
      "Epoch 351/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4344 - auc: 0.591 - ETA: 0s - loss: 0.4281 - auc: 0.581 - 0s 21us/step - loss: 0.4219 - auc: 0.5797 - val_loss: 0.4178 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.41778 to 0.41776, saving model to DeepFM.h5\n",
      "Epoch 352/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.659 - ETA: 0s - loss: 0.4268 - auc: 0.626 - 0s 24us/step - loss: 0.4196 - auc: 0.6094 - val_loss: 0.4177 - val_auc: 0.6499\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.41776 to 0.41773, saving model to DeepFM.h5\n",
      "Epoch 353/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4734 - auc: 0.574 - ETA: 0s - loss: 0.4124 - auc: 0.581 - 0s 22us/step - loss: 0.4225 - auc: 0.5742 - val_loss: 0.4177 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.41773 to 0.41771, saving model to DeepFM.h5\n",
      "Epoch 354/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4225 - auc: 0.590 - ETA: 0s - loss: 0.4156 - auc: 0.571 - 0s 23us/step - loss: 0.4211 - auc: 0.5832 - val_loss: 0.4177 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.41771 to 0.41766, saving model to DeepFM.h5\n",
      "Epoch 355/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4507 - auc: 0.622 - ETA: 0s - loss: 0.4166 - auc: 0.595 - 0s 22us/step - loss: 0.4199 - auc: 0.5920 - val_loss: 0.4176 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.41766 to 0.41763, saving model to DeepFM.h5\n",
      "Epoch 356/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4564 - auc: 0.502 - ETA: 0s - loss: 0.4210 - auc: 0.576 - 0s 22us/step - loss: 0.4226 - auc: 0.5832 - val_loss: 0.4176 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.41763 to 0.41761, saving model to DeepFM.h5\n",
      "Epoch 357/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4501 - auc: 0.564 - ETA: 0s - loss: 0.4167 - auc: 0.608 - 0s 22us/step - loss: 0.4202 - auc: 0.5929 - val_loss: 0.4176 - val_auc: 0.6517\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.41761 to 0.41757, saving model to DeepFM.h5\n",
      "Epoch 358/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3932 - auc: 0.615 - ETA: 0s - loss: 0.4224 - auc: 0.582 - 0s 23us/step - loss: 0.4215 - auc: 0.5797 - val_loss: 0.4175 - val_auc: 0.6528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00358: val_loss improved from 0.41757 to 0.41754, saving model to DeepFM.h5\n",
      "Epoch 359/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4330 - auc: 0.451 - ETA: 0s - loss: 0.4194 - auc: 0.545 - 0s 22us/step - loss: 0.4230 - auc: 0.5675 - val_loss: 0.4175 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.41754 to 0.41752, saving model to DeepFM.h5\n",
      "Epoch 360/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4667 - auc: 0.524 - ETA: 0s - loss: 0.4201 - auc: 0.571 - 0s 21us/step - loss: 0.4217 - auc: 0.5798 - val_loss: 0.4175 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.41752 to 0.41748, saving model to DeepFM.h5\n",
      "Epoch 361/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4231 - auc: 0.627 - ETA: 0s - loss: 0.4209 - auc: 0.584 - 0s 23us/step - loss: 0.4220 - auc: 0.5764 - val_loss: 0.4174 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.41748 to 0.41745, saving model to DeepFM.h5\n",
      "Epoch 362/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3396 - auc: 0.583 - ETA: 0s - loss: 0.4149 - auc: 0.602 - 0s 23us/step - loss: 0.4197 - auc: 0.6035 - val_loss: 0.4174 - val_auc: 0.6551\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.41745 to 0.41741, saving model to DeepFM.h5\n",
      "Epoch 363/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4087 - auc: 0.620 - ETA: 0s - loss: 0.4200 - auc: 0.583 - 0s 23us/step - loss: 0.4207 - auc: 0.5832 - val_loss: 0.4174 - val_auc: 0.6548\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.41741 to 0.41738, saving model to DeepFM.h5\n",
      "Epoch 364/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4285 - auc: 0.660 - ETA: 0s - loss: 0.4204 - auc: 0.610 - 0s 22us/step - loss: 0.4188 - auc: 0.6121 - val_loss: 0.4173 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.41738 to 0.41734, saving model to DeepFM.h5\n",
      "Epoch 365/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4093 - auc: 0.617 - ETA: 0s - loss: 0.4184 - auc: 0.592 - 0s 22us/step - loss: 0.4204 - auc: 0.5918 - val_loss: 0.4173 - val_auc: 0.6547\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.41734 to 0.41730, saving model to DeepFM.h5\n",
      "Epoch 366/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4839 - auc: 0.650 - ETA: 0s - loss: 0.4180 - auc: 0.588 - 0s 22us/step - loss: 0.4194 - auc: 0.5989 - val_loss: 0.4173 - val_auc: 0.6553\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.41730 to 0.41726, saving model to DeepFM.h5\n",
      "Epoch 367/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4394 - auc: 0.553 - ETA: 0s - loss: 0.4236 - auc: 0.620 - 0s 22us/step - loss: 0.4188 - auc: 0.6066 - val_loss: 0.4172 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.41726 to 0.41724, saving model to DeepFM.h5\n",
      "Epoch 368/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4884 - auc: 0.625 - ETA: 0s - loss: 0.4188 - auc: 0.596 - 0s 22us/step - loss: 0.4208 - auc: 0.5925 - val_loss: 0.4172 - val_auc: 0.6521\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.41724 to 0.41720, saving model to DeepFM.h5\n",
      "Epoch 369/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4602 - auc: 0.620 - ETA: 0s - loss: 0.4206 - auc: 0.607 - 0s 21us/step - loss: 0.4198 - auc: 0.6019 - val_loss: 0.4172 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.41720 to 0.41716, saving model to DeepFM.h5\n",
      "Epoch 370/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4631 - auc: 0.554 - ETA: 0s - loss: 0.4216 - auc: 0.575 - 0s 22us/step - loss: 0.4214 - auc: 0.5833 - val_loss: 0.4171 - val_auc: 0.6530\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.41716 to 0.41713, saving model to DeepFM.h5\n",
      "Epoch 371/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3902 - auc: 0.611 - ETA: 0s - loss: 0.4209 - auc: 0.586 - 0s 22us/step - loss: 0.4191 - auc: 0.6016 - val_loss: 0.4171 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.41713 to 0.41710, saving model to DeepFM.h5\n",
      "Epoch 372/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3673 - auc: 0.596 - ETA: 0s - loss: 0.4144 - auc: 0.599 - 0s 24us/step - loss: 0.4203 - auc: 0.5969 - val_loss: 0.4171 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.41710 to 0.41708, saving model to DeepFM.h5\n",
      "Epoch 373/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3839 - auc: 0.637 - ETA: 0s - loss: 0.4212 - auc: 0.612 - 0s 22us/step - loss: 0.4192 - auc: 0.6084 - val_loss: 0.4171 - val_auc: 0.6512\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.41708 to 0.41705, saving model to DeepFM.h5\n",
      "Epoch 374/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4277 - auc: 0.537 - ETA: 0s - loss: 0.4187 - auc: 0.573 - 0s 22us/step - loss: 0.4213 - auc: 0.5781 - val_loss: 0.4170 - val_auc: 0.6520\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.41705 to 0.41702, saving model to DeepFM.h5\n",
      "Epoch 375/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3660 - auc: 0.498 - ETA: 0s - loss: 0.4297 - auc: 0.567 - 0s 22us/step - loss: 0.4226 - auc: 0.5684 - val_loss: 0.4170 - val_auc: 0.6502\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.41702 to 0.41700, saving model to DeepFM.h5\n",
      "Epoch 376/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3861 - auc: 0.589 - ETA: 0s - loss: 0.4143 - auc: 0.576 - 0s 21us/step - loss: 0.4204 - auc: 0.5962 - val_loss: 0.4170 - val_auc: 0.6498\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.41700 to 0.41696, saving model to DeepFM.h5\n",
      "Epoch 377/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4615 - auc: 0.607 - ETA: 0s - loss: 0.4129 - auc: 0.595 - 0s 22us/step - loss: 0.4190 - auc: 0.6094 - val_loss: 0.4169 - val_auc: 0.6506\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.41696 to 0.41693, saving model to DeepFM.h5\n",
      "Epoch 378/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3964 - auc: 0.552 - ETA: 0s - loss: 0.4209 - auc: 0.578 - 0s 21us/step - loss: 0.4203 - auc: 0.5853 - val_loss: 0.4169 - val_auc: 0.6541\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.41693 to 0.41689, saving model to DeepFM.h5\n",
      "Epoch 379/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4321 - auc: 0.574 - ETA: 0s - loss: 0.4170 - auc: 0.584 - 0s 22us/step - loss: 0.4213 - auc: 0.5873 - val_loss: 0.4169 - val_auc: 0.6544\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.41689 to 0.41687, saving model to DeepFM.h5\n",
      "Epoch 380/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3596 - auc: 0.572 - ETA: 0s - loss: 0.4094 - auc: 0.571 - 0s 22us/step - loss: 0.4221 - auc: 0.5784 - val_loss: 0.4168 - val_auc: 0.6551\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.41687 to 0.41685, saving model to DeepFM.h5\n",
      "Epoch 381/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4566 - auc: 0.596 - ETA: 0s - loss: 0.4219 - auc: 0.589 - 0s 23us/step - loss: 0.4208 - auc: 0.5900 - val_loss: 0.4168 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.41685 to 0.41682, saving model to DeepFM.h5\n",
      "Epoch 382/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4453 - auc: 0.590 - ETA: 0s - loss: 0.4436 - auc: 0.582 - 0s 22us/step - loss: 0.4221 - auc: 0.5769 - val_loss: 0.4168 - val_auc: 0.6553\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.41682 to 0.41680, saving model to DeepFM.h5\n",
      "Epoch 383/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4291 - auc: 0.578 - ETA: 0s - loss: 0.4057 - auc: 0.579 - 0s 22us/step - loss: 0.4218 - auc: 0.5720 - val_loss: 0.4168 - val_auc: 0.6565\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.41680 to 0.41676, saving model to DeepFM.h5\n",
      "Epoch 384/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5832 - auc: 0.643 - ETA: 0s - loss: 0.4115 - auc: 0.609 - 0s 22us/step - loss: 0.4202 - auc: 0.5931 - val_loss: 0.4167 - val_auc: 0.6548\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.41676 to 0.41674, saving model to DeepFM.h5\n",
      "Epoch 385/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3921 - auc: 0.511 - ETA: 0s - loss: 0.4201 - auc: 0.600 - 0s 23us/step - loss: 0.4197 - auc: 0.5992 - val_loss: 0.4167 - val_auc: 0.6518\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.41674 to 0.41671, saving model to DeepFM.h5\n",
      "Epoch 386/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4492 - auc: 0.602 - ETA: 0s - loss: 0.4237 - auc: 0.585 - 0s 23us/step - loss: 0.4212 - auc: 0.5857 - val_loss: 0.4167 - val_auc: 0.6547\n",
      "\n",
      "Epoch 00386: val_loss improved from 0.41671 to 0.41668, saving model to DeepFM.h5\n",
      "Epoch 387/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4012 - auc: 0.573 - ETA: 0s - loss: 0.4249 - auc: 0.596 - 0s 24us/step - loss: 0.4206 - auc: 0.5926 - val_loss: 0.4167 - val_auc: 0.6527\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.41668 to 0.41666, saving model to DeepFM.h5\n",
      "Epoch 388/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4506 - auc: 0.600 - ETA: 0s - loss: 0.4201 - auc: 0.594 - 0s 24us/step - loss: 0.4213 - auc: 0.5814 - val_loss: 0.4166 - val_auc: 0.6544\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.41666 to 0.41663, saving model to DeepFM.h5\n",
      "Epoch 389/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4251 - auc: 0.636 - ETA: 0s - loss: 0.4210 - auc: 0.565 - 0s 23us/step - loss: 0.4211 - auc: 0.5819 - val_loss: 0.4166 - val_auc: 0.6554\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.41663 to 0.41658, saving model to DeepFM.h5\n",
      "Epoch 390/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4201 - auc: 0.605 - ETA: 0s - loss: 0.4189 - auc: 0.581 - 0s 24us/step - loss: 0.4207 - auc: 0.5892 - val_loss: 0.4165 - val_auc: 0.6544\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.41658 to 0.41655, saving model to DeepFM.h5\n",
      "Epoch 391/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3820 - auc: 0.580 - ETA: 0s - loss: 0.4148 - auc: 0.594 - 0s 23us/step - loss: 0.4189 - auc: 0.5980 - val_loss: 0.4165 - val_auc: 0.6555\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.41655 to 0.41650, saving model to DeepFM.h5\n",
      "Epoch 392/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3946 - auc: 0.587 - ETA: 0s - loss: 0.4167 - auc: 0.590 - 0s 24us/step - loss: 0.4202 - auc: 0.5951 - val_loss: 0.4165 - val_auc: 0.6541\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.41650 to 0.41646, saving model to DeepFM.h5\n",
      "Epoch 393/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4549 - auc: 0.534 - ETA: 0s - loss: 0.4268 - auc: 0.599 - 0s 27us/step - loss: 0.4208 - auc: 0.5926 - val_loss: 0.4164 - val_auc: 0.6546\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.41646 to 0.41644, saving model to DeepFM.h5\n",
      "Epoch 394/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4370 - auc: 0.486 - ETA: 0s - loss: 0.4309 - auc: 0.571 - 0s 23us/step - loss: 0.4233 - auc: 0.5602 - val_loss: 0.4164 - val_auc: 0.6551\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.41644 to 0.41642, saving model to DeepFM.h5\n",
      "Epoch 395/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3727 - auc: 0.548 - ETA: 0s - loss: 0.4339 - auc: 0.552 - 0s 25us/step - loss: 0.4211 - auc: 0.5768 - val_loss: 0.4164 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.41642 to 0.41640, saving model to DeepFM.h5\n",
      "Epoch 396/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3730 - auc: 0.530 - ETA: 0s - loss: 0.4130 - auc: 0.602 - 0s 27us/step - loss: 0.4191 - auc: 0.6027 - val_loss: 0.4164 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.41640 to 0.41637, saving model to DeepFM.h5\n",
      "Epoch 397/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4041 - auc: 0.502 - ETA: 0s - loss: 0.4253 - auc: 0.588 - 0s 23us/step - loss: 0.4213 - auc: 0.5857 - val_loss: 0.4163 - val_auc: 0.6560\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.41637 to 0.41635, saving model to DeepFM.h5\n",
      "Epoch 398/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4360 - auc: 0.663 - ETA: 0s - loss: 0.4162 - auc: 0.616 - 0s 27us/step - loss: 0.4197 - auc: 0.6035 - val_loss: 0.4163 - val_auc: 0.6569\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.41635 to 0.41633, saving model to DeepFM.h5\n",
      "Epoch 399/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3840 - auc: 0.693 - ETA: 0s - loss: 0.4182 - auc: 0.592 - 0s 22us/step - loss: 0.4196 - auc: 0.5968 - val_loss: 0.4163 - val_auc: 0.6541\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.41633 to 0.41629, saving model to DeepFM.h5\n",
      "Epoch 400/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4050 - auc: 0.515 - ETA: 0s - loss: 0.4285 - auc: 0.589 - 0s 24us/step - loss: 0.4198 - auc: 0.5927 - val_loss: 0.4163 - val_auc: 0.6557\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.41629 to 0.41625, saving model to DeepFM.h5\n",
      "Epoch 401/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4321 - auc: 0.561 - ETA: 0s - loss: 0.4208 - auc: 0.587 - 0s 26us/step - loss: 0.4206 - auc: 0.5867 - val_loss: 0.4162 - val_auc: 0.6557\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.41625 to 0.41623, saving model to DeepFM.h5\n",
      "Epoch 402/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4239 - auc: 0.686 - ETA: 0s - loss: 0.4031 - auc: 0.612 - 0s 23us/step - loss: 0.4182 - auc: 0.6086 - val_loss: 0.4162 - val_auc: 0.6548\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.41623 to 0.41619, saving model to DeepFM.h5\n",
      "Epoch 403/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3750 - auc: 0.626 - ETA: 0s - loss: 0.4037 - auc: 0.595 - 0s 27us/step - loss: 0.4185 - auc: 0.6004 - val_loss: 0.4161 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.41619 to 0.41615, saving model to DeepFM.h5\n",
      "Epoch 404/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4146 - auc: 0.637 - ETA: 0s - loss: 0.4205 - auc: 0.588 - 0s 23us/step - loss: 0.4211 - auc: 0.5842 - val_loss: 0.4161 - val_auc: 0.6552\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.41615 to 0.41612, saving model to DeepFM.h5\n",
      "Epoch 405/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4014 - auc: 0.651 - ETA: 0s - loss: 0.4280 - auc: 0.616 - 0s 23us/step - loss: 0.4186 - auc: 0.6046 - val_loss: 0.4161 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.41612 to 0.41608, saving model to DeepFM.h5\n",
      "Epoch 406/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4049 - auc: 0.643 - ETA: 0s - loss: 0.4151 - auc: 0.611 - 0s 22us/step - loss: 0.4194 - auc: 0.5988 - val_loss: 0.4160 - val_auc: 0.6534\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.41608 to 0.41605, saving model to DeepFM.h5\n",
      "Epoch 407/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3605 - auc: 0.654 - ETA: 0s - loss: 0.4187 - auc: 0.590 - 0s 21us/step - loss: 0.4213 - auc: 0.5795 - val_loss: 0.4160 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00407: val_loss improved from 0.41605 to 0.41603, saving model to DeepFM.h5\n",
      "Epoch 408/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4354 - auc: 0.569 - ETA: 0s - loss: 0.4143 - auc: 0.576 - 0s 22us/step - loss: 0.4211 - auc: 0.5832 - val_loss: 0.4160 - val_auc: 0.6543\n",
      "\n",
      "Epoch 00408: val_loss improved from 0.41603 to 0.41599, saving model to DeepFM.h5\n",
      "Epoch 409/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3853 - auc: 0.688 - ETA: 0s - loss: 0.4200 - auc: 0.593 - 0s 21us/step - loss: 0.4188 - auc: 0.6008 - val_loss: 0.4160 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.41599 to 0.41596, saving model to DeepFM.h5\n",
      "Epoch 410/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4121 - auc: 0.550 - ETA: 0s - loss: 0.4286 - auc: 0.569 - 0s 23us/step - loss: 0.4215 - auc: 0.5819 - val_loss: 0.4159 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00410: val_loss improved from 0.41596 to 0.41595, saving model to DeepFM.h5\n",
      "Epoch 411/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4224 - auc: 0.595 - ETA: 0s - loss: 0.4161 - auc: 0.610 - 0s 22us/step - loss: 0.4177 - auc: 0.6116 - val_loss: 0.4159 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.41595 to 0.41592, saving model to DeepFM.h5\n",
      "Epoch 412/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4198 - auc: 0.544 - ETA: 0s - loss: 0.4137 - auc: 0.588 - 0s 21us/step - loss: 0.4201 - auc: 0.5895 - val_loss: 0.4159 - val_auc: 0.6537\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.41592 to 0.41589, saving model to DeepFM.h5\n",
      "Epoch 413/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3799 - auc: 0.602 - ETA: 0s - loss: 0.4208 - auc: 0.592 - 0s 21us/step - loss: 0.4196 - auc: 0.5890 - val_loss: 0.4159 - val_auc: 0.6516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00413: val_loss improved from 0.41589 to 0.41586, saving model to DeepFM.h5\n",
      "Epoch 414/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4612 - auc: 0.536 - ETA: 0s - loss: 0.4179 - auc: 0.583 - 0s 21us/step - loss: 0.4211 - auc: 0.5872 - val_loss: 0.4158 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00414: val_loss improved from 0.41586 to 0.41583, saving model to DeepFM.h5\n",
      "Epoch 415/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4312 - auc: 0.632 - ETA: 0s - loss: 0.4237 - auc: 0.602 - 0s 22us/step - loss: 0.4203 - auc: 0.5886 - val_loss: 0.4158 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00415: val_loss improved from 0.41583 to 0.41580, saving model to DeepFM.h5\n",
      "Epoch 416/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4289 - auc: 0.569 - ETA: 0s - loss: 0.4231 - auc: 0.594 - 0s 23us/step - loss: 0.4211 - auc: 0.5818 - val_loss: 0.4158 - val_auc: 0.6512\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.41580 to 0.41577, saving model to DeepFM.h5\n",
      "Epoch 417/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4280 - auc: 0.582 - ETA: 0s - loss: 0.4239 - auc: 0.587 - 0s 22us/step - loss: 0.4203 - auc: 0.5880 - val_loss: 0.4157 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.41577 to 0.41572, saving model to DeepFM.h5\n",
      "Epoch 418/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.586 - ETA: 0s - loss: 0.4225 - auc: 0.609 - 0s 23us/step - loss: 0.4179 - auc: 0.6118 - val_loss: 0.4157 - val_auc: 0.6509\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.41572 to 0.41570, saving model to DeepFM.h5\n",
      "Epoch 419/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4335 - auc: 0.549 - ETA: 0s - loss: 0.4210 - auc: 0.599 - 0s 21us/step - loss: 0.4179 - auc: 0.5993 - val_loss: 0.4157 - val_auc: 0.6508\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.41570 to 0.41566, saving model to DeepFM.h5\n",
      "Epoch 420/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4476 - auc: 0.587 - ETA: 0s - loss: 0.4198 - auc: 0.582 - 0s 22us/step - loss: 0.4206 - auc: 0.5832 - val_loss: 0.4156 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00420: val_loss improved from 0.41566 to 0.41564, saving model to DeepFM.h5\n",
      "Epoch 421/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3603 - auc: 0.577 - ETA: 0s - loss: 0.4194 - auc: 0.583 - 0s 23us/step - loss: 0.4201 - auc: 0.5918 - val_loss: 0.4156 - val_auc: 0.6551\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.41564 to 0.41561, saving model to DeepFM.h5\n",
      "Epoch 422/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4634 - auc: 0.600 - ETA: 0s - loss: 0.4139 - auc: 0.604 - 0s 22us/step - loss: 0.4195 - auc: 0.5991 - val_loss: 0.4156 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00422: val_loss improved from 0.41561 to 0.41559, saving model to DeepFM.h5\n",
      "Epoch 423/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4371 - auc: 0.608 - ETA: 0s - loss: 0.4259 - auc: 0.630 - 0s 23us/step - loss: 0.4166 - auc: 0.6203 - val_loss: 0.4156 - val_auc: 0.6554\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.41559 to 0.41555, saving model to DeepFM.h5\n",
      "Epoch 424/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4051 - auc: 0.529 - ETA: 0s - loss: 0.4104 - auc: 0.580 - 0s 22us/step - loss: 0.4207 - auc: 0.5792 - val_loss: 0.4155 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.41555 to 0.41552, saving model to DeepFM.h5\n",
      "Epoch 425/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4144 - auc: 0.529 - ETA: 0s - loss: 0.4206 - auc: 0.571 - 0s 21us/step - loss: 0.4209 - auc: 0.5813 - val_loss: 0.4155 - val_auc: 0.6544\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.41552 to 0.41550, saving model to DeepFM.h5\n",
      "Epoch 426/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3301 - auc: 0.637 - ETA: 0s - loss: 0.4191 - auc: 0.595 - 0s 22us/step - loss: 0.4190 - auc: 0.5981 - val_loss: 0.4155 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.41550 to 0.41547, saving model to DeepFM.h5\n",
      "Epoch 427/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4448 - auc: 0.541 - ETA: 0s - loss: 0.4308 - auc: 0.586 - 0s 23us/step - loss: 0.4203 - auc: 0.5904 - val_loss: 0.4154 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.41547 to 0.41544, saving model to DeepFM.h5\n",
      "Epoch 428/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4251 - auc: 0.604 - ETA: 0s - loss: 0.4213 - auc: 0.602 - 0s 24us/step - loss: 0.4176 - auc: 0.6093 - val_loss: 0.4154 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.41544 to 0.41540, saving model to DeepFM.h5\n",
      "Epoch 429/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3811 - auc: 0.648 - ETA: 0s - loss: 0.4100 - auc: 0.593 - 0s 22us/step - loss: 0.4188 - auc: 0.5961 - val_loss: 0.4154 - val_auc: 0.6515\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.41540 to 0.41536, saving model to DeepFM.h5\n",
      "Epoch 430/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4545 - auc: 0.544 - ETA: 0s - loss: 0.4210 - auc: 0.605 - 0s 22us/step - loss: 0.4181 - auc: 0.6042 - val_loss: 0.4153 - val_auc: 0.6532\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.41536 to 0.41534, saving model to DeepFM.h5\n",
      "Epoch 431/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3500 - auc: 0.653 - ETA: 0s - loss: 0.4176 - auc: 0.597 - 0s 23us/step - loss: 0.4181 - auc: 0.6088 - val_loss: 0.4153 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00431: val_loss improved from 0.41534 to 0.41530, saving model to DeepFM.h5\n",
      "Epoch 432/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3889 - auc: 0.505 - ETA: 0s - loss: 0.4147 - auc: 0.582 - 0s 24us/step - loss: 0.4181 - auc: 0.6008 - val_loss: 0.4153 - val_auc: 0.6521\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.41530 to 0.41525, saving model to DeepFM.h5\n",
      "Epoch 433/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3470 - auc: 0.650 - ETA: 0s - loss: 0.4225 - auc: 0.588 - 0s 21us/step - loss: 0.4197 - auc: 0.5939 - val_loss: 0.4152 - val_auc: 0.6511\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.41525 to 0.41521, saving model to DeepFM.h5\n",
      "Epoch 434/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3984 - auc: 0.598 - ETA: 0s - loss: 0.4125 - auc: 0.586 - 0s 22us/step - loss: 0.4201 - auc: 0.5816 - val_loss: 0.4152 - val_auc: 0.6512\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.41521 to 0.41519, saving model to DeepFM.h5\n",
      "Epoch 435/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4166 - auc: 0.632 - ETA: 0s - loss: 0.4391 - auc: 0.603 - 0s 22us/step - loss: 0.4190 - auc: 0.6021 - val_loss: 0.4152 - val_auc: 0.6511\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.41519 to 0.41515, saving model to DeepFM.h5\n",
      "Epoch 436/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.542 - ETA: 0s - loss: 0.4263 - auc: 0.592 - 0s 24us/step - loss: 0.4195 - auc: 0.5870 - val_loss: 0.4151 - val_auc: 0.6495\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.41515 to 0.41512, saving model to DeepFM.h5\n",
      "Epoch 437/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3985 - auc: 0.620 - ETA: 0s - loss: 0.4170 - auc: 0.567 - 0s 23us/step - loss: 0.4207 - auc: 0.5858 - val_loss: 0.4151 - val_auc: 0.6508\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.41512 to 0.41511, saving model to DeepFM.h5\n",
      "Epoch 438/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4032 - auc: 0.624 - ETA: 0s - loss: 0.4183 - auc: 0.623 - 0s 20us/step - loss: 0.4157 - auc: 0.6233 - val_loss: 0.4151 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.41511 to 0.41507, saving model to DeepFM.h5\n",
      "Epoch 439/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3997 - auc: 0.656 - ETA: 0s - loss: 0.4273 - auc: 0.590 - 0s 22us/step - loss: 0.4190 - auc: 0.5890 - val_loss: 0.4150 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.41507 to 0.41503, saving model to DeepFM.h5\n",
      "Epoch 440/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3950 - auc: 0.704 - ETA: 0s - loss: 0.4147 - auc: 0.608 - 0s 23us/step - loss: 0.4183 - auc: 0.6033 - val_loss: 0.4150 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.41503 to 0.41500, saving model to DeepFM.h5\n",
      "Epoch 441/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4788 - auc: 0.594 - ETA: 0s - loss: 0.4159 - auc: 0.595 - 0s 23us/step - loss: 0.4188 - auc: 0.5987 - val_loss: 0.4150 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.41500 to 0.41496, saving model to DeepFM.h5\n",
      "Epoch 442/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4863 - auc: 0.552 - ETA: 0s - loss: 0.4281 - auc: 0.591 - 0s 22us/step - loss: 0.4195 - auc: 0.5930 - val_loss: 0.4149 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.41496 to 0.41494, saving model to DeepFM.h5\n",
      "Epoch 443/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4471 - auc: 0.612 - ETA: 0s - loss: 0.4267 - auc: 0.576 - 0s 22us/step - loss: 0.4210 - auc: 0.5756 - val_loss: 0.4149 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00443: val_loss improved from 0.41494 to 0.41491, saving model to DeepFM.h5\n",
      "Epoch 444/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4030 - auc: 0.741 - ETA: 0s - loss: 0.4180 - auc: 0.617 - 0s 23us/step - loss: 0.4173 - auc: 0.6129 - val_loss: 0.4149 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.41491 to 0.41488, saving model to DeepFM.h5\n",
      "Epoch 445/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3924 - auc: 0.600 - ETA: 0s - loss: 0.4115 - auc: 0.613 - 0s 21us/step - loss: 0.4169 - auc: 0.6139 - val_loss: 0.4148 - val_auc: 0.6534\n",
      "\n",
      "Epoch 00445: val_loss improved from 0.41488 to 0.41483, saving model to DeepFM.h5\n",
      "Epoch 446/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4150 - auc: 0.579 - ETA: 0s - loss: 0.4165 - auc: 0.591 - 0s 22us/step - loss: 0.4206 - auc: 0.5877 - val_loss: 0.4148 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.41483 to 0.41480, saving model to DeepFM.h5\n",
      "Epoch 447/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4711 - auc: 0.586 - ETA: 0s - loss: 0.4147 - auc: 0.589 - 0s 22us/step - loss: 0.4183 - auc: 0.6004 - val_loss: 0.4148 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.41480 to 0.41477, saving model to DeepFM.h5\n",
      "Epoch 448/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4181 - auc: 0.603 - ETA: 0s - loss: 0.4151 - auc: 0.597 - 0s 23us/step - loss: 0.4180 - auc: 0.6000 - val_loss: 0.4147 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.41477 to 0.41474, saving model to DeepFM.h5\n",
      "Epoch 449/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4773 - auc: 0.568 - ETA: 0s - loss: 0.4191 - auc: 0.615 - 0s 22us/step - loss: 0.4178 - auc: 0.6013 - val_loss: 0.4147 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.41474 to 0.41470, saving model to DeepFM.h5\n",
      "Epoch 450/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4387 - auc: 0.631 - ETA: 0s - loss: 0.4055 - auc: 0.619 - 0s 25us/step - loss: 0.4160 - auc: 0.6175 - val_loss: 0.4147 - val_auc: 0.6514\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.41470 to 0.41467, saving model to DeepFM.h5\n",
      "Epoch 451/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4253 - auc: 0.579 - ETA: 0s - loss: 0.4222 - auc: 0.593 - 0s 21us/step - loss: 0.4176 - auc: 0.6013 - val_loss: 0.4146 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.41467 to 0.41464, saving model to DeepFM.h5\n",
      "Epoch 452/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4225 - auc: 0.571 - ETA: 0s - loss: 0.4111 - auc: 0.606 - 0s 22us/step - loss: 0.4164 - auc: 0.6080 - val_loss: 0.4146 - val_auc: 0.6503\n",
      "\n",
      "Epoch 00452: val_loss improved from 0.41464 to 0.41460, saving model to DeepFM.h5\n",
      "Epoch 453/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4140 - auc: 0.520 - ETA: 0s - loss: 0.4078 - auc: 0.612 - 0s 22us/step - loss: 0.4187 - auc: 0.6024 - val_loss: 0.4146 - val_auc: 0.6501\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.41460 to 0.41457, saving model to DeepFM.h5\n",
      "Epoch 454/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4011 - auc: 0.654 - ETA: 0s - loss: 0.4237 - auc: 0.603 - 0s 22us/step - loss: 0.4181 - auc: 0.5999 - val_loss: 0.4145 - val_auc: 0.6504\n",
      "\n",
      "Epoch 00454: val_loss improved from 0.41457 to 0.41455, saving model to DeepFM.h5\n",
      "Epoch 455/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3604 - auc: 0.606 - ETA: 0s - loss: 0.4091 - auc: 0.610 - 0s 21us/step - loss: 0.4182 - auc: 0.5995 - val_loss: 0.4145 - val_auc: 0.6515\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.41455 to 0.41453, saving model to DeepFM.h5\n",
      "Epoch 456/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4170 - auc: 0.615 - ETA: 0s - loss: 0.4199 - auc: 0.623 - 0s 21us/step - loss: 0.4164 - auc: 0.6166 - val_loss: 0.4145 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.41453 to 0.41450, saving model to DeepFM.h5\n",
      "Epoch 457/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3961 - auc: 0.643 - ETA: 0s - loss: 0.4323 - auc: 0.596 - 0s 22us/step - loss: 0.4190 - auc: 0.5959 - val_loss: 0.4145 - val_auc: 0.6503\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.41450 to 0.41448, saving model to DeepFM.h5\n",
      "Epoch 458/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4466 - auc: 0.603 - ETA: 0s - loss: 0.4050 - auc: 0.595 - 0s 22us/step - loss: 0.4182 - auc: 0.6019 - val_loss: 0.4144 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.41448 to 0.41445, saving model to DeepFM.h5\n",
      "Epoch 459/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4516 - auc: 0.637 - ETA: 0s - loss: 0.4217 - auc: 0.579 - 0s 22us/step - loss: 0.4183 - auc: 0.6025 - val_loss: 0.4144 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.41445 to 0.41442, saving model to DeepFM.h5\n",
      "Epoch 460/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4792 - auc: 0.583 - ETA: 0s - loss: 0.4132 - auc: 0.606 - 0s 23us/step - loss: 0.4164 - auc: 0.6150 - val_loss: 0.4144 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00460: val_loss improved from 0.41442 to 0.41439, saving model to DeepFM.h5\n",
      "Epoch 461/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3923 - auc: 0.648 - ETA: 0s - loss: 0.4188 - auc: 0.620 - 0s 22us/step - loss: 0.4166 - auc: 0.6121 - val_loss: 0.4144 - val_auc: 0.6537\n",
      "\n",
      "Epoch 00461: val_loss improved from 0.41439 to 0.41435, saving model to DeepFM.h5\n",
      "Epoch 462/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4490 - auc: 0.581 - ETA: 0s - loss: 0.4250 - auc: 0.608 - 0s 22us/step - loss: 0.4174 - auc: 0.6130 - val_loss: 0.4143 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.41435 to 0.41433, saving model to DeepFM.h5\n",
      "Epoch 463/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.626 - ETA: 0s - loss: 0.4240 - auc: 0.596 - 0s 22us/step - loss: 0.4192 - auc: 0.5975 - val_loss: 0.4143 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00463: val_loss improved from 0.41433 to 0.41431, saving model to DeepFM.h5\n",
      "Epoch 464/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4558 - auc: 0.599 - ETA: 0s - loss: 0.4235 - auc: 0.592 - 0s 22us/step - loss: 0.4180 - auc: 0.6125 - val_loss: 0.4143 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.41431 to 0.41428, saving model to DeepFM.h5\n",
      "Epoch 465/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3636 - auc: 0.638 - ETA: 0s - loss: 0.4105 - auc: 0.601 - 0s 22us/step - loss: 0.4192 - auc: 0.5905 - val_loss: 0.4143 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.41428 to 0.41427, saving model to DeepFM.h5\n",
      "Epoch 466/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4224 - auc: 0.587 - ETA: 0s - loss: 0.4241 - auc: 0.583 - 0s 23us/step - loss: 0.4186 - auc: 0.5980 - val_loss: 0.4142 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.41427 to 0.41425, saving model to DeepFM.h5\n",
      "Epoch 467/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4268 - auc: 0.615 - ETA: 0s - loss: 0.4234 - auc: 0.617 - 0s 23us/step - loss: 0.4172 - auc: 0.6122 - val_loss: 0.4142 - val_auc: 0.6515\n",
      "\n",
      "Epoch 00467: val_loss improved from 0.41425 to 0.41421, saving model to DeepFM.h5\n",
      "Epoch 468/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3917 - auc: 0.521 - ETA: 0s - loss: 0.4200 - auc: 0.587 - 0s 23us/step - loss: 0.4197 - auc: 0.5921 - val_loss: 0.4142 - val_auc: 0.6513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00468: val_loss improved from 0.41421 to 0.41419, saving model to DeepFM.h5\n",
      "Epoch 469/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3430 - auc: 0.662 - ETA: 0s - loss: 0.4124 - auc: 0.619 - 0s 22us/step - loss: 0.4166 - auc: 0.6156 - val_loss: 0.4142 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00469: val_loss improved from 0.41419 to 0.41415, saving model to DeepFM.h5\n",
      "Epoch 470/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4624 - auc: 0.549 - ETA: 0s - loss: 0.4213 - auc: 0.634 - 0s 24us/step - loss: 0.4159 - auc: 0.6291 - val_loss: 0.4141 - val_auc: 0.6539\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.41415 to 0.41411, saving model to DeepFM.h5\n",
      "Epoch 471/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3994 - auc: 0.609 - ETA: 0s - loss: 0.4146 - auc: 0.597 - 0s 22us/step - loss: 0.4185 - auc: 0.6027 - val_loss: 0.4141 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.41411 to 0.41409, saving model to DeepFM.h5\n",
      "Epoch 472/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4518 - auc: 0.649 - ETA: 0s - loss: 0.4289 - auc: 0.597 - 0s 22us/step - loss: 0.4188 - auc: 0.5941 - val_loss: 0.4141 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00472: val_loss improved from 0.41409 to 0.41406, saving model to DeepFM.h5\n",
      "Epoch 473/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4053 - auc: 0.606 - ETA: 0s - loss: 0.4202 - auc: 0.597 - 0s 20us/step - loss: 0.4182 - auc: 0.6041 - val_loss: 0.4140 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.41406 to 0.41403, saving model to DeepFM.h5\n",
      "Epoch 474/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3918 - auc: 0.581 - ETA: 0s - loss: 0.4177 - auc: 0.602 - 0s 23us/step - loss: 0.4172 - auc: 0.6052 - val_loss: 0.4140 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.41403 to 0.41399, saving model to DeepFM.h5\n",
      "Epoch 475/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3480 - auc: 0.588 - ETA: 0s - loss: 0.4241 - auc: 0.586 - 0s 21us/step - loss: 0.4200 - auc: 0.5883 - val_loss: 0.4140 - val_auc: 0.6550\n",
      "\n",
      "Epoch 00475: val_loss improved from 0.41399 to 0.41397, saving model to DeepFM.h5\n",
      "Epoch 476/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4270 - auc: 0.652 - ETA: 0s - loss: 0.4279 - auc: 0.596 - 0s 22us/step - loss: 0.4172 - auc: 0.6069 - val_loss: 0.4139 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.41397 to 0.41393, saving model to DeepFM.h5\n",
      "Epoch 477/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4441 - auc: 0.527 - ETA: 0s - loss: 0.4209 - auc: 0.626 - 0s 22us/step - loss: 0.4166 - auc: 0.6139 - val_loss: 0.4139 - val_auc: 0.6551\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.41393 to 0.41391, saving model to DeepFM.h5\n",
      "Epoch 478/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4211 - auc: 0.644 - ETA: 0s - loss: 0.4098 - auc: 0.628 - 0s 21us/step - loss: 0.4178 - auc: 0.6056 - val_loss: 0.4139 - val_auc: 0.6548\n",
      "\n",
      "Epoch 00478: val_loss improved from 0.41391 to 0.41388, saving model to DeepFM.h5\n",
      "Epoch 479/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.563 - ETA: 0s - loss: 0.4190 - auc: 0.595 - 0s 22us/step - loss: 0.4181 - auc: 0.5972 - val_loss: 0.4139 - val_auc: 0.6550\n",
      "\n",
      "Epoch 00479: val_loss improved from 0.41388 to 0.41386, saving model to DeepFM.h5\n",
      "Epoch 480/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4026 - auc: 0.680 - ETA: 0s - loss: 0.4224 - auc: 0.592 - 0s 22us/step - loss: 0.4172 - auc: 0.6108 - val_loss: 0.4138 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00480: val_loss improved from 0.41386 to 0.41383, saving model to DeepFM.h5\n",
      "Epoch 481/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3895 - auc: 0.666 - ETA: 0s - loss: 0.4133 - auc: 0.601 - 0s 21us/step - loss: 0.4177 - auc: 0.6047 - val_loss: 0.4138 - val_auc: 0.6554\n",
      "\n",
      "Epoch 00481: val_loss improved from 0.41383 to 0.41379, saving model to DeepFM.h5\n",
      "Epoch 482/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4133 - auc: 0.515 - ETA: 0s - loss: 0.4173 - auc: 0.599 - 0s 22us/step - loss: 0.4168 - auc: 0.6115 - val_loss: 0.4138 - val_auc: 0.6555\n",
      "\n",
      "Epoch 00482: val_loss improved from 0.41379 to 0.41375, saving model to DeepFM.h5\n",
      "Epoch 483/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4169 - auc: 0.540 - ETA: 0s - loss: 0.4170 - auc: 0.619 - 0s 27us/step - loss: 0.4159 - auc: 0.6195 - val_loss: 0.4137 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.41375 to 0.41371, saving model to DeepFM.h5\n",
      "Epoch 484/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4578 - auc: 0.618 - ETA: 0s - loss: 0.4184 - auc: 0.572 - 0s 26us/step - loss: 0.4196 - auc: 0.5863 - val_loss: 0.4137 - val_auc: 0.6558\n",
      "\n",
      "Epoch 00484: val_loss improved from 0.41371 to 0.41368, saving model to DeepFM.h5\n",
      "Epoch 485/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4138 - auc: 0.573 - ETA: 0s - loss: 0.4327 - auc: 0.609 - 0s 25us/step - loss: 0.4178 - auc: 0.6060 - val_loss: 0.4137 - val_auc: 0.6548\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.41368 to 0.41365, saving model to DeepFM.h5\n",
      "Epoch 486/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4072 - auc: 0.737 - ETA: 0s - loss: 0.4135 - auc: 0.606 - 0s 21us/step - loss: 0.4181 - auc: 0.5988 - val_loss: 0.4136 - val_auc: 0.6547\n",
      "\n",
      "Epoch 00486: val_loss improved from 0.41365 to 0.41364, saving model to DeepFM.h5\n",
      "Epoch 487/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4301 - auc: 0.571 - ETA: 0s - loss: 0.4113 - auc: 0.619 - 0s 26us/step - loss: 0.4166 - auc: 0.6171 - val_loss: 0.4136 - val_auc: 0.6553\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.41364 to 0.41361, saving model to DeepFM.h5\n",
      "Epoch 488/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4712 - auc: 0.631 - ETA: 0s - loss: 0.4105 - auc: 0.597 - 0s 25us/step - loss: 0.4160 - auc: 0.6080 - val_loss: 0.4136 - val_auc: 0.6543\n",
      "\n",
      "Epoch 00488: val_loss improved from 0.41361 to 0.41356, saving model to DeepFM.h5\n",
      "Epoch 489/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3681 - auc: 0.662 - ETA: 0s - loss: 0.4171 - auc: 0.605 - 0s 25us/step - loss: 0.4184 - auc: 0.5918 - val_loss: 0.4135 - val_auc: 0.6537\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.41356 to 0.41354, saving model to DeepFM.h5\n",
      "Epoch 490/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3885 - auc: 0.578 - ETA: 0s - loss: 0.4308 - auc: 0.625 - 0s 23us/step - loss: 0.4145 - auc: 0.6249 - val_loss: 0.4135 - val_auc: 0.6535\n",
      "\n",
      "Epoch 00490: val_loss improved from 0.41354 to 0.41352, saving model to DeepFM.h5\n",
      "Epoch 491/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3250 - auc: 0.649 - ETA: 0s - loss: 0.4224 - auc: 0.582 - 0s 24us/step - loss: 0.4183 - auc: 0.5987 - val_loss: 0.4135 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00491: val_loss improved from 0.41352 to 0.41350, saving model to DeepFM.h5\n",
      "Epoch 492/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.508 - ETA: 0s - loss: 0.4138 - auc: 0.592 - 0s 22us/step - loss: 0.4173 - auc: 0.6020 - val_loss: 0.4135 - val_auc: 0.6530\n",
      "\n",
      "Epoch 00492: val_loss improved from 0.41350 to 0.41347, saving model to DeepFM.h5\n",
      "Epoch 493/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3869 - auc: 0.649 - ETA: 0s - loss: 0.4241 - auc: 0.612 - 0s 22us/step - loss: 0.4175 - auc: 0.6045 - val_loss: 0.4134 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.41347 to 0.41343, saving model to DeepFM.h5\n",
      "Epoch 494/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3905 - auc: 0.650 - ETA: 0s - loss: 0.4151 - auc: 0.599 - 0s 23us/step - loss: 0.4188 - auc: 0.5993 - val_loss: 0.4134 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.41343 to 0.41341, saving model to DeepFM.h5\n",
      "Epoch 495/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4770 - auc: 0.694 - ETA: 0s - loss: 0.4110 - auc: 0.595 - 0s 26us/step - loss: 0.4175 - auc: 0.6065 - val_loss: 0.4134 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00495: val_loss improved from 0.41341 to 0.41338, saving model to DeepFM.h5\n",
      "Epoch 496/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4277 - auc: 0.635 - ETA: 0s - loss: 0.4407 - auc: 0.619 - 0s 27us/step - loss: 0.4154 - auc: 0.6200 - val_loss: 0.4133 - val_auc: 0.6553\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.41338 to 0.41335, saving model to DeepFM.h5\n",
      "Epoch 497/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3973 - auc: 0.633 - ETA: 0s - loss: 0.4177 - auc: 0.588 - 0s 29us/step - loss: 0.4191 - auc: 0.5891 - val_loss: 0.4133 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.41335 to 0.41333, saving model to DeepFM.h5\n",
      "Epoch 498/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3596 - auc: 0.650 - ETA: 0s - loss: 0.4105 - auc: 0.615 - 0s 28us/step - loss: 0.4167 - auc: 0.6098 - val_loss: 0.4133 - val_auc: 0.6547\n",
      "\n",
      "Epoch 00498: val_loss improved from 0.41333 to 0.41329, saving model to DeepFM.h5\n",
      "Epoch 499/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3972 - auc: 0.496 - ETA: 0s - loss: 0.4113 - auc: 0.585 - 0s 23us/step - loss: 0.4205 - auc: 0.5810 - val_loss: 0.4133 - val_auc: 0.6527\n",
      "\n",
      "Epoch 00499: val_loss improved from 0.41329 to 0.41326, saving model to DeepFM.h5\n",
      "Epoch 500/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4151 - auc: 0.589 - ETA: 0s - loss: 0.4248 - auc: 0.609 - 0s 23us/step - loss: 0.4155 - auc: 0.6231 - val_loss: 0.4132 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.41326 to 0.41323, saving model to DeepFM.h5\n",
      "Epoch 501/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4157 - auc: 0.593 - ETA: 0s - loss: 0.4141 - auc: 0.616 - 0s 23us/step - loss: 0.4154 - auc: 0.6170 - val_loss: 0.4132 - val_auc: 0.6546\n",
      "\n",
      "Epoch 00501: val_loss improved from 0.41323 to 0.41320, saving model to DeepFM.h5\n",
      "Epoch 502/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4469 - auc: 0.628 - ETA: 0s - loss: 0.4290 - auc: 0.632 - 0s 23us/step - loss: 0.4154 - auc: 0.6178 - val_loss: 0.4132 - val_auc: 0.6555\n",
      "\n",
      "Epoch 00502: val_loss improved from 0.41320 to 0.41317, saving model to DeepFM.h5\n",
      "Epoch 503/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3785 - auc: 0.586 - ETA: 0s - loss: 0.4344 - auc: 0.592 - 0s 25us/step - loss: 0.4170 - auc: 0.6026 - val_loss: 0.4131 - val_auc: 0.6554\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.41317 to 0.41314, saving model to DeepFM.h5\n",
      "Epoch 504/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4300 - auc: 0.686 - ETA: 0s - loss: 0.4094 - auc: 0.626 - 0s 26us/step - loss: 0.4157 - auc: 0.6191 - val_loss: 0.4131 - val_auc: 0.6548\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.41314 to 0.41309, saving model to DeepFM.h5\n",
      "Epoch 505/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4048 - auc: 0.680 - ETA: 0s - loss: 0.4107 - auc: 0.627 - 0s 23us/step - loss: 0.4164 - auc: 0.6181 - val_loss: 0.4131 - val_auc: 0.6553\n",
      "\n",
      "Epoch 00505: val_loss improved from 0.41309 to 0.41306, saving model to DeepFM.h5\n",
      "Epoch 506/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4254 - auc: 0.546 - ETA: 0s - loss: 0.4239 - auc: 0.612 - 0s 21us/step - loss: 0.4155 - auc: 0.6150 - val_loss: 0.4130 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00506: val_loss improved from 0.41306 to 0.41303, saving model to DeepFM.h5\n",
      "Epoch 507/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4368 - auc: 0.629 - ETA: 0s - loss: 0.4329 - auc: 0.608 - 0s 21us/step - loss: 0.4174 - auc: 0.6015 - val_loss: 0.4130 - val_auc: 0.6571\n",
      "\n",
      "Epoch 00507: val_loss improved from 0.41303 to 0.41299, saving model to DeepFM.h5\n",
      "Epoch 508/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5235 - auc: 0.536 - ETA: 0s - loss: 0.4242 - auc: 0.612 - 0s 24us/step - loss: 0.4169 - auc: 0.6089 - val_loss: 0.4130 - val_auc: 0.6564\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.41299 to 0.41297, saving model to DeepFM.h5\n",
      "Epoch 509/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3952 - auc: 0.640 - ETA: 0s - loss: 0.4021 - auc: 0.607 - 0s 22us/step - loss: 0.4154 - auc: 0.6197 - val_loss: 0.4129 - val_auc: 0.6567\n",
      "\n",
      "Epoch 00509: val_loss improved from 0.41297 to 0.41293, saving model to DeepFM.h5\n",
      "Epoch 510/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3890 - auc: 0.664 - ETA: 0s - loss: 0.4109 - auc: 0.607 - 0s 21us/step - loss: 0.4181 - auc: 0.5996 - val_loss: 0.4129 - val_auc: 0.6574\n",
      "\n",
      "Epoch 00510: val_loss improved from 0.41293 to 0.41292, saving model to DeepFM.h5\n",
      "Epoch 511/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4277 - auc: 0.700 - ETA: 0s - loss: 0.4277 - auc: 0.623 - 0s 22us/step - loss: 0.4140 - auc: 0.6283 - val_loss: 0.4129 - val_auc: 0.6570\n",
      "\n",
      "Epoch 00511: val_loss improved from 0.41292 to 0.41288, saving model to DeepFM.h5\n",
      "Epoch 512/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4585 - auc: 0.661 - ETA: 0s - loss: 0.4210 - auc: 0.619 - 0s 21us/step - loss: 0.4160 - auc: 0.6130 - val_loss: 0.4128 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00512: val_loss improved from 0.41288 to 0.41284, saving model to DeepFM.h5\n",
      "Epoch 513/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4118 - auc: 0.680 - ETA: 0s - loss: 0.4124 - auc: 0.608 - 0s 23us/step - loss: 0.4154 - auc: 0.6185 - val_loss: 0.4128 - val_auc: 0.6578\n",
      "\n",
      "Epoch 00513: val_loss improved from 0.41284 to 0.41281, saving model to DeepFM.h5\n",
      "Epoch 514/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4078 - auc: 0.640 - ETA: 0s - loss: 0.4226 - auc: 0.611 - 0s 22us/step - loss: 0.4155 - auc: 0.6187 - val_loss: 0.4128 - val_auc: 0.6584\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.41281 to 0.41279, saving model to DeepFM.h5\n",
      "Epoch 515/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4135 - auc: 0.638 - ETA: 0s - loss: 0.4245 - auc: 0.599 - 0s 25us/step - loss: 0.4178 - auc: 0.6012 - val_loss: 0.4128 - val_auc: 0.6589\n",
      "\n",
      "Epoch 00515: val_loss improved from 0.41279 to 0.41277, saving model to DeepFM.h5\n",
      "Epoch 516/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4362 - auc: 0.635 - ETA: 0s - loss: 0.4247 - auc: 0.616 - 0s 24us/step - loss: 0.4152 - auc: 0.6221 - val_loss: 0.4128 - val_auc: 0.6604\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.41277 to 0.41276, saving model to DeepFM.h5\n",
      "Epoch 517/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4576 - auc: 0.618 - ETA: 0s - loss: 0.4266 - auc: 0.626 - 0s 25us/step - loss: 0.4151 - auc: 0.6185 - val_loss: 0.4127 - val_auc: 0.6586\n",
      "\n",
      "Epoch 00517: val_loss improved from 0.41276 to 0.41273, saving model to DeepFM.h5\n",
      "Epoch 518/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4173 - auc: 0.603 - ETA: 0s - loss: 0.4124 - auc: 0.593 - 0s 23us/step - loss: 0.4169 - auc: 0.6094 - val_loss: 0.4127 - val_auc: 0.6584\n",
      "\n",
      "Epoch 00518: val_loss improved from 0.41273 to 0.41270, saving model to DeepFM.h5\n",
      "Epoch 519/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3890 - auc: 0.649 - ETA: 0s - loss: 0.4198 - auc: 0.610 - 0s 21us/step - loss: 0.4155 - auc: 0.6174 - val_loss: 0.4127 - val_auc: 0.6586\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.41270 to 0.41266, saving model to DeepFM.h5\n",
      "Epoch 520/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3588 - auc: 0.502 - ETA: 0s - loss: 0.4097 - auc: 0.598 - 0s 21us/step - loss: 0.4157 - auc: 0.6146 - val_loss: 0.4126 - val_auc: 0.6557\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.41266 to 0.41262, saving model to DeepFM.h5\n",
      "Epoch 521/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3892 - auc: 0.633 - ETA: 0s - loss: 0.4192 - auc: 0.597 - 0s 23us/step - loss: 0.4172 - auc: 0.6053 - val_loss: 0.4126 - val_auc: 0.6562\n",
      "\n",
      "Epoch 00521: val_loss improved from 0.41262 to 0.41260, saving model to DeepFM.h5\n",
      "Epoch 522/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4410 - auc: 0.658 - ETA: 0s - loss: 0.4188 - auc: 0.608 - 0s 21us/step - loss: 0.4173 - auc: 0.5999 - val_loss: 0.4126 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.41260 to 0.41257, saving model to DeepFM.h5\n",
      "Epoch 523/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.693 - ETA: 0s - loss: 0.4146 - auc: 0.637 - 0s 22us/step - loss: 0.4143 - auc: 0.6265 - val_loss: 0.4125 - val_auc: 0.6540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00523: val_loss improved from 0.41257 to 0.41253, saving model to DeepFM.h5\n",
      "Epoch 524/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3851 - auc: 0.641 - ETA: 0s - loss: 0.4168 - auc: 0.599 - 0s 22us/step - loss: 0.4177 - auc: 0.6003 - val_loss: 0.4125 - val_auc: 0.6531\n",
      "\n",
      "Epoch 00524: val_loss improved from 0.41253 to 0.41250, saving model to DeepFM.h5\n",
      "Epoch 525/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3733 - auc: 0.554 - ETA: 0s - loss: 0.4139 - auc: 0.607 - 0s 22us/step - loss: 0.4175 - auc: 0.5989 - val_loss: 0.4125 - val_auc: 0.6533\n",
      "\n",
      "Epoch 00525: val_loss improved from 0.41250 to 0.41248, saving model to DeepFM.h5\n",
      "Epoch 526/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4115 - auc: 0.707 - ETA: 0s - loss: 0.4115 - auc: 0.607 - 0s 21us/step - loss: 0.4157 - auc: 0.6129 - val_loss: 0.4124 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00526: val_loss improved from 0.41248 to 0.41245, saving model to DeepFM.h5\n",
      "Epoch 527/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4363 - auc: 0.661 - ETA: 0s - loss: 0.4230 - auc: 0.607 - 0s 22us/step - loss: 0.4166 - auc: 0.6069 - val_loss: 0.4124 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00527: val_loss improved from 0.41245 to 0.41241, saving model to DeepFM.h5\n",
      "Epoch 528/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4335 - auc: 0.663 - ETA: 0s - loss: 0.4174 - auc: 0.604 - 0s 24us/step - loss: 0.4166 - auc: 0.6090 - val_loss: 0.4124 - val_auc: 0.6550\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.41241 to 0.41239, saving model to DeepFM.h5\n",
      "Epoch 529/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4138 - auc: 0.646 - ETA: 0s - loss: 0.4185 - auc: 0.613 - 0s 20us/step - loss: 0.4157 - auc: 0.6196 - val_loss: 0.4124 - val_auc: 0.6540\n",
      "\n",
      "Epoch 00529: val_loss improved from 0.41239 to 0.41237, saving model to DeepFM.h5\n",
      "Epoch 530/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4437 - auc: 0.602 - ETA: 0s - loss: 0.4135 - auc: 0.598 - 0s 22us/step - loss: 0.4173 - auc: 0.6039 - val_loss: 0.4123 - val_auc: 0.6561\n",
      "\n",
      "Epoch 00530: val_loss improved from 0.41237 to 0.41234, saving model to DeepFM.h5\n",
      "Epoch 531/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4741 - auc: 0.641 - ETA: 0s - loss: 0.4235 - auc: 0.614 - 0s 26us/step - loss: 0.4158 - auc: 0.6215 - val_loss: 0.4123 - val_auc: 0.6565\n",
      "\n",
      "Epoch 00531: val_loss improved from 0.41234 to 0.41232, saving model to DeepFM.h5\n",
      "Epoch 532/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3852 - auc: 0.687 - ETA: 0s - loss: 0.4111 - auc: 0.602 - 0s 24us/step - loss: 0.4171 - auc: 0.6029 - val_loss: 0.4123 - val_auc: 0.6581\n",
      "\n",
      "Epoch 00532: val_loss improved from 0.41232 to 0.41228, saving model to DeepFM.h5\n",
      "Epoch 533/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4626 - auc: 0.527 - ETA: 0s - loss: 0.4187 - auc: 0.622 - 0s 23us/step - loss: 0.4150 - auc: 0.6200 - val_loss: 0.4122 - val_auc: 0.6576\n",
      "\n",
      "Epoch 00533: val_loss improved from 0.41228 to 0.41225, saving model to DeepFM.h5\n",
      "Epoch 534/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3743 - auc: 0.629 - ETA: 0s - loss: 0.4099 - auc: 0.627 - 0s 20us/step - loss: 0.4152 - auc: 0.6223 - val_loss: 0.4122 - val_auc: 0.6583\n",
      "\n",
      "Epoch 00534: val_loss improved from 0.41225 to 0.41222, saving model to DeepFM.h5\n",
      "Epoch 535/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4586 - auc: 0.634 - ETA: 0s - loss: 0.4089 - auc: 0.605 - 0s 22us/step - loss: 0.4169 - auc: 0.6111 - val_loss: 0.4122 - val_auc: 0.6561\n",
      "\n",
      "Epoch 00535: val_loss improved from 0.41222 to 0.41220, saving model to DeepFM.h5\n",
      "Epoch 536/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3801 - auc: 0.523 - ETA: 0s - loss: 0.4147 - auc: 0.628 - 0s 22us/step - loss: 0.4132 - auc: 0.6313 - val_loss: 0.4121 - val_auc: 0.6581\n",
      "\n",
      "Epoch 00536: val_loss improved from 0.41220 to 0.41215, saving model to DeepFM.h5\n",
      "Epoch 537/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4227 - auc: 0.586 - ETA: 0s - loss: 0.4122 - auc: 0.618 - 0s 22us/step - loss: 0.4157 - auc: 0.6126 - val_loss: 0.4121 - val_auc: 0.6580\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.41215 to 0.41210, saving model to DeepFM.h5\n",
      "Epoch 538/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4077 - auc: 0.654 - ETA: 0s - loss: 0.4162 - auc: 0.620 - 0s 22us/step - loss: 0.4153 - auc: 0.6231 - val_loss: 0.4121 - val_auc: 0.6580\n",
      "\n",
      "Epoch 00538: val_loss improved from 0.41210 to 0.41208, saving model to DeepFM.h5\n",
      "Epoch 539/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4069 - auc: 0.699 - ETA: 0s - loss: 0.4225 - auc: 0.632 - 0s 24us/step - loss: 0.4141 - auc: 0.6208 - val_loss: 0.4120 - val_auc: 0.6582\n",
      "\n",
      "Epoch 00539: val_loss improved from 0.41208 to 0.41203, saving model to DeepFM.h5\n",
      "Epoch 540/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4286 - auc: 0.638 - ETA: 0s - loss: 0.4180 - auc: 0.641 - 0s 24us/step - loss: 0.4128 - auc: 0.6380 - val_loss: 0.4120 - val_auc: 0.6574\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.41203 to 0.41200, saving model to DeepFM.h5\n",
      "Epoch 541/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4310 - auc: 0.608 - ETA: 0s - loss: 0.4133 - auc: 0.611 - 0s 23us/step - loss: 0.4160 - auc: 0.6100 - val_loss: 0.4120 - val_auc: 0.6575\n",
      "\n",
      "Epoch 00541: val_loss improved from 0.41200 to 0.41196, saving model to DeepFM.h5\n",
      "Epoch 542/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3745 - auc: 0.702 - ETA: 0s - loss: 0.4114 - auc: 0.607 - 0s 21us/step - loss: 0.4162 - auc: 0.6061 - val_loss: 0.4119 - val_auc: 0.6584\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.41196 to 0.41191, saving model to DeepFM.h5\n",
      "Epoch 543/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3548 - auc: 0.738 - ETA: 0s - loss: 0.4212 - auc: 0.624 - 0s 22us/step - loss: 0.4151 - auc: 0.6163 - val_loss: 0.4119 - val_auc: 0.6579\n",
      "\n",
      "Epoch 00543: val_loss improved from 0.41191 to 0.41188, saving model to DeepFM.h5\n",
      "Epoch 544/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4495 - auc: 0.644 - ETA: 0s - loss: 0.4217 - auc: 0.621 - 0s 22us/step - loss: 0.4158 - auc: 0.6046 - val_loss: 0.4119 - val_auc: 0.6564\n",
      "\n",
      "Epoch 00544: val_loss improved from 0.41188 to 0.41185, saving model to DeepFM.h5\n",
      "Epoch 545/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4157 - auc: 0.633 - ETA: 0s - loss: 0.4206 - auc: 0.611 - 0s 23us/step - loss: 0.4144 - auc: 0.6179 - val_loss: 0.4118 - val_auc: 0.6570\n",
      "\n",
      "Epoch 00545: val_loss improved from 0.41185 to 0.41182, saving model to DeepFM.h5\n",
      "Epoch 546/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4276 - auc: 0.600 - ETA: 0s - loss: 0.4084 - auc: 0.639 - 0s 22us/step - loss: 0.4143 - auc: 0.6291 - val_loss: 0.4118 - val_auc: 0.6578\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.41182 to 0.41179, saving model to DeepFM.h5\n",
      "Epoch 547/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3884 - auc: 0.581 - ETA: 0s - loss: 0.4210 - auc: 0.612 - 0s 25us/step - loss: 0.4169 - auc: 0.6063 - val_loss: 0.4118 - val_auc: 0.6575\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.41179 to 0.41177, saving model to DeepFM.h5\n",
      "Epoch 548/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3729 - auc: 0.575 - ETA: 0s - loss: 0.4126 - auc: 0.616 - 0s 23us/step - loss: 0.4167 - auc: 0.6093 - val_loss: 0.4117 - val_auc: 0.6573\n",
      "\n",
      "Epoch 00548: val_loss improved from 0.41177 to 0.41173, saving model to DeepFM.h5\n",
      "Epoch 549/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3770 - auc: 0.671 - ETA: 0s - loss: 0.4158 - auc: 0.628 - 0s 21us/step - loss: 0.4139 - auc: 0.6292 - val_loss: 0.4117 - val_auc: 0.6574\n",
      "\n",
      "Epoch 00549: val_loss improved from 0.41173 to 0.41170, saving model to DeepFM.h5\n",
      "Epoch 550/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3494 - auc: 0.626 - ETA: 0s - loss: 0.4168 - auc: 0.614 - 0s 21us/step - loss: 0.4146 - auc: 0.6189 - val_loss: 0.4117 - val_auc: 0.6586\n",
      "\n",
      "Epoch 00550: val_loss improved from 0.41170 to 0.41167, saving model to DeepFM.h5\n",
      "Epoch 551/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3967 - auc: 0.721 - ETA: 0s - loss: 0.4093 - auc: 0.626 - 0s 22us/step - loss: 0.4144 - auc: 0.6237 - val_loss: 0.4116 - val_auc: 0.6591\n",
      "\n",
      "Epoch 00551: val_loss improved from 0.41167 to 0.41163, saving model to DeepFM.h5\n",
      "Epoch 552/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3688 - auc: 0.566 - ETA: 0s - loss: 0.4193 - auc: 0.617 - 0s 20us/step - loss: 0.4149 - auc: 0.6175 - val_loss: 0.4116 - val_auc: 0.6587\n",
      "\n",
      "Epoch 00552: val_loss improved from 0.41163 to 0.41160, saving model to DeepFM.h5\n",
      "Epoch 553/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4018 - auc: 0.634 - ETA: 0s - loss: 0.4118 - auc: 0.632 - 0s 23us/step - loss: 0.4144 - auc: 0.6203 - val_loss: 0.4116 - val_auc: 0.6584\n",
      "\n",
      "Epoch 00553: val_loss improved from 0.41160 to 0.41157, saving model to DeepFM.h5\n",
      "Epoch 554/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4037 - auc: 0.655 - ETA: 0s - loss: 0.4170 - auc: 0.618 - 0s 23us/step - loss: 0.4153 - auc: 0.6131 - val_loss: 0.4115 - val_auc: 0.6590\n",
      "\n",
      "Epoch 00554: val_loss improved from 0.41157 to 0.41154, saving model to DeepFM.h5\n",
      "Epoch 555/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4425 - auc: 0.588 - ETA: 0s - loss: 0.4230 - auc: 0.612 - 0s 24us/step - loss: 0.4154 - auc: 0.6159 - val_loss: 0.4115 - val_auc: 0.6599\n",
      "\n",
      "Epoch 00555: val_loss improved from 0.41154 to 0.41150, saving model to DeepFM.h5\n",
      "Epoch 556/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4211 - auc: 0.717 - ETA: 0s - loss: 0.4162 - auc: 0.609 - 0s 23us/step - loss: 0.4146 - auc: 0.6191 - val_loss: 0.4115 - val_auc: 0.6617\n",
      "\n",
      "Epoch 00556: val_loss improved from 0.41150 to 0.41146, saving model to DeepFM.h5\n",
      "Epoch 557/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4360 - auc: 0.631 - ETA: 0s - loss: 0.4040 - auc: 0.629 - 0s 25us/step - loss: 0.4135 - auc: 0.6290 - val_loss: 0.4114 - val_auc: 0.6612\n",
      "\n",
      "Epoch 00557: val_loss improved from 0.41146 to 0.41142, saving model to DeepFM.h5\n",
      "Epoch 558/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3884 - auc: 0.573 - ETA: 0s - loss: 0.4203 - auc: 0.616 - 0s 23us/step - loss: 0.4152 - auc: 0.6126 - val_loss: 0.4114 - val_auc: 0.6603\n",
      "\n",
      "Epoch 00558: val_loss improved from 0.41142 to 0.41138, saving model to DeepFM.h5\n",
      "Epoch 559/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4264 - auc: 0.628 - ETA: 0s - loss: 0.4064 - auc: 0.621 - 0s 22us/step - loss: 0.4133 - auc: 0.6268 - val_loss: 0.4113 - val_auc: 0.6617\n",
      "\n",
      "Epoch 00559: val_loss improved from 0.41138 to 0.41134, saving model to DeepFM.h5\n",
      "Epoch 560/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3628 - auc: 0.625 - ETA: 0s - loss: 0.4094 - auc: 0.604 - 0s 22us/step - loss: 0.4165 - auc: 0.6064 - val_loss: 0.4113 - val_auc: 0.6626\n",
      "\n",
      "Epoch 00560: val_loss improved from 0.41134 to 0.41133, saving model to DeepFM.h5\n",
      "Epoch 561/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3635 - auc: 0.658 - ETA: 0s - loss: 0.4095 - auc: 0.637 - 0s 24us/step - loss: 0.4132 - auc: 0.6363 - val_loss: 0.4113 - val_auc: 0.6626\n",
      "\n",
      "Epoch 00561: val_loss improved from 0.41133 to 0.41131, saving model to DeepFM.h5\n",
      "Epoch 562/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3734 - auc: 0.725 - ETA: 0s - loss: 0.4149 - auc: 0.622 - 0s 25us/step - loss: 0.4127 - auc: 0.6404 - val_loss: 0.4113 - val_auc: 0.6583\n",
      "\n",
      "Epoch 00562: val_loss improved from 0.41131 to 0.41130, saving model to DeepFM.h5\n",
      "Epoch 563/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4420 - auc: 0.654 - ETA: 0s - loss: 0.4143 - auc: 0.615 - 0s 26us/step - loss: 0.4162 - auc: 0.6148 - val_loss: 0.4113 - val_auc: 0.6580\n",
      "\n",
      "Epoch 00563: val_loss improved from 0.41130 to 0.41127, saving model to DeepFM.h5\n",
      "Epoch 564/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4497 - auc: 0.593 - ETA: 0s - loss: 0.4072 - auc: 0.614 - 0s 24us/step - loss: 0.4156 - auc: 0.6172 - val_loss: 0.4112 - val_auc: 0.6586\n",
      "\n",
      "Epoch 00564: val_loss improved from 0.41127 to 0.41125, saving model to DeepFM.h5\n",
      "Epoch 565/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4519 - auc: 0.650 - ETA: 0s - loss: 0.4219 - auc: 0.628 - 0s 25us/step - loss: 0.4140 - auc: 0.6301 - val_loss: 0.4112 - val_auc: 0.6628\n",
      "\n",
      "Epoch 00565: val_loss improved from 0.41125 to 0.41121, saving model to DeepFM.h5\n",
      "Epoch 566/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4487 - auc: 0.573 - ETA: 0s - loss: 0.4174 - auc: 0.616 - 0s 24us/step - loss: 0.4144 - auc: 0.6242 - val_loss: 0.4112 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00566: val_loss improved from 0.41121 to 0.41118, saving model to DeepFM.h5\n",
      "Epoch 567/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4391 - auc: 0.644 - ETA: 0s - loss: 0.4185 - auc: 0.626 - 0s 22us/step - loss: 0.4138 - auc: 0.6197 - val_loss: 0.4111 - val_auc: 0.6570\n",
      "\n",
      "Epoch 00567: val_loss improved from 0.41118 to 0.41114, saving model to DeepFM.h5\n",
      "Epoch 568/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4670 - auc: 0.644 - ETA: 0s - loss: 0.4038 - auc: 0.623 - 0s 20us/step - loss: 0.4139 - auc: 0.6207 - val_loss: 0.4111 - val_auc: 0.6587\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.41114 to 0.41111, saving model to DeepFM.h5\n",
      "Epoch 569/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4125 - auc: 0.671 - ETA: 0s - loss: 0.4116 - auc: 0.619 - 0s 22us/step - loss: 0.4149 - auc: 0.6135 - val_loss: 0.4111 - val_auc: 0.6570\n",
      "\n",
      "Epoch 00569: val_loss improved from 0.41111 to 0.41109, saving model to DeepFM.h5\n",
      "Epoch 570/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3773 - auc: 0.668 - ETA: 0s - loss: 0.4102 - auc: 0.637 - 0s 22us/step - loss: 0.4141 - auc: 0.6248 - val_loss: 0.4111 - val_auc: 0.6614\n",
      "\n",
      "Epoch 00570: val_loss improved from 0.41109 to 0.41106, saving model to DeepFM.h5\n",
      "Epoch 571/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5061 - auc: 0.598 - ETA: 0s - loss: 0.4081 - auc: 0.612 - 0s 22us/step - loss: 0.4134 - auc: 0.6257 - val_loss: 0.4110 - val_auc: 0.6583\n",
      "\n",
      "Epoch 00571: val_loss improved from 0.41106 to 0.41102, saving model to DeepFM.h5\n",
      "Epoch 572/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3979 - auc: 0.687 - ETA: 0s - loss: 0.4020 - auc: 0.638 - 0s 25us/step - loss: 0.4139 - auc: 0.6184 - val_loss: 0.4110 - val_auc: 0.6604\n",
      "\n",
      "Epoch 00572: val_loss improved from 0.41102 to 0.41099, saving model to DeepFM.h5\n",
      "Epoch 573/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4357 - auc: 0.627 - ETA: 0s - loss: 0.4044 - auc: 0.613 - 0s 22us/step - loss: 0.4135 - auc: 0.6224 - val_loss: 0.4109 - val_auc: 0.6620\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.41099 to 0.41094, saving model to DeepFM.h5\n",
      "Epoch 574/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3305 - auc: 0.737 - ETA: 0s - loss: 0.4105 - auc: 0.639 - 0s 22us/step - loss: 0.4121 - auc: 0.6386 - val_loss: 0.4109 - val_auc: 0.6624\n",
      "\n",
      "Epoch 00574: val_loss improved from 0.41094 to 0.41091, saving model to DeepFM.h5\n",
      "Epoch 575/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3886 - auc: 0.619 - ETA: 0s - loss: 0.4137 - auc: 0.626 - 0s 22us/step - loss: 0.4138 - auc: 0.6256 - val_loss: 0.4109 - val_auc: 0.6623\n",
      "\n",
      "Epoch 00575: val_loss improved from 0.41091 to 0.41088, saving model to DeepFM.h5\n",
      "Epoch 576/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4080 - auc: 0.568 - ETA: 0s - loss: 0.4079 - auc: 0.625 - 0s 22us/step - loss: 0.4140 - auc: 0.6279 - val_loss: 0.4109 - val_auc: 0.6620\n",
      "\n",
      "Epoch 00576: val_loss improved from 0.41088 to 0.41085, saving model to DeepFM.h5\n",
      "Epoch 577/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4474 - auc: 0.622 - ETA: 0s - loss: 0.4138 - auc: 0.624 - 0s 22us/step - loss: 0.4145 - auc: 0.6217 - val_loss: 0.4108 - val_auc: 0.6627\n",
      "\n",
      "Epoch 00577: val_loss improved from 0.41085 to 0.41084, saving model to DeepFM.h5\n",
      "Epoch 578/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2879 - auc: 0.692 - ETA: 0s - loss: 0.4223 - auc: 0.628 - 0s 23us/step - loss: 0.4135 - auc: 0.6310 - val_loss: 0.4108 - val_auc: 0.6632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00578: val_loss improved from 0.41084 to 0.41080, saving model to DeepFM.h5\n",
      "Epoch 579/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3546 - auc: 0.708 - ETA: 0s - loss: 0.4098 - auc: 0.644 - 0s 23us/step - loss: 0.4114 - auc: 0.6412 - val_loss: 0.4108 - val_auc: 0.6637\n",
      "\n",
      "Epoch 00579: val_loss improved from 0.41080 to 0.41075, saving model to DeepFM.h5\n",
      "Epoch 580/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4276 - auc: 0.589 - ETA: 0s - loss: 0.4103 - auc: 0.616 - 0s 21us/step - loss: 0.4152 - auc: 0.6170 - val_loss: 0.4107 - val_auc: 0.6629\n",
      "\n",
      "Epoch 00580: val_loss improved from 0.41075 to 0.41073, saving model to DeepFM.h5\n",
      "Epoch 581/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3383 - auc: 0.640 - ETA: 0s - loss: 0.4061 - auc: 0.629 - 0s 23us/step - loss: 0.4129 - auc: 0.6287 - val_loss: 0.4107 - val_auc: 0.6632\n",
      "\n",
      "Epoch 00581: val_loss improved from 0.41073 to 0.41069, saving model to DeepFM.h5\n",
      "Epoch 582/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4427 - auc: 0.582 - ETA: 0s - loss: 0.4178 - auc: 0.583 - 0s 24us/step - loss: 0.4181 - auc: 0.5952 - val_loss: 0.4107 - val_auc: 0.6630\n",
      "\n",
      "Epoch 00582: val_loss improved from 0.41069 to 0.41066, saving model to DeepFM.h5\n",
      "Epoch 583/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4470 - auc: 0.608 - ETA: 0s - loss: 0.4076 - auc: 0.620 - 0s 22us/step - loss: 0.4156 - auc: 0.6141 - val_loss: 0.4107 - val_auc: 0.6630\n",
      "\n",
      "Epoch 00583: val_loss improved from 0.41066 to 0.41065, saving model to DeepFM.h5\n",
      "Epoch 584/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4403 - auc: 0.656 - ETA: 0s - loss: 0.4121 - auc: 0.608 - 0s 22us/step - loss: 0.4149 - auc: 0.6132 - val_loss: 0.4106 - val_auc: 0.6625\n",
      "\n",
      "Epoch 00584: val_loss improved from 0.41065 to 0.41062, saving model to DeepFM.h5\n",
      "Epoch 585/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4427 - auc: 0.549 - ETA: 0s - loss: 0.4132 - auc: 0.615 - 0s 22us/step - loss: 0.4126 - auc: 0.6276 - val_loss: 0.4106 - val_auc: 0.6645\n",
      "\n",
      "Epoch 00585: val_loss improved from 0.41062 to 0.41057, saving model to DeepFM.h5\n",
      "Epoch 586/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3771 - auc: 0.676 - ETA: 0s - loss: 0.4227 - auc: 0.615 - 0s 21us/step - loss: 0.4123 - auc: 0.6295 - val_loss: 0.4105 - val_auc: 0.6646\n",
      "\n",
      "Epoch 00586: val_loss improved from 0.41057 to 0.41053, saving model to DeepFM.h5\n",
      "Epoch 587/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4933 - auc: 0.615 - ETA: 0s - loss: 0.4174 - auc: 0.617 - 0s 23us/step - loss: 0.4140 - auc: 0.6207 - val_loss: 0.4105 - val_auc: 0.6631\n",
      "\n",
      "Epoch 00587: val_loss improved from 0.41053 to 0.41051, saving model to DeepFM.h5\n",
      "Epoch 588/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4089 - auc: 0.580 - ETA: 0s - loss: 0.4091 - auc: 0.611 - 0s 22us/step - loss: 0.4135 - auc: 0.6245 - val_loss: 0.4105 - val_auc: 0.6636\n",
      "\n",
      "Epoch 00588: val_loss improved from 0.41051 to 0.41049, saving model to DeepFM.h5\n",
      "Epoch 589/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.615 - ETA: 0s - loss: 0.4107 - auc: 0.626 - 0s 23us/step - loss: 0.4138 - auc: 0.6272 - val_loss: 0.4105 - val_auc: 0.6644\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.41049 to 0.41046, saving model to DeepFM.h5\n",
      "Epoch 590/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4425 - auc: 0.580 - ETA: 0s - loss: 0.4150 - auc: 0.619 - 0s 23us/step - loss: 0.4138 - auc: 0.6174 - val_loss: 0.4104 - val_auc: 0.6647\n",
      "\n",
      "Epoch 00590: val_loss improved from 0.41046 to 0.41043, saving model to DeepFM.h5\n",
      "Epoch 591/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4491 - auc: 0.569 - ETA: 0s - loss: 0.4146 - auc: 0.612 - 0s 22us/step - loss: 0.4140 - auc: 0.6186 - val_loss: 0.4104 - val_auc: 0.6659\n",
      "\n",
      "Epoch 00591: val_loss improved from 0.41043 to 0.41041, saving model to DeepFM.h5\n",
      "Epoch 592/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4039 - auc: 0.618 - ETA: 0s - loss: 0.4108 - auc: 0.633 - 0s 23us/step - loss: 0.4129 - auc: 0.6370 - val_loss: 0.4104 - val_auc: 0.6648\n",
      "\n",
      "Epoch 00592: val_loss improved from 0.41041 to 0.41037, saving model to DeepFM.h5\n",
      "Epoch 593/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4129 - auc: 0.578 - ETA: 0s - loss: 0.4150 - auc: 0.624 - 0s 21us/step - loss: 0.4148 - auc: 0.6225 - val_loss: 0.4103 - val_auc: 0.6648\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.41037 to 0.41035, saving model to DeepFM.h5\n",
      "Epoch 594/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4588 - auc: 0.690 - ETA: 0s - loss: 0.4217 - auc: 0.618 - 0s 21us/step - loss: 0.4139 - auc: 0.6175 - val_loss: 0.4103 - val_auc: 0.6649\n",
      "\n",
      "Epoch 00594: val_loss improved from 0.41035 to 0.41033, saving model to DeepFM.h5\n",
      "Epoch 595/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3830 - auc: 0.679 - ETA: 0s - loss: 0.4010 - auc: 0.636 - 0s 22us/step - loss: 0.4137 - auc: 0.6239 - val_loss: 0.4103 - val_auc: 0.6654\n",
      "\n",
      "Epoch 00595: val_loss improved from 0.41033 to 0.41030, saving model to DeepFM.h5\n",
      "Epoch 596/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4340 - auc: 0.622 - ETA: 0s - loss: 0.4146 - auc: 0.639 - 0s 22us/step - loss: 0.4112 - auc: 0.6370 - val_loss: 0.4103 - val_auc: 0.6650\n",
      "\n",
      "Epoch 00596: val_loss improved from 0.41030 to 0.41026, saving model to DeepFM.h5\n",
      "Epoch 597/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3844 - auc: 0.568 - ETA: 0s - loss: 0.4131 - auc: 0.610 - 0s 25us/step - loss: 0.4136 - auc: 0.6176 - val_loss: 0.4102 - val_auc: 0.6649\n",
      "\n",
      "Epoch 00597: val_loss improved from 0.41026 to 0.41023, saving model to DeepFM.h5\n",
      "Epoch 598/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3995 - auc: 0.790 - ETA: 0s - loss: 0.4032 - auc: 0.635 - 0s 24us/step - loss: 0.4112 - auc: 0.6427 - val_loss: 0.4102 - val_auc: 0.6660\n",
      "\n",
      "Epoch 00598: val_loss improved from 0.41023 to 0.41018, saving model to DeepFM.h5\n",
      "Epoch 599/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.699 - ETA: 0s - loss: 0.4194 - auc: 0.628 - 0s 23us/step - loss: 0.4123 - auc: 0.6281 - val_loss: 0.4101 - val_auc: 0.6653\n",
      "\n",
      "Epoch 00599: val_loss improved from 0.41018 to 0.41014, saving model to DeepFM.h5\n",
      "Epoch 600/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4549 - auc: 0.651 - ETA: 0s - loss: 0.4189 - auc: 0.630 - 0s 24us/step - loss: 0.4134 - auc: 0.6239 - val_loss: 0.4101 - val_auc: 0.6654\n",
      "\n",
      "Epoch 00600: val_loss improved from 0.41014 to 0.41012, saving model to DeepFM.h5\n",
      "Epoch 601/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3575 - auc: 0.682 - ETA: 0s - loss: 0.4047 - auc: 0.630 - 0s 23us/step - loss: 0.4128 - auc: 0.6297 - val_loss: 0.4101 - val_auc: 0.6648\n",
      "\n",
      "Epoch 00601: val_loss improved from 0.41012 to 0.41009, saving model to DeepFM.h5\n",
      "Epoch 602/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4751 - auc: 0.590 - ETA: 0s - loss: 0.4190 - auc: 0.631 - 0s 23us/step - loss: 0.4119 - auc: 0.6322 - val_loss: 0.4101 - val_auc: 0.6654\n",
      "\n",
      "Epoch 00602: val_loss improved from 0.41009 to 0.41005, saving model to DeepFM.h5\n",
      "Epoch 603/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4525 - auc: 0.581 - ETA: 0s - loss: 0.4160 - auc: 0.619 - 0s 23us/step - loss: 0.4124 - auc: 0.6301 - val_loss: 0.4100 - val_auc: 0.6653\n",
      "\n",
      "Epoch 00603: val_loss improved from 0.41005 to 0.41002, saving model to DeepFM.h5\n",
      "Epoch 604/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3450 - auc: 0.550 - ETA: 0s - loss: 0.4134 - auc: 0.621 - 0s 24us/step - loss: 0.4110 - auc: 0.6409 - val_loss: 0.4100 - val_auc: 0.6653\n",
      "\n",
      "Epoch 00604: val_loss improved from 0.41002 to 0.40998, saving model to DeepFM.h5\n",
      "Epoch 605/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4797 - auc: 0.591 - ETA: 0s - loss: 0.4212 - auc: 0.631 - 0s 24us/step - loss: 0.4132 - auc: 0.6221 - val_loss: 0.4100 - val_auc: 0.6643\n",
      "\n",
      "Epoch 00605: val_loss improved from 0.40998 to 0.40995, saving model to DeepFM.h5\n",
      "Epoch 606/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3823 - auc: 0.619 - ETA: 0s - loss: 0.4123 - auc: 0.611 - 0s 23us/step - loss: 0.4148 - auc: 0.6166 - val_loss: 0.4099 - val_auc: 0.6645\n",
      "\n",
      "Epoch 00606: val_loss improved from 0.40995 to 0.40993, saving model to DeepFM.h5\n",
      "Epoch 607/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3984 - auc: 0.649 - ETA: 0s - loss: 0.4148 - auc: 0.611 - 0s 22us/step - loss: 0.4121 - auc: 0.6338 - val_loss: 0.4099 - val_auc: 0.6657\n",
      "\n",
      "Epoch 00607: val_loss improved from 0.40993 to 0.40989, saving model to DeepFM.h5\n",
      "Epoch 608/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4442 - auc: 0.656 - ETA: 0s - loss: 0.4188 - auc: 0.623 - 0s 22us/step - loss: 0.4118 - auc: 0.6342 - val_loss: 0.4099 - val_auc: 0.6659\n",
      "\n",
      "Epoch 00608: val_loss improved from 0.40989 to 0.40986, saving model to DeepFM.h5\n",
      "Epoch 609/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3647 - auc: 0.597 - ETA: 0s - loss: 0.4084 - auc: 0.617 - 0s 21us/step - loss: 0.4133 - auc: 0.6272 - val_loss: 0.4098 - val_auc: 0.6649\n",
      "\n",
      "Epoch 00609: val_loss improved from 0.40986 to 0.40985, saving model to DeepFM.h5\n",
      "Epoch 610/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3745 - auc: 0.625 - ETA: 0s - loss: 0.4147 - auc: 0.623 - 0s 22us/step - loss: 0.4131 - auc: 0.6251 - val_loss: 0.4098 - val_auc: 0.6654\n",
      "\n",
      "Epoch 00610: val_loss improved from 0.40985 to 0.40982, saving model to DeepFM.h5\n",
      "Epoch 611/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3746 - auc: 0.677 - ETA: 0s - loss: 0.4093 - auc: 0.611 - 0s 22us/step - loss: 0.4156 - auc: 0.6108 - val_loss: 0.4098 - val_auc: 0.6664\n",
      "\n",
      "Epoch 00611: val_loss improved from 0.40982 to 0.40979, saving model to DeepFM.h5\n",
      "Epoch 612/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4068 - auc: 0.617 - ETA: 0s - loss: 0.4143 - auc: 0.633 - 0s 21us/step - loss: 0.4116 - auc: 0.6346 - val_loss: 0.4097 - val_auc: 0.6669\n",
      "\n",
      "Epoch 00612: val_loss improved from 0.40979 to 0.40974, saving model to DeepFM.h5\n",
      "Epoch 613/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4413 - auc: 0.600 - ETA: 0s - loss: 0.4155 - auc: 0.641 - 0s 21us/step - loss: 0.4128 - auc: 0.6216 - val_loss: 0.4097 - val_auc: 0.6665\n",
      "\n",
      "Epoch 00613: val_loss improved from 0.40974 to 0.40970, saving model to DeepFM.h5\n",
      "Epoch 614/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4019 - auc: 0.701 - ETA: 0s - loss: 0.3982 - auc: 0.656 - 0s 22us/step - loss: 0.4086 - auc: 0.6549 - val_loss: 0.4096 - val_auc: 0.6667\n",
      "\n",
      "Epoch 00614: val_loss improved from 0.40970 to 0.40965, saving model to DeepFM.h5\n",
      "Epoch 615/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3797 - auc: 0.520 - ETA: 0s - loss: 0.4060 - auc: 0.630 - 0s 22us/step - loss: 0.4128 - auc: 0.6313 - val_loss: 0.4096 - val_auc: 0.6670\n",
      "\n",
      "Epoch 00615: val_loss improved from 0.40965 to 0.40962, saving model to DeepFM.h5\n",
      "Epoch 616/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3601 - auc: 0.715 - ETA: 0s - loss: 0.4052 - auc: 0.624 - 0s 23us/step - loss: 0.4142 - auc: 0.6217 - val_loss: 0.4096 - val_auc: 0.6670\n",
      "\n",
      "Epoch 00616: val_loss improved from 0.40962 to 0.40961, saving model to DeepFM.h5\n",
      "Epoch 617/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3847 - auc: 0.707 - ETA: 0s - loss: 0.4217 - auc: 0.636 - 0s 23us/step - loss: 0.4124 - auc: 0.6262 - val_loss: 0.4096 - val_auc: 0.6655\n",
      "\n",
      "Epoch 00617: val_loss improved from 0.40961 to 0.40957, saving model to DeepFM.h5\n",
      "Epoch 618/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4271 - auc: 0.589 - ETA: 0s - loss: 0.4093 - auc: 0.633 - 0s 22us/step - loss: 0.4115 - auc: 0.6317 - val_loss: 0.4095 - val_auc: 0.6653\n",
      "\n",
      "Epoch 00618: val_loss improved from 0.40957 to 0.40954, saving model to DeepFM.h5\n",
      "Epoch 619/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3960 - auc: 0.600 - ETA: 0s - loss: 0.4076 - auc: 0.618 - 0s 23us/step - loss: 0.4127 - auc: 0.6244 - val_loss: 0.4095 - val_auc: 0.6652\n",
      "\n",
      "Epoch 00619: val_loss improved from 0.40954 to 0.40951, saving model to DeepFM.h5\n",
      "Epoch 620/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3736 - auc: 0.614 - ETA: 0s - loss: 0.4165 - auc: 0.618 - 0s 22us/step - loss: 0.4128 - auc: 0.6225 - val_loss: 0.4095 - val_auc: 0.6678\n",
      "\n",
      "Epoch 00620: val_loss improved from 0.40951 to 0.40949, saving model to DeepFM.h5\n",
      "Epoch 621/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3376 - auc: 0.631 - ETA: 0s - loss: 0.4138 - auc: 0.616 - 0s 22us/step - loss: 0.4123 - auc: 0.6323 - val_loss: 0.4095 - val_auc: 0.6684\n",
      "\n",
      "Epoch 00621: val_loss improved from 0.40949 to 0.40947, saving model to DeepFM.h5\n",
      "Epoch 622/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4297 - auc: 0.606 - ETA: 0s - loss: 0.4163 - auc: 0.636 - 0s 23us/step - loss: 0.4117 - auc: 0.6310 - val_loss: 0.4094 - val_auc: 0.6667\n",
      "\n",
      "Epoch 00622: val_loss improved from 0.40947 to 0.40942, saving model to DeepFM.h5\n",
      "Epoch 623/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3658 - auc: 0.641 - ETA: 0s - loss: 0.4163 - auc: 0.624 - 0s 21us/step - loss: 0.4128 - auc: 0.6257 - val_loss: 0.4094 - val_auc: 0.6676\n",
      "\n",
      "Epoch 00623: val_loss improved from 0.40942 to 0.40939, saving model to DeepFM.h5\n",
      "Epoch 624/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3750 - auc: 0.653 - ETA: 0s - loss: 0.4078 - auc: 0.627 - 0s 22us/step - loss: 0.4131 - auc: 0.6248 - val_loss: 0.4094 - val_auc: 0.6663\n",
      "\n",
      "Epoch 00624: val_loss improved from 0.40939 to 0.40936, saving model to DeepFM.h5\n",
      "Epoch 625/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3523 - auc: 0.704 - ETA: 0s - loss: 0.4111 - auc: 0.633 - 0s 22us/step - loss: 0.4107 - auc: 0.6422 - val_loss: 0.4093 - val_auc: 0.6666\n",
      "\n",
      "Epoch 00625: val_loss improved from 0.40936 to 0.40933, saving model to DeepFM.h5\n",
      "Epoch 626/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4062 - auc: 0.561 - ETA: 0s - loss: 0.4084 - auc: 0.644 - 0s 22us/step - loss: 0.4101 - auc: 0.6397 - val_loss: 0.4093 - val_auc: 0.6671\n",
      "\n",
      "Epoch 00626: val_loss improved from 0.40933 to 0.40929, saving model to DeepFM.h5\n",
      "Epoch 627/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3760 - auc: 0.553 - ETA: 0s - loss: 0.4045 - auc: 0.632 - 0s 23us/step - loss: 0.4133 - auc: 0.6206 - val_loss: 0.4093 - val_auc: 0.6657\n",
      "\n",
      "Epoch 00627: val_loss improved from 0.40929 to 0.40925, saving model to DeepFM.h5\n",
      "Epoch 628/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3670 - auc: 0.646 - ETA: 0s - loss: 0.3995 - auc: 0.637 - 0s 23us/step - loss: 0.4124 - auc: 0.6290 - val_loss: 0.4092 - val_auc: 0.6655\n",
      "\n",
      "Epoch 00628: val_loss improved from 0.40925 to 0.40920, saving model to DeepFM.h5\n",
      "Epoch 629/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3855 - auc: 0.669 - ETA: 0s - loss: 0.4153 - auc: 0.644 - 0s 23us/step - loss: 0.4103 - auc: 0.6444 - val_loss: 0.4092 - val_auc: 0.6659\n",
      "\n",
      "Epoch 00629: val_loss improved from 0.40920 to 0.40918, saving model to DeepFM.h5\n",
      "Epoch 630/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4053 - auc: 0.653 - ETA: 0s - loss: 0.4181 - auc: 0.642 - 0s 22us/step - loss: 0.4115 - auc: 0.6344 - val_loss: 0.4092 - val_auc: 0.6655\n",
      "\n",
      "Epoch 00630: val_loss improved from 0.40918 to 0.40917, saving model to DeepFM.h5\n",
      "Epoch 631/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3337 - auc: 0.603 - ETA: 0s - loss: 0.4090 - auc: 0.625 - 0s 22us/step - loss: 0.4126 - auc: 0.6270 - val_loss: 0.4091 - val_auc: 0.6656\n",
      "\n",
      "Epoch 00631: val_loss improved from 0.40917 to 0.40913, saving model to DeepFM.h5\n",
      "Epoch 632/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3776 - auc: 0.560 - ETA: 0s - loss: 0.4105 - auc: 0.632 - 0s 22us/step - loss: 0.4108 - auc: 0.6403 - val_loss: 0.4091 - val_auc: 0.6658\n",
      "\n",
      "Epoch 00632: val_loss improved from 0.40913 to 0.40909, saving model to DeepFM.h5\n",
      "Epoch 633/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3509 - auc: 0.608 - ETA: 0s - loss: 0.4249 - auc: 0.611 - 0s 23us/step - loss: 0.4131 - auc: 0.6286 - val_loss: 0.4091 - val_auc: 0.6666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00633: val_loss improved from 0.40909 to 0.40908, saving model to DeepFM.h5\n",
      "Epoch 634/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5208 - auc: 0.619 - ETA: 0s - loss: 0.4281 - auc: 0.620 - 0s 21us/step - loss: 0.4116 - auc: 0.6312 - val_loss: 0.4090 - val_auc: 0.6658\n",
      "\n",
      "Epoch 00634: val_loss improved from 0.40908 to 0.40904, saving model to DeepFM.h5\n",
      "Epoch 635/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4625 - auc: 0.614 - ETA: 0s - loss: 0.4108 - auc: 0.631 - 0s 23us/step - loss: 0.4107 - auc: 0.6395 - val_loss: 0.4090 - val_auc: 0.6651\n",
      "\n",
      "Epoch 00635: val_loss improved from 0.40904 to 0.40900, saving model to DeepFM.h5\n",
      "Epoch 636/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4525 - auc: 0.636 - ETA: 0s - loss: 0.4122 - auc: 0.643 - 0s 22us/step - loss: 0.4101 - auc: 0.6426 - val_loss: 0.4090 - val_auc: 0.6658\n",
      "\n",
      "Epoch 00636: val_loss improved from 0.40900 to 0.40896, saving model to DeepFM.h5\n",
      "Epoch 637/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4304 - auc: 0.654 - ETA: 0s - loss: 0.4071 - auc: 0.628 - 0s 22us/step - loss: 0.4121 - auc: 0.6299 - val_loss: 0.4089 - val_auc: 0.6659\n",
      "\n",
      "Epoch 00637: val_loss improved from 0.40896 to 0.40891, saving model to DeepFM.h5\n",
      "Epoch 638/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4144 - auc: 0.623 - ETA: 0s - loss: 0.4042 - auc: 0.664 - 0s 21us/step - loss: 0.4092 - auc: 0.6503 - val_loss: 0.4089 - val_auc: 0.6656\n",
      "\n",
      "Epoch 00638: val_loss improved from 0.40891 to 0.40887, saving model to DeepFM.h5\n",
      "Epoch 639/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3976 - auc: 0.554 - ETA: 0s - loss: 0.4134 - auc: 0.636 - 0s 21us/step - loss: 0.4113 - auc: 0.6390 - val_loss: 0.4088 - val_auc: 0.6671\n",
      "\n",
      "Epoch 00639: val_loss improved from 0.40887 to 0.40884, saving model to DeepFM.h5\n",
      "Epoch 640/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4236 - auc: 0.637 - ETA: 0s - loss: 0.4037 - auc: 0.634 - 0s 22us/step - loss: 0.4116 - auc: 0.6336 - val_loss: 0.4088 - val_auc: 0.6663\n",
      "\n",
      "Epoch 00640: val_loss improved from 0.40884 to 0.40881, saving model to DeepFM.h5\n",
      "Epoch 641/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4193 - auc: 0.593 - ETA: 0s - loss: 0.3971 - auc: 0.624 - 0s 22us/step - loss: 0.4113 - auc: 0.6289 - val_loss: 0.4088 - val_auc: 0.6666\n",
      "\n",
      "Epoch 00641: val_loss improved from 0.40881 to 0.40876, saving model to DeepFM.h5\n",
      "Epoch 642/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4044 - auc: 0.606 - ETA: 0s - loss: 0.4095 - auc: 0.642 - 0s 23us/step - loss: 0.4101 - auc: 0.6419 - val_loss: 0.4087 - val_auc: 0.6664\n",
      "\n",
      "Epoch 00642: val_loss improved from 0.40876 to 0.40874, saving model to DeepFM.h5\n",
      "Epoch 643/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3471 - auc: 0.650 - ETA: 0s - loss: 0.4042 - auc: 0.624 - 0s 23us/step - loss: 0.4105 - auc: 0.6412 - val_loss: 0.4087 - val_auc: 0.6665\n",
      "\n",
      "Epoch 00643: val_loss improved from 0.40874 to 0.40871, saving model to DeepFM.h5\n",
      "Epoch 644/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3933 - auc: 0.633 - ETA: 0s - loss: 0.4181 - auc: 0.634 - 0s 23us/step - loss: 0.4109 - auc: 0.6374 - val_loss: 0.4087 - val_auc: 0.6672\n",
      "\n",
      "Epoch 00644: val_loss improved from 0.40871 to 0.40867, saving model to DeepFM.h5\n",
      "Epoch 645/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3546 - auc: 0.653 - ETA: 0s - loss: 0.4126 - auc: 0.631 - 0s 22us/step - loss: 0.4125 - auc: 0.6258 - val_loss: 0.4086 - val_auc: 0.6666\n",
      "\n",
      "Epoch 00645: val_loss improved from 0.40867 to 0.40864, saving model to DeepFM.h5\n",
      "Epoch 646/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4103 - auc: 0.621 - ETA: 0s - loss: 0.4095 - auc: 0.624 - 0s 24us/step - loss: 0.4124 - auc: 0.6349 - val_loss: 0.4086 - val_auc: 0.6669\n",
      "\n",
      "Epoch 00646: val_loss improved from 0.40864 to 0.40860, saving model to DeepFM.h5\n",
      "Epoch 647/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3940 - auc: 0.622 - ETA: 0s - loss: 0.4124 - auc: 0.620 - 0s 21us/step - loss: 0.4103 - auc: 0.6335 - val_loss: 0.4085 - val_auc: 0.6682\n",
      "\n",
      "Epoch 00647: val_loss improved from 0.40860 to 0.40854, saving model to DeepFM.h5\n",
      "Epoch 648/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4368 - auc: 0.671 - ETA: 0s - loss: 0.4096 - auc: 0.656 - 0s 21us/step - loss: 0.4094 - auc: 0.6512 - val_loss: 0.4085 - val_auc: 0.6688\n",
      "\n",
      "Epoch 00648: val_loss improved from 0.40854 to 0.40851, saving model to DeepFM.h5\n",
      "Epoch 649/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3608 - auc: 0.757 - ETA: 0s - loss: 0.4095 - auc: 0.647 - 0s 21us/step - loss: 0.4090 - auc: 0.6408 - val_loss: 0.4085 - val_auc: 0.6687\n",
      "\n",
      "Epoch 00649: val_loss improved from 0.40851 to 0.40847, saving model to DeepFM.h5\n",
      "Epoch 650/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4260 - auc: 0.678 - ETA: 0s - loss: 0.4100 - auc: 0.635 - 0s 22us/step - loss: 0.4087 - auc: 0.6434 - val_loss: 0.4084 - val_auc: 0.6673\n",
      "\n",
      "Epoch 00650: val_loss improved from 0.40847 to 0.40845, saving model to DeepFM.h5\n",
      "Epoch 651/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4811 - auc: 0.625 - ETA: 0s - loss: 0.4249 - auc: 0.641 - 0s 23us/step - loss: 0.4110 - auc: 0.6395 - val_loss: 0.4084 - val_auc: 0.6673\n",
      "\n",
      "Epoch 00651: val_loss improved from 0.40845 to 0.40842, saving model to DeepFM.h5\n",
      "Epoch 652/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3852 - auc: 0.641 - ETA: 0s - loss: 0.4153 - auc: 0.639 - 0s 22us/step - loss: 0.4104 - auc: 0.6394 - val_loss: 0.4084 - val_auc: 0.6682\n",
      "\n",
      "Epoch 00652: val_loss improved from 0.40842 to 0.40838, saving model to DeepFM.h5\n",
      "Epoch 653/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.629 - ETA: 0s - loss: 0.4084 - auc: 0.629 - 0s 22us/step - loss: 0.4112 - auc: 0.6377 - val_loss: 0.4084 - val_auc: 0.6684\n",
      "\n",
      "Epoch 00653: val_loss improved from 0.40838 to 0.40836, saving model to DeepFM.h5\n",
      "Epoch 654/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4656 - auc: 0.617 - ETA: 0s - loss: 0.4068 - auc: 0.663 - 0s 23us/step - loss: 0.4106 - auc: 0.6442 - val_loss: 0.4083 - val_auc: 0.6676\n",
      "\n",
      "Epoch 00654: val_loss improved from 0.40836 to 0.40833, saving model to DeepFM.h5\n",
      "Epoch 655/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4113 - auc: 0.662 - ETA: 0s - loss: 0.4142 - auc: 0.639 - 0s 22us/step - loss: 0.4083 - auc: 0.6488 - val_loss: 0.4083 - val_auc: 0.6685\n",
      "\n",
      "Epoch 00655: val_loss improved from 0.40833 to 0.40832, saving model to DeepFM.h5\n",
      "Epoch 656/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4031 - auc: 0.616 - ETA: 0s - loss: 0.4181 - auc: 0.623 - 0s 24us/step - loss: 0.4123 - auc: 0.6256 - val_loss: 0.4083 - val_auc: 0.6694\n",
      "\n",
      "Epoch 00656: val_loss improved from 0.40832 to 0.40828, saving model to DeepFM.h5\n",
      "Epoch 657/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3481 - auc: 0.655 - ETA: 0s - loss: 0.4120 - auc: 0.641 - 0s 22us/step - loss: 0.4090 - auc: 0.6421 - val_loss: 0.4082 - val_auc: 0.6698\n",
      "\n",
      "Epoch 00657: val_loss improved from 0.40828 to 0.40823, saving model to DeepFM.h5\n",
      "Epoch 658/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3567 - auc: 0.704 - ETA: 0s - loss: 0.4114 - auc: 0.621 - 0s 22us/step - loss: 0.4125 - auc: 0.6283 - val_loss: 0.4082 - val_auc: 0.6689\n",
      "\n",
      "Epoch 00658: val_loss improved from 0.40823 to 0.40820, saving model to DeepFM.h5\n",
      "Epoch 659/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4389 - auc: 0.620 - ETA: 0s - loss: 0.4086 - auc: 0.631 - 0s 22us/step - loss: 0.4112 - auc: 0.6340 - val_loss: 0.4082 - val_auc: 0.6691\n",
      "\n",
      "Epoch 00659: val_loss improved from 0.40820 to 0.40817, saving model to DeepFM.h5\n",
      "Epoch 660/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3866 - auc: 0.708 - ETA: 0s - loss: 0.4102 - auc: 0.650 - 0s 22us/step - loss: 0.4105 - auc: 0.6369 - val_loss: 0.4081 - val_auc: 0.6697\n",
      "\n",
      "Epoch 00660: val_loss improved from 0.40817 to 0.40813, saving model to DeepFM.h5\n",
      "Epoch 661/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4363 - auc: 0.603 - ETA: 0s - loss: 0.4179 - auc: 0.621 - 0s 22us/step - loss: 0.4111 - auc: 0.6251 - val_loss: 0.4081 - val_auc: 0.6695\n",
      "\n",
      "Epoch 00661: val_loss improved from 0.40813 to 0.40810, saving model to DeepFM.h5\n",
      "Epoch 662/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3677 - auc: 0.653 - ETA: 0s - loss: 0.4130 - auc: 0.608 - 0s 23us/step - loss: 0.4142 - auc: 0.6113 - val_loss: 0.4081 - val_auc: 0.6679\n",
      "\n",
      "Epoch 00662: val_loss improved from 0.40810 to 0.40807, saving model to DeepFM.h5\n",
      "Epoch 663/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4370 - auc: 0.581 - ETA: 0s - loss: 0.4207 - auc: 0.625 - 0s 22us/step - loss: 0.4120 - auc: 0.6293 - val_loss: 0.4081 - val_auc: 0.6687\n",
      "\n",
      "Epoch 00663: val_loss improved from 0.40807 to 0.40805, saving model to DeepFM.h5\n",
      "Epoch 664/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4243 - auc: 0.608 - ETA: 0s - loss: 0.4030 - auc: 0.624 - 0s 21us/step - loss: 0.4111 - auc: 0.6309 - val_loss: 0.4080 - val_auc: 0.6689\n",
      "\n",
      "Epoch 00664: val_loss improved from 0.40805 to 0.40803, saving model to DeepFM.h5\n",
      "Epoch 665/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3994 - auc: 0.590 - ETA: 0s - loss: 0.4196 - auc: 0.631 - 0s 22us/step - loss: 0.4108 - auc: 0.6340 - val_loss: 0.4080 - val_auc: 0.6700\n",
      "\n",
      "Epoch 00665: val_loss improved from 0.40803 to 0.40801, saving model to DeepFM.h5\n",
      "Epoch 666/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4253 - auc: 0.641 - ETA: 0s - loss: 0.4195 - auc: 0.624 - 0s 22us/step - loss: 0.4108 - auc: 0.6393 - val_loss: 0.4080 - val_auc: 0.6694\n",
      "\n",
      "Epoch 00666: val_loss improved from 0.40801 to 0.40798, saving model to DeepFM.h5\n",
      "Epoch 667/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3471 - auc: 0.621 - ETA: 0s - loss: 0.4085 - auc: 0.644 - 0s 24us/step - loss: 0.4084 - auc: 0.6471 - val_loss: 0.4079 - val_auc: 0.6703\n",
      "\n",
      "Epoch 00667: val_loss improved from 0.40798 to 0.40794, saving model to DeepFM.h5\n",
      "Epoch 668/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4507 - auc: 0.683 - ETA: 0s - loss: 0.4085 - auc: 0.638 - 0s 21us/step - loss: 0.4087 - auc: 0.6517 - val_loss: 0.4079 - val_auc: 0.6711\n",
      "\n",
      "Epoch 00668: val_loss improved from 0.40794 to 0.40791, saving model to DeepFM.h5\n",
      "Epoch 669/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4010 - auc: 0.544 - ETA: 0s - loss: 0.4071 - auc: 0.635 - 0s 22us/step - loss: 0.4105 - auc: 0.6376 - val_loss: 0.4079 - val_auc: 0.6711\n",
      "\n",
      "Epoch 00669: val_loss improved from 0.40791 to 0.40788, saving model to DeepFM.h5\n",
      "Epoch 670/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4384 - auc: 0.731 - ETA: 0s - loss: 0.4136 - auc: 0.640 - 0s 21us/step - loss: 0.4107 - auc: 0.6360 - val_loss: 0.4079 - val_auc: 0.6703\n",
      "\n",
      "Epoch 00670: val_loss improved from 0.40788 to 0.40787, saving model to DeepFM.h5\n",
      "Epoch 671/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3423 - auc: 0.609 - ETA: 0s - loss: 0.4065 - auc: 0.632 - 0s 23us/step - loss: 0.4128 - auc: 0.6285 - val_loss: 0.4078 - val_auc: 0.6692\n",
      "\n",
      "Epoch 00671: val_loss improved from 0.40787 to 0.40785, saving model to DeepFM.h5\n",
      "Epoch 672/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3987 - auc: 0.690 - ETA: 0s - loss: 0.4099 - auc: 0.645 - 0s 22us/step - loss: 0.4105 - auc: 0.6422 - val_loss: 0.4078 - val_auc: 0.6706\n",
      "\n",
      "Epoch 00672: val_loss improved from 0.40785 to 0.40780, saving model to DeepFM.h5\n",
      "Epoch 673/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4438 - auc: 0.661 - ETA: 0s - loss: 0.4212 - auc: 0.628 - 0s 22us/step - loss: 0.4105 - auc: 0.6334 - val_loss: 0.4078 - val_auc: 0.6704\n",
      "\n",
      "Epoch 00673: val_loss improved from 0.40780 to 0.40777, saving model to DeepFM.h5\n",
      "Epoch 674/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4118 - auc: 0.644 - ETA: 0s - loss: 0.3973 - auc: 0.653 - 0s 21us/step - loss: 0.4078 - auc: 0.6571 - val_loss: 0.4077 - val_auc: 0.6719\n",
      "\n",
      "Epoch 00674: val_loss improved from 0.40777 to 0.40771, saving model to DeepFM.h5\n",
      "Epoch 675/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4347 - auc: 0.615 - ETA: 0s - loss: 0.4035 - auc: 0.619 - 0s 22us/step - loss: 0.4095 - auc: 0.6397 - val_loss: 0.4077 - val_auc: 0.6718\n",
      "\n",
      "Epoch 00675: val_loss improved from 0.40771 to 0.40767, saving model to DeepFM.h5\n",
      "Epoch 676/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3856 - auc: 0.655 - ETA: 0s - loss: 0.3882 - auc: 0.664 - 0s 21us/step - loss: 0.4091 - auc: 0.6501 - val_loss: 0.4076 - val_auc: 0.6713\n",
      "\n",
      "Epoch 00676: val_loss improved from 0.40767 to 0.40764, saving model to DeepFM.h5\n",
      "Epoch 677/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4355 - auc: 0.665 - ETA: 0s - loss: 0.4180 - auc: 0.639 - 0s 22us/step - loss: 0.4117 - auc: 0.6347 - val_loss: 0.4076 - val_auc: 0.6711\n",
      "\n",
      "Epoch 00677: val_loss improved from 0.40764 to 0.40761, saving model to DeepFM.h5\n",
      "Epoch 678/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3324 - auc: 0.585 - ETA: 0s - loss: 0.4115 - auc: 0.635 - 0s 22us/step - loss: 0.4096 - auc: 0.6450 - val_loss: 0.4076 - val_auc: 0.6699\n",
      "\n",
      "Epoch 00678: val_loss improved from 0.40761 to 0.40758, saving model to DeepFM.h5\n",
      "Epoch 679/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4439 - auc: 0.598 - ETA: 0s - loss: 0.4173 - auc: 0.631 - 0s 22us/step - loss: 0.4112 - auc: 0.6307 - val_loss: 0.4076 - val_auc: 0.6700\n",
      "\n",
      "Epoch 00679: val_loss improved from 0.40758 to 0.40755, saving model to DeepFM.h5\n",
      "Epoch 680/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4466 - auc: 0.592 - ETA: 0s - loss: 0.4138 - auc: 0.624 - 0s 22us/step - loss: 0.4111 - auc: 0.6351 - val_loss: 0.4075 - val_auc: 0.6714\n",
      "\n",
      "Epoch 00680: val_loss improved from 0.40755 to 0.40752, saving model to DeepFM.h5\n",
      "Epoch 681/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4216 - auc: 0.685 - ETA: 0s - loss: 0.4061 - auc: 0.651 - 0s 20us/step - loss: 0.4096 - auc: 0.6429 - val_loss: 0.4075 - val_auc: 0.6712\n",
      "\n",
      "Epoch 00681: val_loss improved from 0.40752 to 0.40747, saving model to DeepFM.h5\n",
      "Epoch 682/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3851 - auc: 0.703 - ETA: 0s - loss: 0.4136 - auc: 0.647 - 0s 22us/step - loss: 0.4100 - auc: 0.6416 - val_loss: 0.4074 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00682: val_loss improved from 0.40747 to 0.40744, saving model to DeepFM.h5\n",
      "Epoch 683/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4486 - auc: 0.664 - ETA: 0s - loss: 0.4211 - auc: 0.647 - 0s 23us/step - loss: 0.4072 - auc: 0.6516 - val_loss: 0.4074 - val_auc: 0.6703\n",
      "\n",
      "Epoch 00683: val_loss improved from 0.40744 to 0.40739, saving model to DeepFM.h5\n",
      "Epoch 684/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3670 - auc: 0.646 - ETA: 0s - loss: 0.3938 - auc: 0.623 - 0s 22us/step - loss: 0.4103 - auc: 0.6352 - val_loss: 0.4074 - val_auc: 0.6704\n",
      "\n",
      "Epoch 00684: val_loss improved from 0.40739 to 0.40735, saving model to DeepFM.h5\n",
      "Epoch 685/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4106 - auc: 0.676 - ETA: 0s - loss: 0.4182 - auc: 0.623 - 0s 22us/step - loss: 0.4116 - auc: 0.6228 - val_loss: 0.4073 - val_auc: 0.6708\n",
      "\n",
      "Epoch 00685: val_loss improved from 0.40735 to 0.40731, saving model to DeepFM.h5\n",
      "Epoch 686/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4013 - auc: 0.651 - ETA: 0s - loss: 0.4093 - auc: 0.627 - 0s 22us/step - loss: 0.4097 - auc: 0.6400 - val_loss: 0.4073 - val_auc: 0.6702\n",
      "\n",
      "Epoch 00686: val_loss improved from 0.40731 to 0.40727, saving model to DeepFM.h5\n",
      "Epoch 687/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.564 - ETA: 0s - loss: 0.4169 - auc: 0.640 - 0s 22us/step - loss: 0.4103 - auc: 0.6476 - val_loss: 0.4073 - val_auc: 0.6707\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.40727\n",
      "Epoch 688/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4797 - auc: 0.622 - ETA: 0s - loss: 0.4105 - auc: 0.680 - 0s 23us/step - loss: 0.4055 - auc: 0.6652 - val_loss: 0.4073 - val_auc: 0.6721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00688: val_loss improved from 0.40727 to 0.40725, saving model to DeepFM.h5\n",
      "Epoch 689/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4328 - auc: 0.626 - ETA: 0s - loss: 0.4141 - auc: 0.636 - 0s 23us/step - loss: 0.4086 - auc: 0.6471 - val_loss: 0.4072 - val_auc: 0.6712\n",
      "\n",
      "Epoch 00689: val_loss improved from 0.40725 to 0.40723, saving model to DeepFM.h5\n",
      "Epoch 690/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3554 - auc: 0.675 - ETA: 0s - loss: 0.4083 - auc: 0.649 - 0s 21us/step - loss: 0.4078 - auc: 0.6448 - val_loss: 0.4072 - val_auc: 0.6707\n",
      "\n",
      "Epoch 00690: val_loss improved from 0.40723 to 0.40721, saving model to DeepFM.h5\n",
      "Epoch 691/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4009 - auc: 0.625 - ETA: 0s - loss: 0.3946 - auc: 0.653 - 0s 23us/step - loss: 0.4095 - auc: 0.6467 - val_loss: 0.4072 - val_auc: 0.6707\n",
      "\n",
      "Epoch 00691: val_loss improved from 0.40721 to 0.40718, saving model to DeepFM.h5\n",
      "Epoch 692/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4021 - auc: 0.694 - ETA: 0s - loss: 0.4127 - auc: 0.634 - 0s 22us/step - loss: 0.4112 - auc: 0.6326 - val_loss: 0.4072 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00692: val_loss improved from 0.40718 to 0.40715, saving model to DeepFM.h5\n",
      "Epoch 693/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3806 - auc: 0.649 - ETA: 0s - loss: 0.4168 - auc: 0.644 - 0s 22us/step - loss: 0.4082 - auc: 0.6537 - val_loss: 0.4071 - val_auc: 0.6709\n",
      "\n",
      "Epoch 00693: val_loss improved from 0.40715 to 0.40710, saving model to DeepFM.h5\n",
      "Epoch 694/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4433 - auc: 0.579 - ETA: 0s - loss: 0.4159 - auc: 0.637 - 0s 25us/step - loss: 0.4103 - auc: 0.6391 - val_loss: 0.4071 - val_auc: 0.6708\n",
      "\n",
      "Epoch 00694: val_loss improved from 0.40710 to 0.40706, saving model to DeepFM.h5\n",
      "Epoch 695/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3884 - auc: 0.621 - ETA: 0s - loss: 0.4087 - auc: 0.638 - 0s 22us/step - loss: 0.4109 - auc: 0.6387 - val_loss: 0.4070 - val_auc: 0.6715\n",
      "\n",
      "Epoch 00695: val_loss improved from 0.40706 to 0.40704, saving model to DeepFM.h5\n",
      "Epoch 696/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4083 - auc: 0.705 - ETA: 0s - loss: 0.4225 - auc: 0.644 - 0s 22us/step - loss: 0.4111 - auc: 0.6313 - val_loss: 0.4070 - val_auc: 0.6714\n",
      "\n",
      "Epoch 00696: val_loss improved from 0.40704 to 0.40704, saving model to DeepFM.h5\n",
      "Epoch 697/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4144 - auc: 0.632 - ETA: 0s - loss: 0.4165 - auc: 0.659 - 0s 22us/step - loss: 0.4090 - auc: 0.6515 - val_loss: 0.4070 - val_auc: 0.6712\n",
      "\n",
      "Epoch 00697: val_loss improved from 0.40704 to 0.40699, saving model to DeepFM.h5\n",
      "Epoch 698/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4229 - auc: 0.691 - ETA: 0s - loss: 0.4113 - auc: 0.640 - 0s 21us/step - loss: 0.4093 - auc: 0.6416 - val_loss: 0.4070 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00698: val_loss improved from 0.40699 to 0.40695, saving model to DeepFM.h5\n",
      "Epoch 699/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.611 - ETA: 0s - loss: 0.4056 - auc: 0.640 - 0s 23us/step - loss: 0.4075 - auc: 0.6510 - val_loss: 0.4069 - val_auc: 0.6714\n",
      "\n",
      "Epoch 00699: val_loss improved from 0.40695 to 0.40690, saving model to DeepFM.h5\n",
      "Epoch 700/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4244 - auc: 0.592 - ETA: 0s - loss: 0.3998 - auc: 0.649 - 0s 22us/step - loss: 0.4083 - auc: 0.6454 - val_loss: 0.4069 - val_auc: 0.6720\n",
      "\n",
      "Epoch 00700: val_loss improved from 0.40690 to 0.40687, saving model to DeepFM.h5\n",
      "Epoch 701/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.735 - ETA: 0s - loss: 0.4133 - auc: 0.628 - 0s 22us/step - loss: 0.4112 - auc: 0.6317 - val_loss: 0.4068 - val_auc: 0.6712\n",
      "\n",
      "Epoch 00701: val_loss improved from 0.40687 to 0.40683, saving model to DeepFM.h5\n",
      "Epoch 702/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4161 - auc: 0.560 - ETA: 0s - loss: 0.4034 - auc: 0.639 - 0s 23us/step - loss: 0.4098 - auc: 0.6379 - val_loss: 0.4068 - val_auc: 0.6714\n",
      "\n",
      "Epoch 00702: val_loss improved from 0.40683 to 0.40681, saving model to DeepFM.h5\n",
      "Epoch 703/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4797 - auc: 0.585 - ETA: 0s - loss: 0.4185 - auc: 0.639 - 0s 22us/step - loss: 0.4088 - auc: 0.6447 - val_loss: 0.4068 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00703: val_loss improved from 0.40681 to 0.40678, saving model to DeepFM.h5\n",
      "Epoch 704/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3695 - auc: 0.663 - ETA: 0s - loss: 0.4003 - auc: 0.657 - 0s 22us/step - loss: 0.4080 - auc: 0.6512 - val_loss: 0.4067 - val_auc: 0.6722\n",
      "\n",
      "Epoch 00704: val_loss improved from 0.40678 to 0.40675, saving model to DeepFM.h5\n",
      "Epoch 705/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4146 - auc: 0.670 - ETA: 0s - loss: 0.4071 - auc: 0.673 - 0s 23us/step - loss: 0.4060 - auc: 0.6634 - val_loss: 0.4067 - val_auc: 0.6720\n",
      "\n",
      "Epoch 00705: val_loss improved from 0.40675 to 0.40671, saving model to DeepFM.h5\n",
      "Epoch 706/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3265 - auc: 0.640 - ETA: 0s - loss: 0.4062 - auc: 0.628 - 0s 21us/step - loss: 0.4096 - auc: 0.6386 - val_loss: 0.4067 - val_auc: 0.6711\n",
      "\n",
      "Epoch 00706: val_loss improved from 0.40671 to 0.40668, saving model to DeepFM.h5\n",
      "Epoch 707/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4460 - auc: 0.671 - ETA: 0s - loss: 0.4112 - auc: 0.645 - 0s 22us/step - loss: 0.4095 - auc: 0.6415 - val_loss: 0.4067 - val_auc: 0.6717\n",
      "\n",
      "Epoch 00707: val_loss improved from 0.40668 to 0.40666, saving model to DeepFM.h5\n",
      "Epoch 708/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3998 - auc: 0.619 - ETA: 0s - loss: 0.4012 - auc: 0.658 - 0s 23us/step - loss: 0.4074 - auc: 0.6497 - val_loss: 0.4066 - val_auc: 0.6710\n",
      "\n",
      "Epoch 00708: val_loss improved from 0.40666 to 0.40662, saving model to DeepFM.h5\n",
      "Epoch 709/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3867 - auc: 0.621 - ETA: 0s - loss: 0.4163 - auc: 0.640 - 0s 21us/step - loss: 0.4089 - auc: 0.6457 - val_loss: 0.4066 - val_auc: 0.6718\n",
      "\n",
      "Epoch 00709: val_loss improved from 0.40662 to 0.40658, saving model to DeepFM.h5\n",
      "Epoch 710/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3765 - auc: 0.697 - ETA: 0s - loss: 0.4074 - auc: 0.647 - 0s 22us/step - loss: 0.4084 - auc: 0.6508 - val_loss: 0.4065 - val_auc: 0.6719\n",
      "\n",
      "Epoch 00710: val_loss improved from 0.40658 to 0.40654, saving model to DeepFM.h5\n",
      "Epoch 711/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3996 - auc: 0.615 - ETA: 0s - loss: 0.4000 - auc: 0.646 - 0s 23us/step - loss: 0.4080 - auc: 0.6452 - val_loss: 0.4065 - val_auc: 0.6722\n",
      "\n",
      "Epoch 00711: val_loss improved from 0.40654 to 0.40650, saving model to DeepFM.h5\n",
      "Epoch 712/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4881 - auc: 0.668 - ETA: 0s - loss: 0.4177 - auc: 0.650 - 0s 22us/step - loss: 0.4107 - auc: 0.6422 - val_loss: 0.4065 - val_auc: 0.6722\n",
      "\n",
      "Epoch 00712: val_loss improved from 0.40650 to 0.40646, saving model to DeepFM.h5\n",
      "Epoch 713/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4579 - auc: 0.699 - ETA: 0s - loss: 0.4082 - auc: 0.650 - 0s 22us/step - loss: 0.4074 - auc: 0.6521 - val_loss: 0.4064 - val_auc: 0.6725\n",
      "\n",
      "Epoch 00713: val_loss improved from 0.40646 to 0.40642, saving model to DeepFM.h5\n",
      "Epoch 714/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4777 - auc: 0.673 - ETA: 0s - loss: 0.4283 - auc: 0.636 - 0s 24us/step - loss: 0.4103 - auc: 0.6400 - val_loss: 0.4064 - val_auc: 0.6733\n",
      "\n",
      "Epoch 00714: val_loss improved from 0.40642 to 0.40637, saving model to DeepFM.h5\n",
      "Epoch 715/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3825 - auc: 0.615 - ETA: 0s - loss: 0.3985 - auc: 0.654 - 0s 22us/step - loss: 0.4073 - auc: 0.6557 - val_loss: 0.4063 - val_auc: 0.6728\n",
      "\n",
      "Epoch 00715: val_loss improved from 0.40637 to 0.40632, saving model to DeepFM.h5\n",
      "Epoch 716/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4291 - auc: 0.671 - ETA: 0s - loss: 0.4017 - auc: 0.640 - 0s 22us/step - loss: 0.4081 - auc: 0.6444 - val_loss: 0.4063 - val_auc: 0.6719\n",
      "\n",
      "Epoch 00716: val_loss improved from 0.40632 to 0.40628, saving model to DeepFM.h5\n",
      "Epoch 717/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4136 - auc: 0.596 - ETA: 0s - loss: 0.4058 - auc: 0.640 - 0s 21us/step - loss: 0.4091 - auc: 0.6444 - val_loss: 0.4063 - val_auc: 0.6721\n",
      "\n",
      "Epoch 00717: val_loss improved from 0.40628 to 0.40625, saving model to DeepFM.h5\n",
      "Epoch 718/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4064 - auc: 0.615 - ETA: 0s - loss: 0.4157 - auc: 0.650 - 0s 22us/step - loss: 0.4073 - auc: 0.6560 - val_loss: 0.4062 - val_auc: 0.6721\n",
      "\n",
      "Epoch 00718: val_loss improved from 0.40625 to 0.40622, saving model to DeepFM.h5\n",
      "Epoch 719/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4280 - auc: 0.657 - ETA: 0s - loss: 0.4110 - auc: 0.658 - 0s 21us/step - loss: 0.4074 - auc: 0.6553 - val_loss: 0.4062 - val_auc: 0.6713\n",
      "\n",
      "Epoch 00719: val_loss improved from 0.40622 to 0.40618, saving model to DeepFM.h5\n",
      "Epoch 720/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4342 - auc: 0.650 - ETA: 0s - loss: 0.4008 - auc: 0.637 - 0s 22us/step - loss: 0.4073 - auc: 0.6490 - val_loss: 0.4061 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00720: val_loss improved from 0.40618 to 0.40613, saving model to DeepFM.h5\n",
      "Epoch 721/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4257 - auc: 0.659 - ETA: 0s - loss: 0.4112 - auc: 0.640 - 0s 21us/step - loss: 0.4101 - auc: 0.6384 - val_loss: 0.4061 - val_auc: 0.6724\n",
      "\n",
      "Epoch 00721: val_loss improved from 0.40613 to 0.40613, saving model to DeepFM.h5\n",
      "Epoch 722/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3878 - auc: 0.679 - ETA: 0s - loss: 0.4041 - auc: 0.667 - 0s 23us/step - loss: 0.4072 - auc: 0.6557 - val_loss: 0.4061 - val_auc: 0.6719\n",
      "\n",
      "Epoch 00722: val_loss improved from 0.40613 to 0.40609, saving model to DeepFM.h5\n",
      "Epoch 723/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4323 - auc: 0.598 - ETA: 0s - loss: 0.4142 - auc: 0.644 - 0s 21us/step - loss: 0.4082 - auc: 0.6525 - val_loss: 0.4061 - val_auc: 0.6738\n",
      "\n",
      "Epoch 00723: val_loss improved from 0.40609 to 0.40608, saving model to DeepFM.h5\n",
      "Epoch 724/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3896 - auc: 0.636 - ETA: 0s - loss: 0.4201 - auc: 0.617 - 0s 23us/step - loss: 0.4107 - auc: 0.6276 - val_loss: 0.4060 - val_auc: 0.6736\n",
      "\n",
      "Epoch 00724: val_loss improved from 0.40608 to 0.40605, saving model to DeepFM.h5\n",
      "Epoch 725/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3921 - auc: 0.583 - ETA: 0s - loss: 0.4128 - auc: 0.660 - 0s 21us/step - loss: 0.4067 - auc: 0.6566 - val_loss: 0.4060 - val_auc: 0.6734\n",
      "\n",
      "Epoch 00725: val_loss improved from 0.40605 to 0.40601, saving model to DeepFM.h5\n",
      "Epoch 726/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4283 - auc: 0.692 - ETA: 0s - loss: 0.4089 - auc: 0.664 - 0s 23us/step - loss: 0.4068 - auc: 0.6609 - val_loss: 0.4060 - val_auc: 0.6723\n",
      "\n",
      "Epoch 00726: val_loss improved from 0.40601 to 0.40597, saving model to DeepFM.h5\n",
      "Epoch 727/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4106 - auc: 0.581 - ETA: 0s - loss: 0.4095 - auc: 0.653 - 0s 22us/step - loss: 0.4078 - auc: 0.6504 - val_loss: 0.4059 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00727: val_loss improved from 0.40597 to 0.40593, saving model to DeepFM.h5\n",
      "Epoch 728/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4430 - auc: 0.673 - ETA: 0s - loss: 0.3980 - auc: 0.644 - 0s 21us/step - loss: 0.4077 - auc: 0.6513 - val_loss: 0.4059 - val_auc: 0.6735\n",
      "\n",
      "Epoch 00728: val_loss improved from 0.40593 to 0.40588, saving model to DeepFM.h5\n",
      "Epoch 729/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.654 - ETA: 0s - loss: 0.4134 - auc: 0.646 - 0s 22us/step - loss: 0.4071 - auc: 0.6520 - val_loss: 0.4059 - val_auc: 0.6728\n",
      "\n",
      "Epoch 00729: val_loss improved from 0.40588 to 0.40585, saving model to DeepFM.h5\n",
      "Epoch 730/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4281 - auc: 0.609 - ETA: 0s - loss: 0.4156 - auc: 0.643 - 0s 22us/step - loss: 0.4081 - auc: 0.6507 - val_loss: 0.4058 - val_auc: 0.6720\n",
      "\n",
      "Epoch 00730: val_loss improved from 0.40585 to 0.40582, saving model to DeepFM.h5\n",
      "Epoch 731/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3949 - auc: 0.672 - ETA: 0s - loss: 0.4064 - auc: 0.680 - 0s 22us/step - loss: 0.4057 - auc: 0.6643 - val_loss: 0.4058 - val_auc: 0.6721\n",
      "\n",
      "Epoch 00731: val_loss improved from 0.40582 to 0.40580, saving model to DeepFM.h5\n",
      "Epoch 732/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3460 - auc: 0.579 - ETA: 0s - loss: 0.4027 - auc: 0.632 - 0s 22us/step - loss: 0.4085 - auc: 0.6466 - val_loss: 0.4058 - val_auc: 0.6733\n",
      "\n",
      "Epoch 00732: val_loss improved from 0.40580 to 0.40576, saving model to DeepFM.h5\n",
      "Epoch 733/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3829 - auc: 0.599 - ETA: 0s - loss: 0.4035 - auc: 0.637 - 0s 20us/step - loss: 0.4093 - auc: 0.6356 - val_loss: 0.4057 - val_auc: 0.6731\n",
      "\n",
      "Epoch 00733: val_loss improved from 0.40576 to 0.40573, saving model to DeepFM.h5\n",
      "Epoch 734/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3859 - auc: 0.674 - ETA: 0s - loss: 0.4028 - auc: 0.645 - 0s 22us/step - loss: 0.4074 - auc: 0.6508 - val_loss: 0.4057 - val_auc: 0.6722\n",
      "\n",
      "Epoch 00734: val_loss improved from 0.40573 to 0.40568, saving model to DeepFM.h5\n",
      "Epoch 735/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3577 - auc: 0.702 - ETA: 0s - loss: 0.4099 - auc: 0.649 - 0s 22us/step - loss: 0.4086 - auc: 0.6487 - val_loss: 0.4056 - val_auc: 0.6734\n",
      "\n",
      "Epoch 00735: val_loss improved from 0.40568 to 0.40564, saving model to DeepFM.h5\n",
      "Epoch 736/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4160 - auc: 0.631 - ETA: 0s - loss: 0.4158 - auc: 0.646 - 0s 22us/step - loss: 0.4094 - auc: 0.6443 - val_loss: 0.4056 - val_auc: 0.6729\n",
      "\n",
      "Epoch 00736: val_loss improved from 0.40564 to 0.40561, saving model to DeepFM.h5\n",
      "Epoch 737/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3314 - auc: 0.751 - ETA: 0s - loss: 0.4030 - auc: 0.645 - 0s 22us/step - loss: 0.4098 - auc: 0.6377 - val_loss: 0.4056 - val_auc: 0.6727\n",
      "\n",
      "Epoch 00737: val_loss improved from 0.40561 to 0.40558, saving model to DeepFM.h5\n",
      "Epoch 738/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4107 - auc: 0.642 - ETA: 0s - loss: 0.4134 - auc: 0.651 - 0s 22us/step - loss: 0.4055 - auc: 0.6627 - val_loss: 0.4055 - val_auc: 0.6723\n",
      "\n",
      "Epoch 00738: val_loss improved from 0.40558 to 0.40554, saving model to DeepFM.h5\n",
      "Epoch 739/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4745 - auc: 0.702 - ETA: 0s - loss: 0.4154 - auc: 0.653 - 0s 21us/step - loss: 0.4070 - auc: 0.6477 - val_loss: 0.4055 - val_auc: 0.6732\n",
      "\n",
      "Epoch 00739: val_loss improved from 0.40554 to 0.40551, saving model to DeepFM.h5\n",
      "Epoch 740/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3812 - auc: 0.713 - ETA: 0s - loss: 0.4026 - auc: 0.665 - 0s 24us/step - loss: 0.4049 - auc: 0.6622 - val_loss: 0.4055 - val_auc: 0.6724\n",
      "\n",
      "Epoch 00740: val_loss improved from 0.40551 to 0.40548, saving model to DeepFM.h5\n",
      "Epoch 741/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3866 - auc: 0.648 - ETA: 0s - loss: 0.4147 - auc: 0.638 - 0s 21us/step - loss: 0.4089 - auc: 0.6444 - val_loss: 0.4055 - val_auc: 0.6735\n",
      "\n",
      "Epoch 00741: val_loss improved from 0.40548 to 0.40547, saving model to DeepFM.h5\n",
      "Epoch 742/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3488 - auc: 0.697 - ETA: 0s - loss: 0.4043 - auc: 0.649 - 0s 23us/step - loss: 0.4086 - auc: 0.6391 - val_loss: 0.4054 - val_auc: 0.6732\n",
      "\n",
      "Epoch 00742: val_loss improved from 0.40547 to 0.40544, saving model to DeepFM.h5\n",
      "Epoch 743/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3549 - auc: 0.647 - ETA: 0s - loss: 0.4033 - auc: 0.654 - 0s 21us/step - loss: 0.4055 - auc: 0.6576 - val_loss: 0.4054 - val_auc: 0.6733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00743: val_loss improved from 0.40544 to 0.40539, saving model to DeepFM.h5\n",
      "Epoch 744/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3859 - auc: 0.617 - ETA: 0s - loss: 0.4125 - auc: 0.628 - 0s 22us/step - loss: 0.4093 - auc: 0.6425 - val_loss: 0.4053 - val_auc: 0.6735\n",
      "\n",
      "Epoch 00744: val_loss improved from 0.40539 to 0.40534, saving model to DeepFM.h5\n",
      "Epoch 745/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4142 - auc: 0.612 - ETA: 0s - loss: 0.4065 - auc: 0.638 - 0s 22us/step - loss: 0.4077 - auc: 0.6485 - val_loss: 0.4053 - val_auc: 0.6729\n",
      "\n",
      "Epoch 00745: val_loss improved from 0.40534 to 0.40530, saving model to DeepFM.h5\n",
      "Epoch 746/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4581 - auc: 0.667 - ETA: 0s - loss: 0.4046 - auc: 0.649 - 0s 21us/step - loss: 0.4083 - auc: 0.6470 - val_loss: 0.4053 - val_auc: 0.6729\n",
      "\n",
      "Epoch 00746: val_loss improved from 0.40530 to 0.40526, saving model to DeepFM.h5\n",
      "Epoch 747/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3632 - auc: 0.742 - ETA: 0s - loss: 0.3973 - auc: 0.676 - 0s 23us/step - loss: 0.4061 - auc: 0.6640 - val_loss: 0.4052 - val_auc: 0.6732\n",
      "\n",
      "Epoch 00747: val_loss improved from 0.40526 to 0.40522, saving model to DeepFM.h5\n",
      "Epoch 748/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3512 - auc: 0.680 - ETA: 0s - loss: 0.4142 - auc: 0.647 - 0s 22us/step - loss: 0.4093 - auc: 0.6422 - val_loss: 0.4052 - val_auc: 0.6740\n",
      "\n",
      "Epoch 00748: val_loss improved from 0.40522 to 0.40519, saving model to DeepFM.h5\n",
      "Epoch 749/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3502 - auc: 0.711 - ETA: 0s - loss: 0.4045 - auc: 0.675 - 0s 22us/step - loss: 0.4077 - auc: 0.6548 - val_loss: 0.4052 - val_auc: 0.6734\n",
      "\n",
      "Epoch 00749: val_loss improved from 0.40519 to 0.40517, saving model to DeepFM.h5\n",
      "Epoch 750/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3848 - auc: 0.685 - ETA: 0s - loss: 0.4057 - auc: 0.665 - 0s 23us/step - loss: 0.4053 - auc: 0.6664 - val_loss: 0.4052 - val_auc: 0.6742\n",
      "\n",
      "Epoch 00750: val_loss improved from 0.40517 to 0.40515, saving model to DeepFM.h5\n",
      "Epoch 751/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4171 - auc: 0.664 - ETA: 0s - loss: 0.4021 - auc: 0.644 - 0s 22us/step - loss: 0.4084 - auc: 0.6481 - val_loss: 0.4051 - val_auc: 0.6745\n",
      "\n",
      "Epoch 00751: val_loss improved from 0.40515 to 0.40513, saving model to DeepFM.h5\n",
      "Epoch 752/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4153 - auc: 0.562 - ETA: 0s - loss: 0.4084 - auc: 0.652 - 0s 22us/step - loss: 0.4076 - auc: 0.6459 - val_loss: 0.4051 - val_auc: 0.6752\n",
      "\n",
      "Epoch 00752: val_loss improved from 0.40513 to 0.40510, saving model to DeepFM.h5\n",
      "Epoch 753/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4502 - auc: 0.597 - ETA: 0s - loss: 0.4022 - auc: 0.637 - 0s 23us/step - loss: 0.4055 - auc: 0.6521 - val_loss: 0.4050 - val_auc: 0.6746\n",
      "\n",
      "Epoch 00753: val_loss improved from 0.40510 to 0.40504, saving model to DeepFM.h5\n",
      "Epoch 754/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3790 - auc: 0.643 - ETA: 0s - loss: 0.4097 - auc: 0.662 - 0s 21us/step - loss: 0.4062 - auc: 0.6598 - val_loss: 0.4050 - val_auc: 0.6747\n",
      "\n",
      "Epoch 00754: val_loss improved from 0.40504 to 0.40500, saving model to DeepFM.h5\n",
      "Epoch 755/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3954 - auc: 0.599 - ETA: 0s - loss: 0.4069 - auc: 0.645 - 0s 22us/step - loss: 0.4053 - auc: 0.6544 - val_loss: 0.4050 - val_auc: 0.6758\n",
      "\n",
      "Epoch 00755: val_loss improved from 0.40500 to 0.40495, saving model to DeepFM.h5\n",
      "Epoch 756/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3449 - auc: 0.760 - ETA: 0s - loss: 0.4016 - auc: 0.669 - 0s 23us/step - loss: 0.4047 - auc: 0.6638 - val_loss: 0.4049 - val_auc: 0.6752\n",
      "\n",
      "Epoch 00756: val_loss improved from 0.40495 to 0.40491, saving model to DeepFM.h5\n",
      "Epoch 757/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4492 - auc: 0.698 - ETA: 0s - loss: 0.4075 - auc: 0.636 - 0s 20us/step - loss: 0.4104 - auc: 0.6333 - val_loss: 0.4049 - val_auc: 0.6751\n",
      "\n",
      "Epoch 00757: val_loss improved from 0.40491 to 0.40489, saving model to DeepFM.h5\n",
      "Epoch 758/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4143 - auc: 0.690 - ETA: 0s - loss: 0.4081 - auc: 0.670 - 0s 23us/step - loss: 0.4054 - auc: 0.6637 - val_loss: 0.4049 - val_auc: 0.6752\n",
      "\n",
      "Epoch 00758: val_loss improved from 0.40489 to 0.40487, saving model to DeepFM.h5\n",
      "Epoch 759/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3713 - auc: 0.632 - ETA: 0s - loss: 0.4174 - auc: 0.653 - 0s 21us/step - loss: 0.4057 - auc: 0.6580 - val_loss: 0.4048 - val_auc: 0.6766\n",
      "\n",
      "Epoch 00759: val_loss improved from 0.40487 to 0.40483, saving model to DeepFM.h5\n",
      "Epoch 760/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4131 - auc: 0.614 - ETA: 0s - loss: 0.4070 - auc: 0.648 - 0s 21us/step - loss: 0.4072 - auc: 0.6570 - val_loss: 0.4048 - val_auc: 0.6767\n",
      "\n",
      "Epoch 00760: val_loss improved from 0.40483 to 0.40480, saving model to DeepFM.h5\n",
      "Epoch 761/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4102 - auc: 0.690 - ETA: 0s - loss: 0.4120 - auc: 0.651 - 0s 22us/step - loss: 0.4073 - auc: 0.6493 - val_loss: 0.4048 - val_auc: 0.6763\n",
      "\n",
      "Epoch 00761: val_loss improved from 0.40480 to 0.40476, saving model to DeepFM.h5\n",
      "Epoch 762/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4423 - auc: 0.533 - ETA: 0s - loss: 0.4032 - auc: 0.644 - 0s 22us/step - loss: 0.4060 - auc: 0.6565 - val_loss: 0.4047 - val_auc: 0.6762\n",
      "\n",
      "Epoch 00762: val_loss improved from 0.40476 to 0.40474, saving model to DeepFM.h5\n",
      "Epoch 763/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4563 - auc: 0.626 - ETA: 0s - loss: 0.4054 - auc: 0.668 - 0s 22us/step - loss: 0.4036 - auc: 0.6764 - val_loss: 0.4047 - val_auc: 0.6748\n",
      "\n",
      "Epoch 00763: val_loss improved from 0.40474 to 0.40470, saving model to DeepFM.h5\n",
      "Epoch 764/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4310 - auc: 0.720 - ETA: 0s - loss: 0.4016 - auc: 0.655 - 0s 22us/step - loss: 0.4073 - auc: 0.6499 - val_loss: 0.4047 - val_auc: 0.6757\n",
      "\n",
      "Epoch 00764: val_loss improved from 0.40470 to 0.40465, saving model to DeepFM.h5\n",
      "Epoch 765/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4130 - auc: 0.620 - ETA: 0s - loss: 0.4081 - auc: 0.662 - 0s 22us/step - loss: 0.4054 - auc: 0.6642 - val_loss: 0.4046 - val_auc: 0.6759\n",
      "\n",
      "Epoch 00765: val_loss improved from 0.40465 to 0.40462, saving model to DeepFM.h5\n",
      "Epoch 766/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4073 - auc: 0.576 - ETA: 0s - loss: 0.3888 - auc: 0.653 - 0s 23us/step - loss: 0.4054 - auc: 0.6614 - val_loss: 0.4046 - val_auc: 0.6768\n",
      "\n",
      "Epoch 00766: val_loss improved from 0.40462 to 0.40458, saving model to DeepFM.h5\n",
      "Epoch 767/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3382 - auc: 0.702 - ETA: 0s - loss: 0.4136 - auc: 0.660 - 0s 21us/step - loss: 0.4064 - auc: 0.6585 - val_loss: 0.4045 - val_auc: 0.6777\n",
      "\n",
      "Epoch 00767: val_loss improved from 0.40458 to 0.40454, saving model to DeepFM.h5\n",
      "Epoch 768/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4202 - auc: 0.632 - ETA: 0s - loss: 0.4104 - auc: 0.648 - 0s 22us/step - loss: 0.4072 - auc: 0.6525 - val_loss: 0.4045 - val_auc: 0.6773\n",
      "\n",
      "Epoch 00768: val_loss improved from 0.40454 to 0.40450, saving model to DeepFM.h5\n",
      "Epoch 769/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4645 - auc: 0.601 - ETA: 0s - loss: 0.4089 - auc: 0.655 - 0s 21us/step - loss: 0.4062 - auc: 0.6534 - val_loss: 0.4045 - val_auc: 0.6773\n",
      "\n",
      "Epoch 00769: val_loss improved from 0.40450 to 0.40445, saving model to DeepFM.h5\n",
      "Epoch 770/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4136 - auc: 0.583 - ETA: 0s - loss: 0.4115 - auc: 0.658 - 0s 23us/step - loss: 0.4069 - auc: 0.6580 - val_loss: 0.4044 - val_auc: 0.6789\n",
      "\n",
      "Epoch 00770: val_loss improved from 0.40445 to 0.40442, saving model to DeepFM.h5\n",
      "Epoch 771/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.596 - ETA: 0s - loss: 0.4022 - auc: 0.657 - 0s 22us/step - loss: 0.4065 - auc: 0.6523 - val_loss: 0.4044 - val_auc: 0.6786\n",
      "\n",
      "Epoch 00771: val_loss improved from 0.40442 to 0.40438, saving model to DeepFM.h5\n",
      "Epoch 772/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3923 - auc: 0.641 - ETA: 0s - loss: 0.4035 - auc: 0.634 - 0s 22us/step - loss: 0.4076 - auc: 0.6489 - val_loss: 0.4044 - val_auc: 0.6775\n",
      "\n",
      "Epoch 00772: val_loss improved from 0.40438 to 0.40436, saving model to DeepFM.h5\n",
      "Epoch 773/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4328 - auc: 0.506 - ETA: 0s - loss: 0.4021 - auc: 0.649 - 0s 24us/step - loss: 0.4075 - auc: 0.6512 - val_loss: 0.4043 - val_auc: 0.6779\n",
      "\n",
      "Epoch 00773: val_loss improved from 0.40436 to 0.40434, saving model to DeepFM.h5\n",
      "Epoch 774/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4089 - auc: 0.632 - ETA: 0s - loss: 0.4166 - auc: 0.653 - 0s 22us/step - loss: 0.4049 - auc: 0.6596 - val_loss: 0.4043 - val_auc: 0.6783\n",
      "\n",
      "Epoch 00774: val_loss improved from 0.40434 to 0.40430, saving model to DeepFM.h5\n",
      "Epoch 775/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3808 - auc: 0.684 - ETA: 0s - loss: 0.4092 - auc: 0.643 - 0s 24us/step - loss: 0.4065 - auc: 0.6500 - val_loss: 0.4042 - val_auc: 0.6797\n",
      "\n",
      "Epoch 00775: val_loss improved from 0.40430 to 0.40425, saving model to DeepFM.h5\n",
      "Epoch 776/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4033 - auc: 0.754 - ETA: 0s - loss: 0.4144 - auc: 0.649 - 0s 22us/step - loss: 0.4056 - auc: 0.6611 - val_loss: 0.4042 - val_auc: 0.6787\n",
      "\n",
      "Epoch 00776: val_loss improved from 0.40425 to 0.40421, saving model to DeepFM.h5\n",
      "Epoch 777/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3871 - auc: 0.674 - ETA: 0s - loss: 0.4038 - auc: 0.645 - 0s 22us/step - loss: 0.4068 - auc: 0.6488 - val_loss: 0.4042 - val_auc: 0.6781\n",
      "\n",
      "Epoch 00777: val_loss improved from 0.40421 to 0.40417, saving model to DeepFM.h5\n",
      "Epoch 778/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4098 - auc: 0.742 - ETA: 0s - loss: 0.3971 - auc: 0.665 - 0s 22us/step - loss: 0.4037 - auc: 0.6706 - val_loss: 0.4041 - val_auc: 0.6779\n",
      "\n",
      "Epoch 00778: val_loss improved from 0.40417 to 0.40411, saving model to DeepFM.h5\n",
      "Epoch 779/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4051 - auc: 0.684 - ETA: 0s - loss: 0.4113 - auc: 0.655 - 0s 22us/step - loss: 0.4069 - auc: 0.6546 - val_loss: 0.4041 - val_auc: 0.6771\n",
      "\n",
      "Epoch 00779: val_loss improved from 0.40411 to 0.40407, saving model to DeepFM.h5\n",
      "Epoch 780/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3693 - auc: 0.650 - ETA: 0s - loss: 0.4148 - auc: 0.657 - 0s 22us/step - loss: 0.4066 - auc: 0.6517 - val_loss: 0.4040 - val_auc: 0.6784\n",
      "\n",
      "Epoch 00780: val_loss improved from 0.40407 to 0.40403, saving model to DeepFM.h5\n",
      "Epoch 781/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4735 - auc: 0.691 - ETA: 0s - loss: 0.4144 - auc: 0.649 - 0s 22us/step - loss: 0.4052 - auc: 0.6592 - val_loss: 0.4040 - val_auc: 0.6793\n",
      "\n",
      "Epoch 00781: val_loss improved from 0.40403 to 0.40401, saving model to DeepFM.h5\n",
      "Epoch 782/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4583 - auc: 0.620 - ETA: 0s - loss: 0.4022 - auc: 0.664 - 0s 22us/step - loss: 0.4032 - auc: 0.6745 - val_loss: 0.4040 - val_auc: 0.6792\n",
      "\n",
      "Epoch 00782: val_loss improved from 0.40401 to 0.40397, saving model to DeepFM.h5\n",
      "Epoch 783/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3900 - auc: 0.646 - ETA: 0s - loss: 0.4108 - auc: 0.659 - 0s 22us/step - loss: 0.4050 - auc: 0.6620 - val_loss: 0.4039 - val_auc: 0.6793\n",
      "\n",
      "Epoch 00783: val_loss improved from 0.40397 to 0.40395, saving model to DeepFM.h5\n",
      "Epoch 784/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4402 - auc: 0.576 - ETA: 0s - loss: 0.4131 - auc: 0.650 - 0s 22us/step - loss: 0.4065 - auc: 0.6557 - val_loss: 0.4039 - val_auc: 0.6797\n",
      "\n",
      "Epoch 00784: val_loss improved from 0.40395 to 0.40393, saving model to DeepFM.h5\n",
      "Epoch 785/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3976 - auc: 0.669 - ETA: 0s - loss: 0.4132 - auc: 0.665 - 0s 23us/step - loss: 0.4079 - auc: 0.6470 - val_loss: 0.4039 - val_auc: 0.6796\n",
      "\n",
      "Epoch 00785: val_loss improved from 0.40393 to 0.40391, saving model to DeepFM.h5\n",
      "Epoch 786/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4426 - auc: 0.606 - ETA: 0s - loss: 0.4133 - auc: 0.659 - 0s 21us/step - loss: 0.4064 - auc: 0.6604 - val_loss: 0.4039 - val_auc: 0.6798\n",
      "\n",
      "Epoch 00786: val_loss improved from 0.40391 to 0.40389, saving model to DeepFM.h5\n",
      "Epoch 787/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3963 - auc: 0.544 - ETA: 0s - loss: 0.4150 - auc: 0.652 - 0s 24us/step - loss: 0.4067 - auc: 0.6533 - val_loss: 0.4038 - val_auc: 0.6794\n",
      "\n",
      "Epoch 00787: val_loss improved from 0.40389 to 0.40385, saving model to DeepFM.h5\n",
      "Epoch 788/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3193 - auc: 0.724 - ETA: 0s - loss: 0.4011 - auc: 0.662 - 0s 21us/step - loss: 0.4032 - auc: 0.6764 - val_loss: 0.4038 - val_auc: 0.6801\n",
      "\n",
      "Epoch 00788: val_loss improved from 0.40385 to 0.40380, saving model to DeepFM.h5\n",
      "Epoch 789/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3891 - auc: 0.648 - ETA: 0s - loss: 0.3999 - auc: 0.656 - 0s 21us/step - loss: 0.4066 - auc: 0.6575 - val_loss: 0.4038 - val_auc: 0.6799\n",
      "\n",
      "Epoch 00789: val_loss improved from 0.40380 to 0.40376, saving model to DeepFM.h5\n",
      "Epoch 790/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4767 - auc: 0.619 - ETA: 0s - loss: 0.3962 - auc: 0.654 - 0s 22us/step - loss: 0.4058 - auc: 0.6593 - val_loss: 0.4037 - val_auc: 0.6800\n",
      "\n",
      "Epoch 00790: val_loss improved from 0.40376 to 0.40374, saving model to DeepFM.h5\n",
      "Epoch 791/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4228 - auc: 0.637 - ETA: 0s - loss: 0.4133 - auc: 0.641 - 0s 25us/step - loss: 0.4074 - auc: 0.6479 - val_loss: 0.4037 - val_auc: 0.6805\n",
      "\n",
      "Epoch 00791: val_loss improved from 0.40374 to 0.40372, saving model to DeepFM.h5\n",
      "Epoch 792/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4302 - auc: 0.654 - ETA: 0s - loss: 0.4071 - auc: 0.668 - 0s 22us/step - loss: 0.4044 - auc: 0.6647 - val_loss: 0.4037 - val_auc: 0.6789\n",
      "\n",
      "Epoch 00792: val_loss improved from 0.40372 to 0.40369, saving model to DeepFM.h5\n",
      "Epoch 793/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4037 - auc: 0.577 - ETA: 0s - loss: 0.4091 - auc: 0.648 - 0s 22us/step - loss: 0.4063 - auc: 0.6553 - val_loss: 0.4037 - val_auc: 0.6803\n",
      "\n",
      "Epoch 00793: val_loss improved from 0.40369 to 0.40366, saving model to DeepFM.h5\n",
      "Epoch 794/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4800 - auc: 0.684 - ETA: 0s - loss: 0.3974 - auc: 0.651 - 0s 21us/step - loss: 0.4057 - auc: 0.6553 - val_loss: 0.4036 - val_auc: 0.6806\n",
      "\n",
      "Epoch 00794: val_loss improved from 0.40366 to 0.40360, saving model to DeepFM.h5\n",
      "Epoch 795/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3643 - auc: 0.691 - ETA: 0s - loss: 0.3978 - auc: 0.654 - 0s 22us/step - loss: 0.4018 - auc: 0.6761 - val_loss: 0.4035 - val_auc: 0.6803\n",
      "\n",
      "Epoch 00795: val_loss improved from 0.40360 to 0.40354, saving model to DeepFM.h5\n",
      "Epoch 796/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4132 - auc: 0.714 - ETA: 0s - loss: 0.4072 - auc: 0.666 - 0s 21us/step - loss: 0.4050 - auc: 0.6618 - val_loss: 0.4035 - val_auc: 0.6808\n",
      "\n",
      "Epoch 00796: val_loss improved from 0.40354 to 0.40351, saving model to DeepFM.h5\n",
      "Epoch 797/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3541 - auc: 0.659 - ETA: 0s - loss: 0.4100 - auc: 0.668 - 0s 21us/step - loss: 0.4031 - auc: 0.6778 - val_loss: 0.4035 - val_auc: 0.6809\n",
      "\n",
      "Epoch 00797: val_loss improved from 0.40351 to 0.40345, saving model to DeepFM.h5\n",
      "Epoch 798/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4008 - auc: 0.661 - ETA: 0s - loss: 0.4145 - auc: 0.661 - 0s 22us/step - loss: 0.4055 - auc: 0.6672 - val_loss: 0.4034 - val_auc: 0.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00798: val_loss improved from 0.40345 to 0.40343, saving model to DeepFM.h5\n",
      "Epoch 799/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4087 - auc: 0.668 - ETA: 0s - loss: 0.4089 - auc: 0.636 - 0s 23us/step - loss: 0.4087 - auc: 0.6370 - val_loss: 0.4034 - val_auc: 0.6807\n",
      "\n",
      "Epoch 00799: val_loss improved from 0.40343 to 0.40339, saving model to DeepFM.h5\n",
      "Epoch 800/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4320 - auc: 0.591 - ETA: 0s - loss: 0.4089 - auc: 0.659 - 0s 21us/step - loss: 0.4032 - auc: 0.6664 - val_loss: 0.4033 - val_auc: 0.6821\n",
      "\n",
      "Epoch 00800: val_loss improved from 0.40339 to 0.40332, saving model to DeepFM.h5\n",
      "Epoch 801/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4482 - auc: 0.673 - ETA: 0s - loss: 0.3987 - auc: 0.653 - 0s 22us/step - loss: 0.4033 - auc: 0.6696 - val_loss: 0.4033 - val_auc: 0.6823\n",
      "\n",
      "Epoch 00801: val_loss improved from 0.40332 to 0.40329, saving model to DeepFM.h5\n",
      "Epoch 802/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3811 - auc: 0.560 - ETA: 0s - loss: 0.3981 - auc: 0.674 - 0s 22us/step - loss: 0.4040 - auc: 0.6685 - val_loss: 0.4032 - val_auc: 0.6820\n",
      "\n",
      "Epoch 00802: val_loss improved from 0.40329 to 0.40325, saving model to DeepFM.h5\n",
      "Epoch 803/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3268 - auc: 0.788 - ETA: 0s - loss: 0.3977 - auc: 0.658 - 0s 23us/step - loss: 0.4055 - auc: 0.6531 - val_loss: 0.4032 - val_auc: 0.6812\n",
      "\n",
      "Epoch 00803: val_loss improved from 0.40325 to 0.40320, saving model to DeepFM.h5\n",
      "Epoch 804/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4108 - auc: 0.686 - ETA: 0s - loss: 0.4048 - auc: 0.670 - 0s 22us/step - loss: 0.4044 - auc: 0.6662 - val_loss: 0.4032 - val_auc: 0.6829\n",
      "\n",
      "Epoch 00804: val_loss improved from 0.40320 to 0.40317, saving model to DeepFM.h5\n",
      "Epoch 805/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3965 - auc: 0.741 - ETA: 0s - loss: 0.4127 - auc: 0.649 - 0s 23us/step - loss: 0.4068 - auc: 0.6526 - val_loss: 0.4032 - val_auc: 0.6823\n",
      "\n",
      "Epoch 00805: val_loss improved from 0.40317 to 0.40315, saving model to DeepFM.h5\n",
      "Epoch 806/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4201 - auc: 0.590 - ETA: 0s - loss: 0.4158 - auc: 0.655 - 0s 23us/step - loss: 0.4068 - auc: 0.6573 - val_loss: 0.4031 - val_auc: 0.6829\n",
      "\n",
      "Epoch 00806: val_loss improved from 0.40315 to 0.40311, saving model to DeepFM.h5\n",
      "Epoch 807/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4291 - auc: 0.692 - ETA: 0s - loss: 0.4084 - auc: 0.665 - 0s 23us/step - loss: 0.4053 - auc: 0.6688 - val_loss: 0.4031 - val_auc: 0.6820\n",
      "\n",
      "Epoch 00807: val_loss improved from 0.40311 to 0.40308, saving model to DeepFM.h5\n",
      "Epoch 808/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3674 - auc: 0.703 - ETA: 0s - loss: 0.4041 - auc: 0.652 - 0s 23us/step - loss: 0.4046 - auc: 0.6617 - val_loss: 0.4031 - val_auc: 0.6812\n",
      "\n",
      "Epoch 00808: val_loss improved from 0.40308 to 0.40306, saving model to DeepFM.h5\n",
      "Epoch 809/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3939 - auc: 0.645 - ETA: 0s - loss: 0.4026 - auc: 0.653 - 0s 24us/step - loss: 0.4073 - auc: 0.6474 - val_loss: 0.4030 - val_auc: 0.6825\n",
      "\n",
      "Epoch 00809: val_loss improved from 0.40306 to 0.40299, saving model to DeepFM.h5\n",
      "Epoch 810/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3914 - auc: 0.596 - ETA: 0s - loss: 0.4033 - auc: 0.667 - 0s 22us/step - loss: 0.4050 - auc: 0.6620 - val_loss: 0.4030 - val_auc: 0.6825\n",
      "\n",
      "Epoch 00810: val_loss improved from 0.40299 to 0.40296, saving model to DeepFM.h5\n",
      "Epoch 811/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3650 - auc: 0.735 - ETA: 0s - loss: 0.4010 - auc: 0.673 - 0s 23us/step - loss: 0.4025 - auc: 0.6715 - val_loss: 0.4029 - val_auc: 0.6814\n",
      "\n",
      "Epoch 00811: val_loss improved from 0.40296 to 0.40292, saving model to DeepFM.h5\n",
      "Epoch 812/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4256 - auc: 0.625 - ETA: 0s - loss: 0.4061 - auc: 0.677 - 0s 23us/step - loss: 0.4035 - auc: 0.6694 - val_loss: 0.4029 - val_auc: 0.6813\n",
      "\n",
      "Epoch 00812: val_loss improved from 0.40292 to 0.40288, saving model to DeepFM.h5\n",
      "Epoch 813/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4228 - auc: 0.682 - ETA: 0s - loss: 0.4008 - auc: 0.659 - 0s 22us/step - loss: 0.4053 - auc: 0.6567 - val_loss: 0.4028 - val_auc: 0.6825\n",
      "\n",
      "Epoch 00813: val_loss improved from 0.40288 to 0.40282, saving model to DeepFM.h5\n",
      "Epoch 814/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4020 - auc: 0.673 - ETA: 0s - loss: 0.3961 - auc: 0.661 - ETA: 0s - loss: 0.3980 - auc: 0.675 - 0s 30us/step - loss: 0.4028 - auc: 0.6781 - val_loss: 0.4028 - val_auc: 0.6811\n",
      "\n",
      "Epoch 00814: val_loss improved from 0.40282 to 0.40280, saving model to DeepFM.h5\n",
      "Epoch 815/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3868 - auc: 0.719 - ETA: 0s - loss: 0.3997 - auc: 0.661 - 0s 21us/step - loss: 0.4042 - auc: 0.6621 - val_loss: 0.4028 - val_auc: 0.6813\n",
      "\n",
      "Epoch 00815: val_loss improved from 0.40280 to 0.40278, saving model to DeepFM.h5\n",
      "Epoch 816/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4358 - auc: 0.635 - ETA: 0s - loss: 0.3908 - auc: 0.667 - 0s 21us/step - loss: 0.4039 - auc: 0.6617 - val_loss: 0.4028 - val_auc: 0.6824\n",
      "\n",
      "Epoch 00816: val_loss improved from 0.40278 to 0.40275, saving model to DeepFM.h5\n",
      "Epoch 817/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3955 - auc: 0.623 - ETA: 0s - loss: 0.4045 - auc: 0.671 - 0s 23us/step - loss: 0.4040 - auc: 0.6722 - val_loss: 0.4027 - val_auc: 0.6820\n",
      "\n",
      "Epoch 00817: val_loss improved from 0.40275 to 0.40271, saving model to DeepFM.h5\n",
      "Epoch 818/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4554 - auc: 0.629 - ETA: 0s - loss: 0.4102 - auc: 0.654 - 0s 22us/step - loss: 0.4056 - auc: 0.6595 - val_loss: 0.4026 - val_auc: 0.6837\n",
      "\n",
      "Epoch 00818: val_loss improved from 0.40271 to 0.40263, saving model to DeepFM.h5\n",
      "Epoch 819/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4353 - auc: 0.665 - ETA: 0s - loss: 0.4185 - auc: 0.647 - 0s 22us/step - loss: 0.4060 - auc: 0.6551 - val_loss: 0.4026 - val_auc: 0.6818\n",
      "\n",
      "Epoch 00819: val_loss improved from 0.40263 to 0.40262, saving model to DeepFM.h5\n",
      "Epoch 820/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.629 - ETA: 0s - loss: 0.4015 - auc: 0.653 - 0s 22us/step - loss: 0.4069 - auc: 0.6510 - val_loss: 0.4026 - val_auc: 0.6830\n",
      "\n",
      "Epoch 00820: val_loss improved from 0.40262 to 0.40261, saving model to DeepFM.h5\n",
      "Epoch 821/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3366 - auc: 0.737 - ETA: 0s - loss: 0.3945 - auc: 0.686 - 0s 21us/step - loss: 0.4013 - auc: 0.6738 - val_loss: 0.4025 - val_auc: 0.6833\n",
      "\n",
      "Epoch 00821: val_loss improved from 0.40261 to 0.40254, saving model to DeepFM.h5\n",
      "Epoch 822/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.692 - ETA: 0s - loss: 0.3993 - auc: 0.675 - 0s 23us/step - loss: 0.4028 - auc: 0.6755 - val_loss: 0.4025 - val_auc: 0.6834\n",
      "\n",
      "Epoch 00822: val_loss improved from 0.40254 to 0.40252, saving model to DeepFM.h5\n",
      "Epoch 823/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3755 - auc: 0.722 - ETA: 0s - loss: 0.4011 - auc: 0.691 - 0s 23us/step - loss: 0.4015 - auc: 0.6800 - val_loss: 0.4025 - val_auc: 0.6845\n",
      "\n",
      "Epoch 00823: val_loss improved from 0.40252 to 0.40247, saving model to DeepFM.h5\n",
      "Epoch 824/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4928 - auc: 0.585 - ETA: 0s - loss: 0.3972 - auc: 0.668 - 0s 22us/step - loss: 0.4037 - auc: 0.6682 - val_loss: 0.4025 - val_auc: 0.6841\n",
      "\n",
      "Epoch 00824: val_loss improved from 0.40247 to 0.40245, saving model to DeepFM.h5\n",
      "Epoch 825/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3804 - auc: 0.653 - ETA: 0s - loss: 0.4033 - auc: 0.657 - 0s 23us/step - loss: 0.4039 - auc: 0.6609 - val_loss: 0.4024 - val_auc: 0.6847\n",
      "\n",
      "Epoch 00825: val_loss improved from 0.40245 to 0.40240, saving model to DeepFM.h5\n",
      "Epoch 826/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4100 - auc: 0.758 - ETA: 0s - loss: 0.3989 - auc: 0.689 - 0s 22us/step - loss: 0.4020 - auc: 0.6756 - val_loss: 0.4024 - val_auc: 0.6840\n",
      "\n",
      "Epoch 00826: val_loss improved from 0.40240 to 0.40240, saving model to DeepFM.h5\n",
      "Epoch 827/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4365 - auc: 0.704 - ETA: 0s - loss: 0.4010 - auc: 0.681 - 0s 23us/step - loss: 0.4029 - auc: 0.6735 - val_loss: 0.4024 - val_auc: 0.6824\n",
      "\n",
      "Epoch 00827: val_loss improved from 0.40240 to 0.40239, saving model to DeepFM.h5\n",
      "Epoch 828/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3768 - auc: 0.683 - ETA: 0s - loss: 0.3878 - auc: 0.679 - 0s 21us/step - loss: 0.4035 - auc: 0.6698 - val_loss: 0.4023 - val_auc: 0.6839\n",
      "\n",
      "Epoch 00828: val_loss improved from 0.40239 to 0.40229, saving model to DeepFM.h5\n",
      "Epoch 829/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3924 - auc: 0.713 - ETA: 0s - loss: 0.4022 - auc: 0.657 - 0s 21us/step - loss: 0.4063 - auc: 0.6530 - val_loss: 0.4022 - val_auc: 0.6830\n",
      "\n",
      "Epoch 00829: val_loss improved from 0.40229 to 0.40224, saving model to DeepFM.h5\n",
      "Epoch 830/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4070 - auc: 0.631 - ETA: 0s - loss: 0.4041 - auc: 0.665 - 0s 21us/step - loss: 0.4050 - auc: 0.6594 - val_loss: 0.4022 - val_auc: 0.6838\n",
      "\n",
      "Epoch 00830: val_loss improved from 0.40224 to 0.40222, saving model to DeepFM.h5\n",
      "Epoch 831/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4323 - auc: 0.670 - ETA: 0s - loss: 0.4117 - auc: 0.680 - 0s 23us/step - loss: 0.4043 - auc: 0.6704 - val_loss: 0.4022 - val_auc: 0.6847\n",
      "\n",
      "Epoch 00831: val_loss improved from 0.40222 to 0.40219, saving model to DeepFM.h5\n",
      "Epoch 832/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4002 - auc: 0.578 - ETA: 0s - loss: 0.4106 - auc: 0.659 - 0s 22us/step - loss: 0.4038 - auc: 0.6732 - val_loss: 0.4021 - val_auc: 0.6839\n",
      "\n",
      "Epoch 00832: val_loss improved from 0.40219 to 0.40215, saving model to DeepFM.h5\n",
      "Epoch 833/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4107 - auc: 0.679 - ETA: 0s - loss: 0.4122 - auc: 0.663 - 0s 21us/step - loss: 0.4040 - auc: 0.6672 - val_loss: 0.4021 - val_auc: 0.6843\n",
      "\n",
      "Epoch 00833: val_loss improved from 0.40215 to 0.40211, saving model to DeepFM.h5\n",
      "Epoch 834/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4443 - auc: 0.663 - ETA: 0s - loss: 0.4113 - auc: 0.667 - 0s 22us/step - loss: 0.4011 - auc: 0.6752 - val_loss: 0.4021 - val_auc: 0.6842\n",
      "\n",
      "Epoch 00834: val_loss improved from 0.40211 to 0.40207, saving model to DeepFM.h5\n",
      "Epoch 835/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4067 - auc: 0.650 - ETA: 0s - loss: 0.4068 - auc: 0.666 - 0s 23us/step - loss: 0.4036 - auc: 0.6660 - val_loss: 0.4020 - val_auc: 0.6847\n",
      "\n",
      "Epoch 00835: val_loss improved from 0.40207 to 0.40203, saving model to DeepFM.h5\n",
      "Epoch 836/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3408 - auc: 0.715 - ETA: 0s - loss: 0.3972 - auc: 0.681 - 0s 22us/step - loss: 0.4004 - auc: 0.6794 - val_loss: 0.4019 - val_auc: 0.6837\n",
      "\n",
      "Epoch 00836: val_loss improved from 0.40203 to 0.40194, saving model to DeepFM.h5\n",
      "Epoch 837/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4124 - auc: 0.724 - ETA: 0s - loss: 0.3948 - auc: 0.676 - 0s 22us/step - loss: 0.4009 - auc: 0.6774 - val_loss: 0.4019 - val_auc: 0.6835\n",
      "\n",
      "Epoch 00837: val_loss improved from 0.40194 to 0.40189, saving model to DeepFM.h5\n",
      "Epoch 838/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4167 - auc: 0.707 - ETA: 0s - loss: 0.4066 - auc: 0.681 - 0s 25us/step - loss: 0.4018 - auc: 0.6793 - val_loss: 0.4019 - val_auc: 0.6838\n",
      "\n",
      "Epoch 00838: val_loss improved from 0.40189 to 0.40185, saving model to DeepFM.h5\n",
      "Epoch 839/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.627 - ETA: 0s - loss: 0.4052 - auc: 0.670 - 0s 22us/step - loss: 0.4037 - auc: 0.6669 - val_loss: 0.4018 - val_auc: 0.6843\n",
      "\n",
      "Epoch 00839: val_loss improved from 0.40185 to 0.40181, saving model to DeepFM.h5\n",
      "Epoch 840/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4623 - auc: 0.710 - ETA: 0s - loss: 0.3998 - auc: 0.668 - 0s 22us/step - loss: 0.4035 - auc: 0.6623 - val_loss: 0.4018 - val_auc: 0.6832\n",
      "\n",
      "Epoch 00840: val_loss improved from 0.40181 to 0.40177, saving model to DeepFM.h5\n",
      "Epoch 841/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4139 - auc: 0.625 - ETA: 0s - loss: 0.4002 - auc: 0.664 - 0s 22us/step - loss: 0.4032 - auc: 0.6668 - val_loss: 0.4017 - val_auc: 0.6836\n",
      "\n",
      "Epoch 00841: val_loss improved from 0.40177 to 0.40175, saving model to DeepFM.h5\n",
      "Epoch 842/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4530 - auc: 0.689 - ETA: 0s - loss: 0.3992 - auc: 0.665 - 0s 22us/step - loss: 0.4045 - auc: 0.6616 - val_loss: 0.4017 - val_auc: 0.6839\n",
      "\n",
      "Epoch 00842: val_loss improved from 0.40175 to 0.40171, saving model to DeepFM.h5\n",
      "Epoch 843/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4443 - auc: 0.643 - ETA: 0s - loss: 0.4074 - auc: 0.669 - 0s 21us/step - loss: 0.4036 - auc: 0.6662 - val_loss: 0.4017 - val_auc: 0.6834\n",
      "\n",
      "Epoch 00843: val_loss improved from 0.40171 to 0.40168, saving model to DeepFM.h5\n",
      "Epoch 844/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4040 - auc: 0.674 - ETA: 0s - loss: 0.3869 - auc: 0.673 - 0s 21us/step - loss: 0.3988 - auc: 0.6866 - val_loss: 0.4016 - val_auc: 0.6824\n",
      "\n",
      "Epoch 00844: val_loss improved from 0.40168 to 0.40161, saving model to DeepFM.h5\n",
      "Epoch 845/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4519 - auc: 0.679 - ETA: 0s - loss: 0.4135 - auc: 0.670 - 0s 21us/step - loss: 0.4020 - auc: 0.6745 - val_loss: 0.4016 - val_auc: 0.6843\n",
      "\n",
      "Epoch 00845: val_loss improved from 0.40161 to 0.40157, saving model to DeepFM.h5\n",
      "Epoch 846/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4148 - auc: 0.682 - ETA: 0s - loss: 0.3933 - auc: 0.690 - 0s 24us/step - loss: 0.4000 - auc: 0.6823 - val_loss: 0.4015 - val_auc: 0.6842\n",
      "\n",
      "Epoch 00846: val_loss improved from 0.40157 to 0.40151, saving model to DeepFM.h5\n",
      "Epoch 847/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4684 - auc: 0.658 - ETA: 0s - loss: 0.3937 - auc: 0.667 - 0s 23us/step - loss: 0.3998 - auc: 0.6856 - val_loss: 0.4015 - val_auc: 0.6837\n",
      "\n",
      "Epoch 00847: val_loss improved from 0.40151 to 0.40148, saving model to DeepFM.h5\n",
      "Epoch 848/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4431 - auc: 0.665 - ETA: 0s - loss: 0.4048 - auc: 0.661 - 0s 22us/step - loss: 0.4015 - auc: 0.6746 - val_loss: 0.4014 - val_auc: 0.6846\n",
      "\n",
      "Epoch 00848: val_loss improved from 0.40148 to 0.40144, saving model to DeepFM.h5\n",
      "Epoch 849/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4340 - auc: 0.586 - ETA: 0s - loss: 0.4146 - auc: 0.668 - 0s 22us/step - loss: 0.4020 - auc: 0.6771 - val_loss: 0.4014 - val_auc: 0.6838\n",
      "\n",
      "Epoch 00849: val_loss improved from 0.40144 to 0.40138, saving model to DeepFM.h5\n",
      "Epoch 850/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4579 - auc: 0.649 - ETA: 0s - loss: 0.4054 - auc: 0.664 - 0s 22us/step - loss: 0.4013 - auc: 0.6744 - val_loss: 0.4014 - val_auc: 0.6849\n",
      "\n",
      "Epoch 00850: val_loss improved from 0.40138 to 0.40135, saving model to DeepFM.h5\n",
      "Epoch 851/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3356 - auc: 0.694 - ETA: 0s - loss: 0.4070 - auc: 0.678 - 0s 22us/step - loss: 0.4022 - auc: 0.6728 - val_loss: 0.4013 - val_auc: 0.6844\n",
      "\n",
      "Epoch 00851: val_loss improved from 0.40135 to 0.40135, saving model to DeepFM.h5\n",
      "Epoch 852/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3944 - auc: 0.632 - ETA: 0s - loss: 0.4016 - auc: 0.676 - 0s 23us/step - loss: 0.4026 - auc: 0.6743 - val_loss: 0.4013 - val_auc: 0.6851\n",
      "\n",
      "Epoch 00852: val_loss improved from 0.40135 to 0.40133, saving model to DeepFM.h5\n",
      "Epoch 853/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3886 - auc: 0.667 - ETA: 0s - loss: 0.4067 - auc: 0.668 - 0s 21us/step - loss: 0.4032 - auc: 0.6660 - val_loss: 0.4013 - val_auc: 0.6864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00853: val_loss improved from 0.40133 to 0.40128, saving model to DeepFM.h5\n",
      "Epoch 854/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5146 - auc: 0.611 - ETA: 0s - loss: 0.4055 - auc: 0.656 - 0s 22us/step - loss: 0.4033 - auc: 0.6670 - val_loss: 0.4013 - val_auc: 0.6859\n",
      "\n",
      "Epoch 00854: val_loss improved from 0.40128 to 0.40125, saving model to DeepFM.h5\n",
      "Epoch 855/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3527 - auc: 0.659 - ETA: 0s - loss: 0.4015 - auc: 0.655 - 0s 22us/step - loss: 0.4021 - auc: 0.6676 - val_loss: 0.4012 - val_auc: 0.6852\n",
      "\n",
      "Epoch 00855: val_loss improved from 0.40125 to 0.40120, saving model to DeepFM.h5\n",
      "Epoch 856/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4705 - auc: 0.657 - ETA: 0s - loss: 0.4027 - auc: 0.668 - 0s 23us/step - loss: 0.4046 - auc: 0.6637 - val_loss: 0.4012 - val_auc: 0.6849\n",
      "\n",
      "Epoch 00856: val_loss improved from 0.40120 to 0.40116, saving model to DeepFM.h5\n",
      "Epoch 857/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3703 - auc: 0.680 - ETA: 0s - loss: 0.3997 - auc: 0.687 - 0s 22us/step - loss: 0.4002 - auc: 0.6796 - val_loss: 0.4011 - val_auc: 0.6856\n",
      "\n",
      "Epoch 00857: val_loss improved from 0.40116 to 0.40110, saving model to DeepFM.h5\n",
      "Epoch 858/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4046 - auc: 0.663 - ETA: 0s - loss: 0.4027 - auc: 0.682 - 0s 22us/step - loss: 0.3994 - auc: 0.6907 - val_loss: 0.4010 - val_auc: 0.6863\n",
      "\n",
      "Epoch 00858: val_loss improved from 0.40110 to 0.40104, saving model to DeepFM.h5\n",
      "Epoch 859/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3794 - auc: 0.669 - ETA: 0s - loss: 0.4046 - auc: 0.671 - 0s 22us/step - loss: 0.4020 - auc: 0.6727 - val_loss: 0.4010 - val_auc: 0.6862\n",
      "\n",
      "Epoch 00859: val_loss improved from 0.40104 to 0.40099, saving model to DeepFM.h5\n",
      "Epoch 860/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3809 - auc: 0.662 - ETA: 0s - loss: 0.4016 - auc: 0.673 - 0s 22us/step - loss: 0.4016 - auc: 0.6746 - val_loss: 0.4010 - val_auc: 0.6857\n",
      "\n",
      "Epoch 00860: val_loss improved from 0.40099 to 0.40098, saving model to DeepFM.h5\n",
      "Epoch 861/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4367 - auc: 0.639 - ETA: 0s - loss: 0.4183 - auc: 0.670 - 0s 23us/step - loss: 0.4010 - auc: 0.6820 - val_loss: 0.4009 - val_auc: 0.6860\n",
      "\n",
      "Epoch 00861: val_loss improved from 0.40098 to 0.40093, saving model to DeepFM.h5\n",
      "Epoch 862/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4412 - auc: 0.636 - ETA: 0s - loss: 0.3933 - auc: 0.659 - 0s 21us/step - loss: 0.4025 - auc: 0.6733 - val_loss: 0.4009 - val_auc: 0.6865\n",
      "\n",
      "Epoch 00862: val_loss improved from 0.40093 to 0.40092, saving model to DeepFM.h5\n",
      "Epoch 863/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4630 - auc: 0.662 - ETA: 0s - loss: 0.4045 - auc: 0.670 - 0s 23us/step - loss: 0.4034 - auc: 0.6679 - val_loss: 0.4009 - val_auc: 0.6870\n",
      "\n",
      "Epoch 00863: val_loss improved from 0.40092 to 0.40086, saving model to DeepFM.h5\n",
      "Epoch 864/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4020 - auc: 0.671 - ETA: 0s - loss: 0.3969 - auc: 0.667 - 0s 23us/step - loss: 0.4023 - auc: 0.6739 - val_loss: 0.4008 - val_auc: 0.6871\n",
      "\n",
      "Epoch 00864: val_loss improved from 0.40086 to 0.40081, saving model to DeepFM.h5\n",
      "Epoch 865/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3929 - auc: 0.704 - ETA: 0s - loss: 0.4040 - auc: 0.680 - 0s 22us/step - loss: 0.4007 - auc: 0.6868 - val_loss: 0.4008 - val_auc: 0.6871\n",
      "\n",
      "Epoch 00865: val_loss improved from 0.40081 to 0.40078, saving model to DeepFM.h5\n",
      "Epoch 866/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4500 - auc: 0.662 - ETA: 0s - loss: 0.4002 - auc: 0.687 - 0s 22us/step - loss: 0.3992 - auc: 0.6866 - val_loss: 0.4007 - val_auc: 0.6876\n",
      "\n",
      "Epoch 00866: val_loss improved from 0.40078 to 0.40074, saving model to DeepFM.h5\n",
      "Epoch 867/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4382 - auc: 0.675 - ETA: 0s - loss: 0.3981 - auc: 0.685 - 0s 22us/step - loss: 0.4012 - auc: 0.6767 - val_loss: 0.4007 - val_auc: 0.6877\n",
      "\n",
      "Epoch 00867: val_loss improved from 0.40074 to 0.40071, saving model to DeepFM.h5\n",
      "Epoch 868/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4023 - auc: 0.723 - ETA: 0s - loss: 0.4046 - auc: 0.679 - 0s 21us/step - loss: 0.4010 - auc: 0.6762 - val_loss: 0.4007 - val_auc: 0.6873\n",
      "\n",
      "Epoch 00868: val_loss improved from 0.40071 to 0.40066, saving model to DeepFM.h5\n",
      "Epoch 869/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3544 - auc: 0.696 - ETA: 0s - loss: 0.4070 - auc: 0.679 - 0s 21us/step - loss: 0.4009 - auc: 0.6750 - val_loss: 0.4006 - val_auc: 0.6879\n",
      "\n",
      "Epoch 00869: val_loss improved from 0.40066 to 0.40061, saving model to DeepFM.h5\n",
      "Epoch 870/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3759 - auc: 0.712 - ETA: 0s - loss: 0.3989 - auc: 0.668 - 0s 23us/step - loss: 0.4001 - auc: 0.6787 - val_loss: 0.4006 - val_auc: 0.6882\n",
      "\n",
      "Epoch 00870: val_loss improved from 0.40061 to 0.40058, saving model to DeepFM.h5\n",
      "Epoch 871/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4087 - auc: 0.736 - ETA: 0s - loss: 0.4093 - auc: 0.671 - 0s 23us/step - loss: 0.4008 - auc: 0.6747 - val_loss: 0.4005 - val_auc: 0.6880\n",
      "\n",
      "Epoch 00871: val_loss improved from 0.40058 to 0.40053, saving model to DeepFM.h5\n",
      "Epoch 872/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4481 - auc: 0.656 - ETA: 0s - loss: 0.4095 - auc: 0.655 - 0s 22us/step - loss: 0.4028 - auc: 0.6650 - val_loss: 0.4005 - val_auc: 0.6874\n",
      "\n",
      "Epoch 00872: val_loss improved from 0.40053 to 0.40050, saving model to DeepFM.h5\n",
      "Epoch 873/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.574 - ETA: 0s - loss: 0.4129 - auc: 0.654 - 0s 22us/step - loss: 0.4014 - auc: 0.6716 - val_loss: 0.4005 - val_auc: 0.6866\n",
      "\n",
      "Epoch 00873: val_loss improved from 0.40050 to 0.40049, saving model to DeepFM.h5\n",
      "Epoch 874/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3873 - auc: 0.718 - ETA: 0s - loss: 0.4043 - auc: 0.687 - 0s 21us/step - loss: 0.3973 - auc: 0.6959 - val_loss: 0.4005 - val_auc: 0.6870\n",
      "\n",
      "Epoch 00874: val_loss improved from 0.40049 to 0.40047, saving model to DeepFM.h5\n",
      "Epoch 875/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4147 - auc: 0.659 - ETA: 0s - loss: 0.4057 - auc: 0.664 - 0s 20us/step - loss: 0.4029 - auc: 0.6684 - val_loss: 0.4004 - val_auc: 0.6878\n",
      "\n",
      "Epoch 00875: val_loss improved from 0.40047 to 0.40041, saving model to DeepFM.h5\n",
      "Epoch 876/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3998 - auc: 0.717 - ETA: 0s - loss: 0.3946 - auc: 0.689 - 0s 22us/step - loss: 0.3997 - auc: 0.6860 - val_loss: 0.4004 - val_auc: 0.6872\n",
      "\n",
      "Epoch 00876: val_loss improved from 0.40041 to 0.40039, saving model to DeepFM.h5\n",
      "Epoch 877/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3907 - auc: 0.749 - ETA: 0s - loss: 0.3943 - auc: 0.680 - 0s 22us/step - loss: 0.4021 - auc: 0.6730 - val_loss: 0.4003 - val_auc: 0.6871\n",
      "\n",
      "Epoch 00877: val_loss improved from 0.40039 to 0.40032, saving model to DeepFM.h5\n",
      "Epoch 878/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4372 - auc: 0.711 - ETA: 0s - loss: 0.4056 - auc: 0.668 - 0s 22us/step - loss: 0.4024 - auc: 0.6625 - val_loss: 0.4003 - val_auc: 0.6874\n",
      "\n",
      "Epoch 00878: val_loss improved from 0.40032 to 0.40026, saving model to DeepFM.h5\n",
      "Epoch 879/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3884 - auc: 0.691 - ETA: 0s - loss: 0.4124 - auc: 0.670 - 0s 23us/step - loss: 0.4029 - auc: 0.6730 - val_loss: 0.4002 - val_auc: 0.6896\n",
      "\n",
      "Epoch 00879: val_loss improved from 0.40026 to 0.40020, saving model to DeepFM.h5\n",
      "Epoch 880/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3839 - auc: 0.690 - ETA: 0s - loss: 0.4023 - auc: 0.675 - 0s 22us/step - loss: 0.4012 - auc: 0.6763 - val_loss: 0.4002 - val_auc: 0.6896\n",
      "\n",
      "Epoch 00880: val_loss improved from 0.40020 to 0.40017, saving model to DeepFM.h5\n",
      "Epoch 881/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4143 - auc: 0.672 - ETA: 0s - loss: 0.3942 - auc: 0.684 - 0s 22us/step - loss: 0.4004 - auc: 0.6760 - val_loss: 0.4001 - val_auc: 0.6902\n",
      "\n",
      "Epoch 00881: val_loss improved from 0.40017 to 0.40011, saving model to DeepFM.h5\n",
      "Epoch 882/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3688 - auc: 0.703 - ETA: 0s - loss: 0.4013 - auc: 0.676 - 0s 22us/step - loss: 0.3985 - auc: 0.6825 - val_loss: 0.4001 - val_auc: 0.6898\n",
      "\n",
      "Epoch 00882: val_loss improved from 0.40011 to 0.40008, saving model to DeepFM.h5\n",
      "Epoch 883/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3641 - auc: 0.672 - ETA: 0s - loss: 0.3987 - auc: 0.683 - 0s 22us/step - loss: 0.4003 - auc: 0.6796 - val_loss: 0.4000 - val_auc: 0.6897\n",
      "\n",
      "Epoch 00883: val_loss improved from 0.40008 to 0.40002, saving model to DeepFM.h5\n",
      "Epoch 884/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3699 - auc: 0.745 - ETA: 0s - loss: 0.3956 - auc: 0.679 - 0s 23us/step - loss: 0.4004 - auc: 0.6751 - val_loss: 0.4000 - val_auc: 0.6905\n",
      "\n",
      "Epoch 00884: val_loss improved from 0.40002 to 0.39996, saving model to DeepFM.h5\n",
      "Epoch 885/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4221 - auc: 0.655 - ETA: 0s - loss: 0.4010 - auc: 0.673 - 0s 22us/step - loss: 0.4018 - auc: 0.6674 - val_loss: 0.3999 - val_auc: 0.6902\n",
      "\n",
      "Epoch 00885: val_loss improved from 0.39996 to 0.39991, saving model to DeepFM.h5\n",
      "Epoch 886/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4485 - auc: 0.676 - ETA: 0s - loss: 0.4110 - auc: 0.678 - 0s 22us/step - loss: 0.3998 - auc: 0.6831 - val_loss: 0.3999 - val_auc: 0.6909\n",
      "\n",
      "Epoch 00886: val_loss improved from 0.39991 to 0.39988, saving model to DeepFM.h5\n",
      "Epoch 887/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4187 - auc: 0.735 - ETA: 0s - loss: 0.4086 - auc: 0.679 - 0s 22us/step - loss: 0.4007 - auc: 0.6791 - val_loss: 0.3998 - val_auc: 0.6904\n",
      "\n",
      "Epoch 00887: val_loss improved from 0.39988 to 0.39985, saving model to DeepFM.h5\n",
      "Epoch 888/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4124 - auc: 0.594 - ETA: 0s - loss: 0.4154 - auc: 0.684 - 0s 21us/step - loss: 0.4008 - auc: 0.6788 - val_loss: 0.3998 - val_auc: 0.6898\n",
      "\n",
      "Epoch 00888: val_loss improved from 0.39985 to 0.39982, saving model to DeepFM.h5\n",
      "Epoch 889/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3800 - auc: 0.695 - ETA: 0s - loss: 0.4073 - auc: 0.685 - 0s 22us/step - loss: 0.3991 - auc: 0.6823 - val_loss: 0.3998 - val_auc: 0.6901\n",
      "\n",
      "Epoch 00889: val_loss improved from 0.39982 to 0.39979, saving model to DeepFM.h5\n",
      "Epoch 890/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4149 - auc: 0.713 - ETA: 0s - loss: 0.4021 - auc: 0.677 - 0s 22us/step - loss: 0.3995 - auc: 0.6811 - val_loss: 0.3997 - val_auc: 0.6922\n",
      "\n",
      "Epoch 00890: val_loss improved from 0.39979 to 0.39973, saving model to DeepFM.h5\n",
      "Epoch 891/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4424 - auc: 0.661 - ETA: 0s - loss: 0.4010 - auc: 0.691 - 0s 22us/step - loss: 0.3998 - auc: 0.6813 - val_loss: 0.3997 - val_auc: 0.6901\n",
      "\n",
      "Epoch 00891: val_loss improved from 0.39973 to 0.39971, saving model to DeepFM.h5\n",
      "Epoch 892/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3781 - auc: 0.607 - ETA: 0s - loss: 0.4059 - auc: 0.695 - 0s 22us/step - loss: 0.3993 - auc: 0.6877 - val_loss: 0.3997 - val_auc: 0.6913\n",
      "\n",
      "Epoch 00892: val_loss improved from 0.39971 to 0.39967, saving model to DeepFM.h5\n",
      "Epoch 893/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4413 - auc: 0.659 - ETA: 0s - loss: 0.3921 - auc: 0.685 - 0s 23us/step - loss: 0.3989 - auc: 0.6875 - val_loss: 0.3996 - val_auc: 0.6912\n",
      "\n",
      "Epoch 00893: val_loss improved from 0.39967 to 0.39960, saving model to DeepFM.h5\n",
      "Epoch 894/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4440 - auc: 0.658 - ETA: 0s - loss: 0.4041 - auc: 0.683 - 0s 21us/step - loss: 0.4002 - auc: 0.6817 - val_loss: 0.3996 - val_auc: 0.6917\n",
      "\n",
      "Epoch 00894: val_loss improved from 0.39960 to 0.39957, saving model to DeepFM.h5\n",
      "Epoch 895/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3785 - auc: 0.741 - ETA: 0s - loss: 0.3978 - auc: 0.671 - 0s 22us/step - loss: 0.4017 - auc: 0.6744 - val_loss: 0.3995 - val_auc: 0.6911\n",
      "\n",
      "Epoch 00895: val_loss improved from 0.39957 to 0.39953, saving model to DeepFM.h5\n",
      "Epoch 896/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3657 - auc: 0.708 - ETA: 0s - loss: 0.4034 - auc: 0.685 - 0s 23us/step - loss: 0.3983 - auc: 0.6957 - val_loss: 0.3995 - val_auc: 0.6909\n",
      "\n",
      "Epoch 00896: val_loss improved from 0.39953 to 0.39947, saving model to DeepFM.h5\n",
      "Epoch 897/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4217 - auc: 0.562 - ETA: 0s - loss: 0.3952 - auc: 0.671 - 0s 20us/step - loss: 0.3992 - auc: 0.6820 - val_loss: 0.3995 - val_auc: 0.6914\n",
      "\n",
      "Epoch 00897: val_loss improved from 0.39947 to 0.39945, saving model to DeepFM.h5\n",
      "Epoch 898/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.679 - ETA: 0s - loss: 0.4066 - auc: 0.682 - 0s 22us/step - loss: 0.3990 - auc: 0.6832 - val_loss: 0.3994 - val_auc: 0.6894\n",
      "\n",
      "Epoch 00898: val_loss improved from 0.39945 to 0.39939, saving model to DeepFM.h5\n",
      "Epoch 899/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3603 - auc: 0.668 - ETA: 0s - loss: 0.3985 - auc: 0.679 - 0s 21us/step - loss: 0.4022 - auc: 0.6667 - val_loss: 0.3994 - val_auc: 0.6895\n",
      "\n",
      "Epoch 00899: val_loss improved from 0.39939 to 0.39937, saving model to DeepFM.h5\n",
      "Epoch 900/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4445 - auc: 0.704 - ETA: 0s - loss: 0.3999 - auc: 0.672 - 0s 22us/step - loss: 0.4009 - auc: 0.6754 - val_loss: 0.3994 - val_auc: 0.6923\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.39937\n",
      "Epoch 901/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4612 - auc: 0.612 - ETA: 0s - loss: 0.3920 - auc: 0.678 - 0s 22us/step - loss: 0.3979 - auc: 0.6872 - val_loss: 0.3993 - val_auc: 0.6917\n",
      "\n",
      "Epoch 00901: val_loss improved from 0.39937 to 0.39934, saving model to DeepFM.h5\n",
      "Epoch 902/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4245 - auc: 0.628 - ETA: 0s - loss: 0.3931 - auc: 0.667 - 0s 22us/step - loss: 0.4012 - auc: 0.6716 - val_loss: 0.3993 - val_auc: 0.6918\n",
      "\n",
      "Epoch 00902: val_loss improved from 0.39934 to 0.39926, saving model to DeepFM.h5\n",
      "Epoch 903/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3571 - auc: 0.746 - ETA: 0s - loss: 0.3901 - auc: 0.687 - 0s 21us/step - loss: 0.3999 - auc: 0.6808 - val_loss: 0.3992 - val_auc: 0.6912\n",
      "\n",
      "Epoch 00903: val_loss improved from 0.39926 to 0.39919, saving model to DeepFM.h5\n",
      "Epoch 904/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3918 - auc: 0.752 - ETA: 0s - loss: 0.3958 - auc: 0.694 - 0s 22us/step - loss: 0.3970 - auc: 0.6904 - val_loss: 0.3991 - val_auc: 0.6905\n",
      "\n",
      "Epoch 00904: val_loss improved from 0.39919 to 0.39914, saving model to DeepFM.h5\n",
      "Epoch 905/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4208 - auc: 0.723 - ETA: 0s - loss: 0.3960 - auc: 0.698 - 0s 21us/step - loss: 0.3977 - auc: 0.6930 - val_loss: 0.3991 - val_auc: 0.6913\n",
      "\n",
      "Epoch 00905: val_loss improved from 0.39914 to 0.39910, saving model to DeepFM.h5\n",
      "Epoch 906/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3225 - auc: 0.718 - ETA: 0s - loss: 0.3925 - auc: 0.689 - 0s 22us/step - loss: 0.4007 - auc: 0.6826 - val_loss: 0.3991 - val_auc: 0.6907\n",
      "\n",
      "Epoch 00906: val_loss improved from 0.39910 to 0.39907, saving model to DeepFM.h5\n",
      "Epoch 907/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4091 - auc: 0.679 - ETA: 0s - loss: 0.3971 - auc: 0.677 - 0s 21us/step - loss: 0.3986 - auc: 0.6892 - val_loss: 0.3990 - val_auc: 0.6906\n",
      "\n",
      "Epoch 00907: val_loss improved from 0.39907 to 0.39905, saving model to DeepFM.h5\n",
      "Epoch 908/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4136 - auc: 0.657 - ETA: 0s - loss: 0.3982 - auc: 0.686 - 0s 23us/step - loss: 0.3990 - auc: 0.6878 - val_loss: 0.3990 - val_auc: 0.6906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00908: val_loss improved from 0.39905 to 0.39901, saving model to DeepFM.h5\n",
      "Epoch 909/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3612 - auc: 0.694 - ETA: 0s - loss: 0.3919 - auc: 0.715 - 0s 20us/step - loss: 0.3964 - auc: 0.6899 - val_loss: 0.3989 - val_auc: 0.6928\n",
      "\n",
      "Epoch 00909: val_loss improved from 0.39901 to 0.39893, saving model to DeepFM.h5\n",
      "Epoch 910/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3788 - auc: 0.712 - ETA: 0s - loss: 0.4080 - auc: 0.673 - 0s 22us/step - loss: 0.3993 - auc: 0.6873 - val_loss: 0.3989 - val_auc: 0.6926\n",
      "\n",
      "Epoch 00910: val_loss improved from 0.39893 to 0.39887, saving model to DeepFM.h5\n",
      "Epoch 911/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4106 - auc: 0.631 - ETA: 0s - loss: 0.3950 - auc: 0.668 - 0s 25us/step - loss: 0.3995 - auc: 0.6771 - val_loss: 0.3988 - val_auc: 0.6927\n",
      "\n",
      "Epoch 00911: val_loss improved from 0.39887 to 0.39882, saving model to DeepFM.h5\n",
      "Epoch 912/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3592 - auc: 0.721 - ETA: 0s - loss: 0.3874 - auc: 0.700 - 0s 22us/step - loss: 0.3974 - auc: 0.6945 - val_loss: 0.3988 - val_auc: 0.6922\n",
      "\n",
      "Epoch 00912: val_loss improved from 0.39882 to 0.39876, saving model to DeepFM.h5\n",
      "Epoch 913/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4770 - auc: 0.664 - ETA: 0s - loss: 0.4006 - auc: 0.685 - 0s 22us/step - loss: 0.3990 - auc: 0.6816 - val_loss: 0.3987 - val_auc: 0.6928\n",
      "\n",
      "Epoch 00913: val_loss improved from 0.39876 to 0.39874, saving model to DeepFM.h5\n",
      "Epoch 914/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3486 - auc: 0.689 - ETA: 0s - loss: 0.4110 - auc: 0.678 - 0s 23us/step - loss: 0.3981 - auc: 0.6936 - val_loss: 0.3987 - val_auc: 0.6924\n",
      "\n",
      "Epoch 00914: val_loss improved from 0.39874 to 0.39870, saving model to DeepFM.h5\n",
      "Epoch 915/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3946 - auc: 0.673 - ETA: 0s - loss: 0.4030 - auc: 0.669 - 0s 22us/step - loss: 0.3992 - auc: 0.6834 - val_loss: 0.3987 - val_auc: 0.6919\n",
      "\n",
      "Epoch 00915: val_loss improved from 0.39870 to 0.39869, saving model to DeepFM.h5\n",
      "Epoch 916/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3839 - auc: 0.758 - ETA: 0s - loss: 0.3944 - auc: 0.709 - 0s 22us/step - loss: 0.3999 - auc: 0.6779 - val_loss: 0.3986 - val_auc: 0.6929\n",
      "\n",
      "Epoch 00916: val_loss improved from 0.39869 to 0.39863, saving model to DeepFM.h5\n",
      "Epoch 917/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3486 - auc: 0.650 - ETA: 0s - loss: 0.3942 - auc: 0.690 - 0s 25us/step - loss: 0.3995 - auc: 0.6819 - val_loss: 0.3986 - val_auc: 0.6918\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.39863\n",
      "Epoch 918/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3982 - auc: 0.594 - ETA: 0s - loss: 0.3979 - auc: 0.666 - 0s 22us/step - loss: 0.3974 - auc: 0.6778 - val_loss: 0.3986 - val_auc: 0.6914\n",
      "\n",
      "Epoch 00918: val_loss improved from 0.39863 to 0.39858, saving model to DeepFM.h5\n",
      "Epoch 919/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4414 - auc: 0.662 - ETA: 0s - loss: 0.4005 - auc: 0.693 - 0s 21us/step - loss: 0.3971 - auc: 0.6900 - val_loss: 0.3985 - val_auc: 0.6915\n",
      "\n",
      "Epoch 00919: val_loss improved from 0.39858 to 0.39851, saving model to DeepFM.h5\n",
      "Epoch 920/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3873 - auc: 0.678 - ETA: 0s - loss: 0.3969 - auc: 0.703 - 0s 25us/step - loss: 0.3963 - auc: 0.6985 - val_loss: 0.3985 - val_auc: 0.6915\n",
      "\n",
      "Epoch 00920: val_loss improved from 0.39851 to 0.39846, saving model to DeepFM.h5\n",
      "Epoch 921/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3669 - auc: 0.702 - ETA: 0s - loss: 0.4003 - auc: 0.675 - 0s 22us/step - loss: 0.3981 - auc: 0.6839 - val_loss: 0.3984 - val_auc: 0.6916\n",
      "\n",
      "Epoch 00921: val_loss improved from 0.39846 to 0.39841, saving model to DeepFM.h5\n",
      "Epoch 922/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4288 - auc: 0.578 - ETA: 0s - loss: 0.3940 - auc: 0.669 - 0s 20us/step - loss: 0.3998 - auc: 0.6809 - val_loss: 0.3984 - val_auc: 0.6920\n",
      "\n",
      "Epoch 00922: val_loss improved from 0.39841 to 0.39836, saving model to DeepFM.h5\n",
      "Epoch 923/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4130 - auc: 0.665 - ETA: 0s - loss: 0.4017 - auc: 0.687 - 0s 22us/step - loss: 0.3978 - auc: 0.6889 - val_loss: 0.3983 - val_auc: 0.6911\n",
      "\n",
      "Epoch 00923: val_loss improved from 0.39836 to 0.39828, saving model to DeepFM.h5\n",
      "Epoch 924/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3820 - auc: 0.671 - ETA: 0s - loss: 0.4027 - auc: 0.667 - 0s 22us/step - loss: 0.3983 - auc: 0.6873 - val_loss: 0.3982 - val_auc: 0.6916\n",
      "\n",
      "Epoch 00924: val_loss improved from 0.39828 to 0.39824, saving model to DeepFM.h5\n",
      "Epoch 925/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3709 - auc: 0.700 - ETA: 0s - loss: 0.3821 - auc: 0.699 - 0s 21us/step - loss: 0.3975 - auc: 0.6878 - val_loss: 0.3982 - val_auc: 0.6926\n",
      "\n",
      "Epoch 00925: val_loss improved from 0.39824 to 0.39817, saving model to DeepFM.h5\n",
      "Epoch 926/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4155 - auc: 0.708 - ETA: 0s - loss: 0.4104 - auc: 0.681 - 0s 22us/step - loss: 0.3998 - auc: 0.6815 - val_loss: 0.3981 - val_auc: 0.6930\n",
      "\n",
      "Epoch 00926: val_loss improved from 0.39817 to 0.39811, saving model to DeepFM.h5\n",
      "Epoch 927/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3144 - auc: 0.680 - ETA: 0s - loss: 0.3931 - auc: 0.690 - 0s 21us/step - loss: 0.3985 - auc: 0.6814 - val_loss: 0.3980 - val_auc: 0.6932\n",
      "\n",
      "Epoch 00927: val_loss improved from 0.39811 to 0.39804, saving model to DeepFM.h5\n",
      "Epoch 928/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4314 - auc: 0.574 - ETA: 0s - loss: 0.4072 - auc: 0.664 - 0s 22us/step - loss: 0.4000 - auc: 0.6766 - val_loss: 0.3980 - val_auc: 0.6933\n",
      "\n",
      "Epoch 00928: val_loss improved from 0.39804 to 0.39801, saving model to DeepFM.h5\n",
      "Epoch 929/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4643 - auc: 0.646 - ETA: 0s - loss: 0.4043 - auc: 0.679 - 0s 20us/step - loss: 0.3979 - auc: 0.6883 - val_loss: 0.3980 - val_auc: 0.6936\n",
      "\n",
      "Epoch 00929: val_loss improved from 0.39801 to 0.39796, saving model to DeepFM.h5\n",
      "Epoch 930/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4015 - auc: 0.597 - ETA: 0s - loss: 0.3994 - auc: 0.662 - 0s 23us/step - loss: 0.3988 - auc: 0.6823 - val_loss: 0.3979 - val_auc: 0.6928\n",
      "\n",
      "Epoch 00930: val_loss improved from 0.39796 to 0.39792, saving model to DeepFM.h5\n",
      "Epoch 931/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4005 - auc: 0.673 - ETA: 0s - loss: 0.4010 - auc: 0.696 - 0s 22us/step - loss: 0.3981 - auc: 0.6910 - val_loss: 0.3979 - val_auc: 0.6934\n",
      "\n",
      "Epoch 00931: val_loss improved from 0.39792 to 0.39787, saving model to DeepFM.h5\n",
      "Epoch 932/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4329 - auc: 0.647 - ETA: 0s - loss: 0.4079 - auc: 0.690 - 0s 21us/step - loss: 0.3988 - auc: 0.6920 - val_loss: 0.3978 - val_auc: 0.6924\n",
      "\n",
      "Epoch 00932: val_loss improved from 0.39787 to 0.39784, saving model to DeepFM.h5\n",
      "Epoch 933/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4398 - auc: 0.739 - ETA: 0s - loss: 0.3988 - auc: 0.713 - 0s 21us/step - loss: 0.3933 - auc: 0.7098 - val_loss: 0.3978 - val_auc: 0.6928\n",
      "\n",
      "Epoch 00933: val_loss improved from 0.39784 to 0.39778, saving model to DeepFM.h5\n",
      "Epoch 934/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3092 - auc: 0.693 - ETA: 0s - loss: 0.3980 - auc: 0.688 - 0s 23us/step - loss: 0.3969 - auc: 0.6911 - val_loss: 0.3977 - val_auc: 0.6939\n",
      "\n",
      "Epoch 00934: val_loss improved from 0.39778 to 0.39774, saving model to DeepFM.h5\n",
      "Epoch 935/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3838 - auc: 0.678 - ETA: 0s - loss: 0.3975 - auc: 0.689 - 0s 21us/step - loss: 0.3970 - auc: 0.6906 - val_loss: 0.3977 - val_auc: 0.6940\n",
      "\n",
      "Epoch 00935: val_loss improved from 0.39774 to 0.39769, saving model to DeepFM.h5\n",
      "Epoch 936/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4601 - auc: 0.612 - ETA: 0s - loss: 0.3978 - auc: 0.691 - 0s 23us/step - loss: 0.3975 - auc: 0.6887 - val_loss: 0.3976 - val_auc: 0.6953\n",
      "\n",
      "Epoch 00936: val_loss improved from 0.39769 to 0.39765, saving model to DeepFM.h5\n",
      "Epoch 937/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4017 - auc: 0.626 - ETA: 0s - loss: 0.3874 - auc: 0.695 - 0s 22us/step - loss: 0.3947 - auc: 0.7010 - val_loss: 0.3976 - val_auc: 0.6961\n",
      "\n",
      "Epoch 00937: val_loss improved from 0.39765 to 0.39758, saving model to DeepFM.h5\n",
      "Epoch 938/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3793 - auc: 0.733 - ETA: 0s - loss: 0.3894 - auc: 0.694 - 0s 22us/step - loss: 0.3948 - auc: 0.6995 - val_loss: 0.3975 - val_auc: 0.6945\n",
      "\n",
      "Epoch 00938: val_loss improved from 0.39758 to 0.39750, saving model to DeepFM.h5\n",
      "Epoch 939/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4163 - auc: 0.734 - ETA: 0s - loss: 0.3942 - auc: 0.706 - 0s 22us/step - loss: 0.3961 - auc: 0.7003 - val_loss: 0.3975 - val_auc: 0.6943\n",
      "\n",
      "Epoch 00939: val_loss improved from 0.39750 to 0.39745, saving model to DeepFM.h5\n",
      "Epoch 940/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3549 - auc: 0.711 - ETA: 0s - loss: 0.3944 - auc: 0.676 - 0s 21us/step - loss: 0.3999 - auc: 0.6793 - val_loss: 0.3974 - val_auc: 0.6949\n",
      "\n",
      "Epoch 00940: val_loss improved from 0.39745 to 0.39740, saving model to DeepFM.h5\n",
      "Epoch 941/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4275 - auc: 0.724 - ETA: 0s - loss: 0.3984 - auc: 0.699 - 0s 21us/step - loss: 0.3936 - auc: 0.7122 - val_loss: 0.3974 - val_auc: 0.6958\n",
      "\n",
      "Epoch 00941: val_loss improved from 0.39740 to 0.39739, saving model to DeepFM.h5\n",
      "Epoch 942/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3865 - auc: 0.688 - ETA: 0s - loss: 0.3963 - auc: 0.695 - 0s 21us/step - loss: 0.3957 - auc: 0.6983 - val_loss: 0.3973 - val_auc: 0.6942\n",
      "\n",
      "Epoch 00942: val_loss improved from 0.39739 to 0.39731, saving model to DeepFM.h5\n",
      "Epoch 943/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3543 - auc: 0.693 - ETA: 0s - loss: 0.4070 - auc: 0.693 - 0s 22us/step - loss: 0.3951 - auc: 0.7036 - val_loss: 0.3973 - val_auc: 0.6961\n",
      "\n",
      "Epoch 00943: val_loss improved from 0.39731 to 0.39730, saving model to DeepFM.h5\n",
      "Epoch 944/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3690 - auc: 0.672 - ETA: 0s - loss: 0.4024 - auc: 0.681 - 0s 21us/step - loss: 0.3979 - auc: 0.6838 - val_loss: 0.3972 - val_auc: 0.6941\n",
      "\n",
      "Epoch 00944: val_loss improved from 0.39730 to 0.39721, saving model to DeepFM.h5\n",
      "Epoch 945/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4203 - auc: 0.638 - ETA: 0s - loss: 0.3951 - auc: 0.675 - 0s 21us/step - loss: 0.3970 - auc: 0.6854 - val_loss: 0.3972 - val_auc: 0.6947\n",
      "\n",
      "Epoch 00945: val_loss improved from 0.39721 to 0.39717, saving model to DeepFM.h5\n",
      "Epoch 946/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3976 - auc: 0.639 - ETA: 0s - loss: 0.3937 - auc: 0.696 - 0s 21us/step - loss: 0.3949 - auc: 0.7000 - val_loss: 0.3972 - val_auc: 0.6966\n",
      "\n",
      "Epoch 00946: val_loss improved from 0.39717 to 0.39716, saving model to DeepFM.h5\n",
      "Epoch 947/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3724 - auc: 0.622 - ETA: 0s - loss: 0.4057 - auc: 0.695 - 0s 20us/step - loss: 0.3954 - auc: 0.6978 - val_loss: 0.3971 - val_auc: 0.6969\n",
      "\n",
      "Epoch 00947: val_loss improved from 0.39716 to 0.39708, saving model to DeepFM.h5\n",
      "Epoch 948/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2980 - auc: 0.669 - ETA: 0s - loss: 0.3991 - auc: 0.688 - 0s 22us/step - loss: 0.3981 - auc: 0.6877 - val_loss: 0.3970 - val_auc: 0.6958\n",
      "\n",
      "Epoch 00948: val_loss improved from 0.39708 to 0.39701, saving model to DeepFM.h5\n",
      "Epoch 949/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3112 - auc: 0.716 - ETA: 0s - loss: 0.3971 - auc: 0.695 - 0s 23us/step - loss: 0.3951 - auc: 0.6983 - val_loss: 0.3970 - val_auc: 0.6970\n",
      "\n",
      "Epoch 00949: val_loss improved from 0.39701 to 0.39697, saving model to DeepFM.h5\n",
      "Epoch 950/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4044 - auc: 0.683 - ETA: 0s - loss: 0.3993 - auc: 0.696 - 0s 21us/step - loss: 0.3952 - auc: 0.6981 - val_loss: 0.3969 - val_auc: 0.6975\n",
      "\n",
      "Epoch 00950: val_loss improved from 0.39697 to 0.39692, saving model to DeepFM.h5\n",
      "Epoch 951/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3766 - auc: 0.767 - ETA: 0s - loss: 0.4040 - auc: 0.704 - 0s 22us/step - loss: 0.3967 - auc: 0.7015 - val_loss: 0.3969 - val_auc: 0.6978\n",
      "\n",
      "Epoch 00951: val_loss improved from 0.39692 to 0.39685, saving model to DeepFM.h5\n",
      "Epoch 952/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4200 - auc: 0.687 - ETA: 0s - loss: 0.3913 - auc: 0.701 - 0s 21us/step - loss: 0.3952 - auc: 0.6967 - val_loss: 0.3968 - val_auc: 0.6985\n",
      "\n",
      "Epoch 00952: val_loss improved from 0.39685 to 0.39678, saving model to DeepFM.h5\n",
      "Epoch 953/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3785 - auc: 0.799 - ETA: 0s - loss: 0.3984 - auc: 0.699 - 0s 23us/step - loss: 0.3941 - auc: 0.7036 - val_loss: 0.3967 - val_auc: 0.6978\n",
      "\n",
      "Epoch 00953: val_loss improved from 0.39678 to 0.39672, saving model to DeepFM.h5\n",
      "Epoch 954/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4257 - auc: 0.659 - ETA: 0s - loss: 0.3885 - auc: 0.685 - 0s 23us/step - loss: 0.3991 - auc: 0.6859 - val_loss: 0.3967 - val_auc: 0.6983\n",
      "\n",
      "Epoch 00954: val_loss improved from 0.39672 to 0.39669, saving model to DeepFM.h5\n",
      "Epoch 955/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4076 - auc: 0.668 - ETA: 0s - loss: 0.3959 - auc: 0.700 - 0s 23us/step - loss: 0.3955 - auc: 0.6950 - val_loss: 0.3967 - val_auc: 0.6990\n",
      "\n",
      "Epoch 00955: val_loss improved from 0.39669 to 0.39666, saving model to DeepFM.h5\n",
      "Epoch 956/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4022 - auc: 0.689 - ETA: 0s - loss: 0.3945 - auc: 0.669 - 0s 22us/step - loss: 0.3984 - auc: 0.6844 - val_loss: 0.3966 - val_auc: 0.6991\n",
      "\n",
      "Epoch 00956: val_loss improved from 0.39666 to 0.39660, saving model to DeepFM.h5\n",
      "Epoch 957/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3663 - auc: 0.677 - ETA: 0s - loss: 0.3950 - auc: 0.697 - 0s 21us/step - loss: 0.3954 - auc: 0.6947 - val_loss: 0.3965 - val_auc: 0.6985\n",
      "\n",
      "Epoch 00957: val_loss improved from 0.39660 to 0.39652, saving model to DeepFM.h5\n",
      "Epoch 958/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4009 - auc: 0.705 - ETA: 0s - loss: 0.3940 - auc: 0.698 - 0s 22us/step - loss: 0.3958 - auc: 0.6981 - val_loss: 0.3965 - val_auc: 0.6990\n",
      "\n",
      "Epoch 00958: val_loss improved from 0.39652 to 0.39650, saving model to DeepFM.h5\n",
      "Epoch 959/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4137 - auc: 0.738 - ETA: 0s - loss: 0.3809 - auc: 0.709 - 0s 23us/step - loss: 0.3960 - auc: 0.6929 - val_loss: 0.3964 - val_auc: 0.6993\n",
      "\n",
      "Epoch 00959: val_loss improved from 0.39650 to 0.39644, saving model to DeepFM.h5\n",
      "Epoch 960/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4256 - auc: 0.711 - ETA: 0s - loss: 0.4115 - auc: 0.708 - 0s 22us/step - loss: 0.3950 - auc: 0.6987 - val_loss: 0.3964 - val_auc: 0.6990\n",
      "\n",
      "Epoch 00960: val_loss improved from 0.39644 to 0.39640, saving model to DeepFM.h5\n",
      "Epoch 961/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3611 - auc: 0.713 - ETA: 0s - loss: 0.3892 - auc: 0.693 - 0s 22us/step - loss: 0.3948 - auc: 0.7014 - val_loss: 0.3963 - val_auc: 0.6995\n",
      "\n",
      "Epoch 00961: val_loss improved from 0.39640 to 0.39632, saving model to DeepFM.h5\n",
      "Epoch 962/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4092 - auc: 0.692 - ETA: 0s - loss: 0.3936 - auc: 0.699 - 0s 21us/step - loss: 0.3966 - auc: 0.6967 - val_loss: 0.3963 - val_auc: 0.7001\n",
      "\n",
      "Epoch 00962: val_loss improved from 0.39632 to 0.39626, saving model to DeepFM.h5\n",
      "Epoch 963/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4220 - auc: 0.711 - ETA: 0s - loss: 0.3855 - auc: 0.701 - 0s 23us/step - loss: 0.3944 - auc: 0.6978 - val_loss: 0.3962 - val_auc: 0.7001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00963: val_loss improved from 0.39626 to 0.39618, saving model to DeepFM.h5\n",
      "Epoch 964/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4719 - auc: 0.634 - ETA: 0s - loss: 0.3946 - auc: 0.698 - 0s 20us/step - loss: 0.3935 - auc: 0.7006 - val_loss: 0.3962 - val_auc: 0.7000\n",
      "\n",
      "Epoch 00964: val_loss improved from 0.39618 to 0.39616, saving model to DeepFM.h5\n",
      "Epoch 965/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4293 - auc: 0.723 - ETA: 0s - loss: 0.4098 - auc: 0.695 - 0s 21us/step - loss: 0.3950 - auc: 0.6991 - val_loss: 0.3961 - val_auc: 0.7007\n",
      "\n",
      "Epoch 00965: val_loss improved from 0.39616 to 0.39611, saving model to DeepFM.h5\n",
      "Epoch 966/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3696 - auc: 0.683 - ETA: 0s - loss: 0.3972 - auc: 0.692 - 0s 22us/step - loss: 0.3948 - auc: 0.6970 - val_loss: 0.3960 - val_auc: 0.7008\n",
      "\n",
      "Epoch 00966: val_loss improved from 0.39611 to 0.39603, saving model to DeepFM.h5\n",
      "Epoch 967/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4192 - auc: 0.617 - ETA: 0s - loss: 0.3993 - auc: 0.694 - 0s 22us/step - loss: 0.3942 - auc: 0.7014 - val_loss: 0.3960 - val_auc: 0.7008\n",
      "\n",
      "Epoch 00967: val_loss improved from 0.39603 to 0.39601, saving model to DeepFM.h5\n",
      "Epoch 968/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4336 - auc: 0.704 - ETA: 0s - loss: 0.3948 - auc: 0.692 - 0s 22us/step - loss: 0.3940 - auc: 0.7020 - val_loss: 0.3960 - val_auc: 0.7007\n",
      "\n",
      "Epoch 00968: val_loss improved from 0.39601 to 0.39596, saving model to DeepFM.h5\n",
      "Epoch 969/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4040 - auc: 0.755 - ETA: 0s - loss: 0.4074 - auc: 0.699 - 0s 22us/step - loss: 0.3948 - auc: 0.6953 - val_loss: 0.3959 - val_auc: 0.7002\n",
      "\n",
      "Epoch 00969: val_loss improved from 0.39596 to 0.39592, saving model to DeepFM.h5\n",
      "Epoch 970/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3991 - auc: 0.748 - ETA: 0s - loss: 0.3963 - auc: 0.696 - 0s 22us/step - loss: 0.3941 - auc: 0.7012 - val_loss: 0.3959 - val_auc: 0.7005\n",
      "\n",
      "Epoch 00970: val_loss improved from 0.39592 to 0.39589, saving model to DeepFM.h5\n",
      "Epoch 971/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3841 - auc: 0.712 - ETA: 0s - loss: 0.4049 - auc: 0.679 - 0s 21us/step - loss: 0.3964 - auc: 0.6854 - val_loss: 0.3959 - val_auc: 0.7009\n",
      "\n",
      "Epoch 00971: val_loss improved from 0.39589 to 0.39588, saving model to DeepFM.h5\n",
      "Epoch 972/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4285 - auc: 0.662 - ETA: 0s - loss: 0.3892 - auc: 0.698 - 0s 22us/step - loss: 0.3948 - auc: 0.6993 - val_loss: 0.3959 - val_auc: 0.7004\n",
      "\n",
      "Epoch 00972: val_loss improved from 0.39588 to 0.39586, saving model to DeepFM.h5\n",
      "Epoch 973/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3804 - auc: 0.685 - ETA: 0s - loss: 0.3911 - auc: 0.702 - 0s 22us/step - loss: 0.3946 - auc: 0.7061 - val_loss: 0.3958 - val_auc: 0.7015\n",
      "\n",
      "Epoch 00973: val_loss improved from 0.39586 to 0.39581, saving model to DeepFM.h5\n",
      "Epoch 974/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4121 - auc: 0.694 - ETA: 0s - loss: 0.3997 - auc: 0.695 - 0s 21us/step - loss: 0.3946 - auc: 0.7025 - val_loss: 0.3957 - val_auc: 0.7010\n",
      "\n",
      "Epoch 00974: val_loss improved from 0.39581 to 0.39574, saving model to DeepFM.h5\n",
      "Epoch 975/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3718 - auc: 0.680 - ETA: 0s - loss: 0.3952 - auc: 0.698 - 0s 22us/step - loss: 0.3952 - auc: 0.6956 - val_loss: 0.3957 - val_auc: 0.7010\n",
      "\n",
      "Epoch 00975: val_loss improved from 0.39574 to 0.39568, saving model to DeepFM.h5\n",
      "Epoch 976/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3551 - auc: 0.674 - ETA: 0s - loss: 0.3886 - auc: 0.697 - 0s 21us/step - loss: 0.3940 - auc: 0.7020 - val_loss: 0.3956 - val_auc: 0.7016\n",
      "\n",
      "Epoch 00976: val_loss improved from 0.39568 to 0.39558, saving model to DeepFM.h5\n",
      "Epoch 977/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4255 - auc: 0.724 - ETA: 0s - loss: 0.3923 - auc: 0.703 - 0s 22us/step - loss: 0.3956 - auc: 0.6925 - val_loss: 0.3955 - val_auc: 0.7033\n",
      "\n",
      "Epoch 00977: val_loss improved from 0.39558 to 0.39550, saving model to DeepFM.h5\n",
      "Epoch 978/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3718 - auc: 0.654 - ETA: 0s - loss: 0.3977 - auc: 0.692 - 0s 23us/step - loss: 0.3941 - auc: 0.6974 - val_loss: 0.3955 - val_auc: 0.7008\n",
      "\n",
      "Epoch 00978: val_loss improved from 0.39550 to 0.39546, saving model to DeepFM.h5\n",
      "Epoch 979/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3675 - auc: 0.709 - ETA: 0s - loss: 0.3980 - auc: 0.680 - 0s 22us/step - loss: 0.3974 - auc: 0.6892 - val_loss: 0.3954 - val_auc: 0.7013\n",
      "\n",
      "Epoch 00979: val_loss improved from 0.39546 to 0.39540, saving model to DeepFM.h5\n",
      "Epoch 980/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3793 - auc: 0.699 - ETA: 0s - loss: 0.3910 - auc: 0.713 - 0s 22us/step - loss: 0.3942 - auc: 0.7061 - val_loss: 0.3953 - val_auc: 0.7016\n",
      "\n",
      "Epoch 00980: val_loss improved from 0.39540 to 0.39534, saving model to DeepFM.h5\n",
      "Epoch 981/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3220 - auc: 0.767 - ETA: 0s - loss: 0.3910 - auc: 0.711 - 0s 23us/step - loss: 0.3923 - auc: 0.7131 - val_loss: 0.3953 - val_auc: 0.7022\n",
      "\n",
      "Epoch 00981: val_loss improved from 0.39534 to 0.39527, saving model to DeepFM.h5\n",
      "Epoch 982/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3788 - auc: 0.659 - ETA: 0s - loss: 0.3805 - auc: 0.715 - 0s 22us/step - loss: 0.3927 - auc: 0.7085 - val_loss: 0.3952 - val_auc: 0.7025\n",
      "\n",
      "Epoch 00982: val_loss improved from 0.39527 to 0.39521, saving model to DeepFM.h5\n",
      "Epoch 983/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4089 - auc: 0.725 - ETA: 0s - loss: 0.3832 - auc: 0.712 - 0s 21us/step - loss: 0.3916 - auc: 0.7094 - val_loss: 0.3952 - val_auc: 0.7030\n",
      "\n",
      "Epoch 00983: val_loss improved from 0.39521 to 0.39517, saving model to DeepFM.h5\n",
      "Epoch 984/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3877 - auc: 0.697 - ETA: 0s - loss: 0.4004 - auc: 0.697 - 0s 23us/step - loss: 0.3952 - auc: 0.6955 - val_loss: 0.3951 - val_auc: 0.7041\n",
      "\n",
      "Epoch 00984: val_loss improved from 0.39517 to 0.39513, saving model to DeepFM.h5\n",
      "Epoch 985/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3780 - auc: 0.659 - ETA: 0s - loss: 0.3904 - auc: 0.695 - 0s 22us/step - loss: 0.3953 - auc: 0.6998 - val_loss: 0.3951 - val_auc: 0.7027\n",
      "\n",
      "Epoch 00985: val_loss improved from 0.39513 to 0.39508, saving model to DeepFM.h5\n",
      "Epoch 986/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3593 - auc: 0.745 - ETA: 0s - loss: 0.4005 - auc: 0.705 - 0s 23us/step - loss: 0.3936 - auc: 0.7033 - val_loss: 0.3950 - val_auc: 0.7038\n",
      "\n",
      "Epoch 00986: val_loss improved from 0.39508 to 0.39502, saving model to DeepFM.h5\n",
      "Epoch 987/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3775 - auc: 0.663 - ETA: 0s - loss: 0.3933 - auc: 0.707 - 0s 23us/step - loss: 0.3889 - auc: 0.7173 - val_loss: 0.3950 - val_auc: 0.7042\n",
      "\n",
      "Epoch 00987: val_loss improved from 0.39502 to 0.39499, saving model to DeepFM.h5\n",
      "Epoch 988/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3527 - auc: 0.708 - ETA: 0s - loss: 0.3929 - auc: 0.709 - 0s 22us/step - loss: 0.3918 - auc: 0.7008 - val_loss: 0.3949 - val_auc: 0.7039\n",
      "\n",
      "Epoch 00988: val_loss improved from 0.39499 to 0.39492, saving model to DeepFM.h5\n",
      "Epoch 989/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3446 - auc: 0.682 - ETA: 0s - loss: 0.3921 - auc: 0.687 - 0s 22us/step - loss: 0.3947 - auc: 0.6921 - val_loss: 0.3949 - val_auc: 0.7032\n",
      "\n",
      "Epoch 00989: val_loss improved from 0.39492 to 0.39485, saving model to DeepFM.h5\n",
      "Epoch 990/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3234 - auc: 0.727 - ETA: 0s - loss: 0.3875 - auc: 0.701 - 0s 26us/step - loss: 0.3948 - auc: 0.7004 - val_loss: 0.3948 - val_auc: 0.7027\n",
      "\n",
      "Epoch 00990: val_loss improved from 0.39485 to 0.39480, saving model to DeepFM.h5\n",
      "Epoch 991/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3873 - auc: 0.739 - ETA: 0s - loss: 0.3836 - auc: 0.708 - 0s 23us/step - loss: 0.3911 - auc: 0.7132 - val_loss: 0.3947 - val_auc: 0.7036\n",
      "\n",
      "Epoch 00991: val_loss improved from 0.39480 to 0.39475, saving model to DeepFM.h5\n",
      "Epoch 992/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3503 - auc: 0.702 - ETA: 0s - loss: 0.3880 - auc: 0.706 - 0s 23us/step - loss: 0.3903 - auc: 0.7170 - val_loss: 0.3947 - val_auc: 0.7045\n",
      "\n",
      "Epoch 00992: val_loss improved from 0.39475 to 0.39471, saving model to DeepFM.h5\n",
      "Epoch 993/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3609 - auc: 0.688 - ETA: 0s - loss: 0.3921 - auc: 0.718 - 0s 22us/step - loss: 0.3943 - auc: 0.6983 - val_loss: 0.3946 - val_auc: 0.7043\n",
      "\n",
      "Epoch 00993: val_loss improved from 0.39471 to 0.39462, saving model to DeepFM.h5\n",
      "Epoch 994/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3602 - auc: 0.616 - ETA: 0s - loss: 0.3892 - auc: 0.709 - 0s 22us/step - loss: 0.3928 - auc: 0.7025 - val_loss: 0.3945 - val_auc: 0.7035\n",
      "\n",
      "Epoch 00994: val_loss improved from 0.39462 to 0.39453, saving model to DeepFM.h5\n",
      "Epoch 995/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3442 - auc: 0.748 - ETA: 0s - loss: 0.3988 - auc: 0.698 - 0s 22us/step - loss: 0.3942 - auc: 0.6946 - val_loss: 0.3945 - val_auc: 0.7043\n",
      "\n",
      "Epoch 00995: val_loss improved from 0.39453 to 0.39447, saving model to DeepFM.h5\n",
      "Epoch 996/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3049 - auc: 0.805 - ETA: 0s - loss: 0.3789 - auc: 0.708 - 0s 21us/step - loss: 0.3911 - auc: 0.7074 - val_loss: 0.3944 - val_auc: 0.7038\n",
      "\n",
      "Epoch 00996: val_loss improved from 0.39447 to 0.39441, saving model to DeepFM.h5\n",
      "Epoch 997/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3708 - auc: 0.704 - ETA: 0s - loss: 0.3970 - auc: 0.704 - 0s 22us/step - loss: 0.3920 - auc: 0.7057 - val_loss: 0.3943 - val_auc: 0.7040\n",
      "\n",
      "Epoch 00997: val_loss improved from 0.39441 to 0.39435, saving model to DeepFM.h5\n",
      "Epoch 998/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3394 - auc: 0.754 - ETA: 0s - loss: 0.3805 - auc: 0.722 - 0s 23us/step - loss: 0.3898 - auc: 0.7241 - val_loss: 0.3943 - val_auc: 0.7040\n",
      "\n",
      "Epoch 00998: val_loss improved from 0.39435 to 0.39429, saving model to DeepFM.h5\n",
      "Epoch 999/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4334 - auc: 0.777 - ETA: 0s - loss: 0.3990 - auc: 0.709 - 0s 22us/step - loss: 0.3956 - auc: 0.6949 - val_loss: 0.3943 - val_auc: 0.7052\n",
      "\n",
      "Epoch 00999: val_loss improved from 0.39429 to 0.39426, saving model to DeepFM.h5\n",
      "Epoch 1000/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4255 - auc: 0.732 - ETA: 0s - loss: 0.4003 - auc: 0.694 - 0s 22us/step - loss: 0.3923 - auc: 0.7030 - val_loss: 0.3942 - val_auc: 0.7043\n",
      "\n",
      "Epoch 01000: val_loss improved from 0.39426 to 0.39422, saving model to DeepFM.h5\n",
      "Epoch 1001/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.730 - ETA: 0s - loss: 0.3941 - auc: 0.736 - 0s 22us/step - loss: 0.3884 - auc: 0.7278 - val_loss: 0.3942 - val_auc: 0.7060\n",
      "\n",
      "Epoch 01001: val_loss improved from 0.39422 to 0.39418, saving model to DeepFM.h5\n",
      "Epoch 1002/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3292 - auc: 0.779 - ETA: 0s - loss: 0.3940 - auc: 0.724 - 0s 23us/step - loss: 0.3901 - auc: 0.7227 - val_loss: 0.3941 - val_auc: 0.7061\n",
      "\n",
      "Epoch 01002: val_loss improved from 0.39418 to 0.39411, saving model to DeepFM.h5\n",
      "Epoch 1003/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3551 - auc: 0.669 - ETA: 0s - loss: 0.3927 - auc: 0.697 - 0s 22us/step - loss: 0.3946 - auc: 0.6972 - val_loss: 0.3940 - val_auc: 0.7062\n",
      "\n",
      "Epoch 01003: val_loss improved from 0.39411 to 0.39403, saving model to DeepFM.h5\n",
      "Epoch 1004/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4347 - auc: 0.709 - ETA: 0s - loss: 0.3927 - auc: 0.700 - 0s 23us/step - loss: 0.3908 - auc: 0.7060 - val_loss: 0.3940 - val_auc: 0.7061\n",
      "\n",
      "Epoch 01004: val_loss improved from 0.39403 to 0.39397, saving model to DeepFM.h5\n",
      "Epoch 1005/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3635 - auc: 0.720 - ETA: 0s - loss: 0.3843 - auc: 0.715 - 0s 22us/step - loss: 0.3896 - auc: 0.7158 - val_loss: 0.3939 - val_auc: 0.7061\n",
      "\n",
      "Epoch 01005: val_loss improved from 0.39397 to 0.39389, saving model to DeepFM.h5\n",
      "Epoch 1006/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3517 - auc: 0.829 - ETA: 0s - loss: 0.3900 - auc: 0.710 - 0s 22us/step - loss: 0.3918 - auc: 0.7089 - val_loss: 0.3938 - val_auc: 0.7061\n",
      "\n",
      "Epoch 01006: val_loss improved from 0.39389 to 0.39384, saving model to DeepFM.h5\n",
      "Epoch 1007/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3970 - auc: 0.701 - ETA: 0s - loss: 0.3874 - auc: 0.710 - 0s 21us/step - loss: 0.3890 - auc: 0.7178 - val_loss: 0.3938 - val_auc: 0.7063\n",
      "\n",
      "Epoch 01007: val_loss improved from 0.39384 to 0.39377, saving model to DeepFM.h5\n",
      "Epoch 1008/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3384 - auc: 0.688 - ETA: 0s - loss: 0.3861 - auc: 0.708 - 0s 23us/step - loss: 0.3906 - auc: 0.7136 - val_loss: 0.3937 - val_auc: 0.7070\n",
      "\n",
      "Epoch 01008: val_loss improved from 0.39377 to 0.39371, saving model to DeepFM.h5\n",
      "Epoch 1009/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3866 - auc: 0.732 - ETA: 0s - loss: 0.3833 - auc: 0.736 - 0s 22us/step - loss: 0.3932 - auc: 0.7090 - val_loss: 0.3937 - val_auc: 0.7068\n",
      "\n",
      "Epoch 01009: val_loss improved from 0.39371 to 0.39365, saving model to DeepFM.h5\n",
      "Epoch 1010/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4194 - auc: 0.703 - ETA: 0s - loss: 0.3957 - auc: 0.696 - 0s 24us/step - loss: 0.3927 - auc: 0.6983 - val_loss: 0.3936 - val_auc: 0.7077\n",
      "\n",
      "Epoch 01010: val_loss improved from 0.39365 to 0.39358, saving model to DeepFM.h5\n",
      "Epoch 1011/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4215 - auc: 0.652 - ETA: 0s - loss: 0.3899 - auc: 0.709 - 0s 22us/step - loss: 0.3924 - auc: 0.7051 - val_loss: 0.3935 - val_auc: 0.7065\n",
      "\n",
      "Epoch 01011: val_loss improved from 0.39358 to 0.39353, saving model to DeepFM.h5\n",
      "Epoch 1012/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3893 - auc: 0.741 - ETA: 0s - loss: 0.3810 - auc: 0.709 - 0s 22us/step - loss: 0.3944 - auc: 0.6996 - val_loss: 0.3935 - val_auc: 0.7071\n",
      "\n",
      "Epoch 01012: val_loss improved from 0.39353 to 0.39346, saving model to DeepFM.h5\n",
      "Epoch 1013/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4059 - auc: 0.647 - ETA: 0s - loss: 0.3994 - auc: 0.698 - 0s 22us/step - loss: 0.3903 - auc: 0.7094 - val_loss: 0.3934 - val_auc: 0.7082\n",
      "\n",
      "Epoch 01013: val_loss improved from 0.39346 to 0.39341, saving model to DeepFM.h5\n",
      "Epoch 1014/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3454 - auc: 0.745 - ETA: 0s - loss: 0.3985 - auc: 0.687 - 0s 22us/step - loss: 0.3905 - auc: 0.7107 - val_loss: 0.3933 - val_auc: 0.7085\n",
      "\n",
      "Epoch 01014: val_loss improved from 0.39341 to 0.39334, saving model to DeepFM.h5\n",
      "Epoch 1015/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4115 - auc: 0.716 - ETA: 0s - loss: 0.3871 - auc: 0.723 - 0s 21us/step - loss: 0.3916 - auc: 0.7080 - val_loss: 0.3933 - val_auc: 0.7083\n",
      "\n",
      "Epoch 01015: val_loss improved from 0.39334 to 0.39331, saving model to DeepFM.h5\n",
      "Epoch 1016/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4045 - auc: 0.738 - ETA: 0s - loss: 0.3931 - auc: 0.718 - 0s 21us/step - loss: 0.3909 - auc: 0.7124 - val_loss: 0.3933 - val_auc: 0.7081\n",
      "\n",
      "Epoch 01016: val_loss improved from 0.39331 to 0.39326, saving model to DeepFM.h5\n",
      "Epoch 1017/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4377 - auc: 0.710 - ETA: 0s - loss: 0.3894 - auc: 0.723 - 0s 22us/step - loss: 0.3911 - auc: 0.7103 - val_loss: 0.3932 - val_auc: 0.7084\n",
      "\n",
      "Epoch 01017: val_loss improved from 0.39326 to 0.39322, saving model to DeepFM.h5\n",
      "Epoch 1018/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3891 - auc: 0.756 - ETA: 0s - loss: 0.3945 - auc: 0.710 - 0s 22us/step - loss: 0.3916 - auc: 0.7108 - val_loss: 0.3932 - val_auc: 0.7099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01018: val_loss improved from 0.39322 to 0.39320, saving model to DeepFM.h5\n",
      "Epoch 1019/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4029 - auc: 0.745 - ETA: 0s - loss: 0.3918 - auc: 0.719 - 0s 23us/step - loss: 0.3880 - auc: 0.7263 - val_loss: 0.3932 - val_auc: 0.7103\n",
      "\n",
      "Epoch 01019: val_loss improved from 0.39320 to 0.39315, saving model to DeepFM.h5\n",
      "Epoch 1020/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3381 - auc: 0.750 - ETA: 0s - loss: 0.3862 - auc: 0.717 - 0s 24us/step - loss: 0.3891 - auc: 0.7220 - val_loss: 0.3930 - val_auc: 0.7096\n",
      "\n",
      "Epoch 01020: val_loss improved from 0.39315 to 0.39304, saving model to DeepFM.h5\n",
      "Epoch 1021/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3945 - auc: 0.702 - ETA: 0s - loss: 0.3942 - auc: 0.719 - 0s 22us/step - loss: 0.3911 - auc: 0.7154 - val_loss: 0.3930 - val_auc: 0.7105\n",
      "\n",
      "Epoch 01021: val_loss improved from 0.39304 to 0.39301, saving model to DeepFM.h5\n",
      "Epoch 1022/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4261 - auc: 0.774 - ETA: 0s - loss: 0.3923 - auc: 0.713 - 0s 21us/step - loss: 0.3895 - auc: 0.7179 - val_loss: 0.3930 - val_auc: 0.7108\n",
      "\n",
      "Epoch 01022: val_loss improved from 0.39301 to 0.39295, saving model to DeepFM.h5\n",
      "Epoch 1023/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3514 - auc: 0.783 - ETA: 0s - loss: 0.3920 - auc: 0.721 - 0s 21us/step - loss: 0.3886 - auc: 0.7227 - val_loss: 0.3929 - val_auc: 0.7110\n",
      "\n",
      "Epoch 01023: val_loss improved from 0.39295 to 0.39288, saving model to DeepFM.h5\n",
      "Epoch 1024/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3335 - auc: 0.669 - ETA: 0s - loss: 0.3868 - auc: 0.699 - 0s 21us/step - loss: 0.3943 - auc: 0.6953 - val_loss: 0.3928 - val_auc: 0.7098\n",
      "\n",
      "Epoch 01024: val_loss improved from 0.39288 to 0.39281, saving model to DeepFM.h5\n",
      "Epoch 1025/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3784 - auc: 0.682 - ETA: 0s - loss: 0.3967 - auc: 0.711 - 0s 21us/step - loss: 0.3885 - auc: 0.7140 - val_loss: 0.3927 - val_auc: 0.7108\n",
      "\n",
      "Epoch 01025: val_loss improved from 0.39281 to 0.39273, saving model to DeepFM.h5\n",
      "Epoch 1026/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4248 - auc: 0.672 - ETA: 0s - loss: 0.3852 - auc: 0.720 - 0s 23us/step - loss: 0.3926 - auc: 0.7113 - val_loss: 0.3927 - val_auc: 0.7097\n",
      "\n",
      "Epoch 01026: val_loss improved from 0.39273 to 0.39268, saving model to DeepFM.h5\n",
      "Epoch 1027/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3191 - auc: 0.655 - ETA: 0s - loss: 0.3972 - auc: 0.715 - 0s 22us/step - loss: 0.3882 - auc: 0.7216 - val_loss: 0.3927 - val_auc: 0.7101\n",
      "\n",
      "Epoch 01027: val_loss improved from 0.39268 to 0.39266, saving model to DeepFM.h5\n",
      "Epoch 1028/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3809 - auc: 0.785 - ETA: 0s - loss: 0.3838 - auc: 0.721 - 0s 21us/step - loss: 0.3875 - auc: 0.7298 - val_loss: 0.3926 - val_auc: 0.7108\n",
      "\n",
      "Epoch 01028: val_loss improved from 0.39266 to 0.39257, saving model to DeepFM.h5\n",
      "Epoch 1029/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3702 - auc: 0.782 - ETA: 0s - loss: 0.3931 - auc: 0.713 - 0s 21us/step - loss: 0.3890 - auc: 0.7121 - val_loss: 0.3925 - val_auc: 0.7107\n",
      "\n",
      "Epoch 01029: val_loss improved from 0.39257 to 0.39249, saving model to DeepFM.h5\n",
      "Epoch 1030/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3678 - auc: 0.714 - ETA: 0s - loss: 0.3826 - auc: 0.708 - 0s 21us/step - loss: 0.3909 - auc: 0.7105 - val_loss: 0.3924 - val_auc: 0.7108\n",
      "\n",
      "Epoch 01030: val_loss improved from 0.39249 to 0.39244, saving model to DeepFM.h5\n",
      "Epoch 1031/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4195 - auc: 0.649 - ETA: 0s - loss: 0.3926 - auc: 0.712 - 0s 21us/step - loss: 0.3922 - auc: 0.7083 - val_loss: 0.3924 - val_auc: 0.7108\n",
      "\n",
      "Epoch 01031: val_loss improved from 0.39244 to 0.39238, saving model to DeepFM.h5\n",
      "Epoch 1032/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3744 - auc: 0.702 - ETA: 0s - loss: 0.3868 - auc: 0.704 - 0s 21us/step - loss: 0.3900 - auc: 0.7147 - val_loss: 0.3922 - val_auc: 0.7117\n",
      "\n",
      "Epoch 01032: val_loss improved from 0.39238 to 0.39225, saving model to DeepFM.h5\n",
      "Epoch 1033/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3897 - auc: 0.722 - ETA: 0s - loss: 0.3909 - auc: 0.707 - 0s 23us/step - loss: 0.3900 - auc: 0.7140 - val_loss: 0.3922 - val_auc: 0.7111\n",
      "\n",
      "Epoch 01033: val_loss improved from 0.39225 to 0.39221, saving model to DeepFM.h5\n",
      "Epoch 1034/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3667 - auc: 0.810 - ETA: 0s - loss: 0.3824 - auc: 0.726 - 0s 21us/step - loss: 0.3880 - auc: 0.7183 - val_loss: 0.3922 - val_auc: 0.7114\n",
      "\n",
      "Epoch 01034: val_loss improved from 0.39221 to 0.39218, saving model to DeepFM.h5\n",
      "Epoch 1035/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3628 - auc: 0.746 - ETA: 0s - loss: 0.3841 - auc: 0.728 - 0s 21us/step - loss: 0.3886 - auc: 0.7213 - val_loss: 0.3921 - val_auc: 0.7116\n",
      "\n",
      "Epoch 01035: val_loss improved from 0.39218 to 0.39212, saving model to DeepFM.h5\n",
      "Epoch 1036/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4079 - auc: 0.732 - ETA: 0s - loss: 0.3856 - auc: 0.718 - 0s 23us/step - loss: 0.3878 - auc: 0.7235 - val_loss: 0.3921 - val_auc: 0.7115\n",
      "\n",
      "Epoch 01036: val_loss improved from 0.39212 to 0.39207, saving model to DeepFM.h5\n",
      "Epoch 1037/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4303 - auc: 0.736 - ETA: 0s - loss: 0.3831 - auc: 0.742 - 0s 22us/step - loss: 0.3849 - auc: 0.7335 - val_loss: 0.3920 - val_auc: 0.7124\n",
      "\n",
      "Epoch 01037: val_loss improved from 0.39207 to 0.39201, saving model to DeepFM.h5\n",
      "Epoch 1038/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3246 - auc: 0.727 - ETA: 0s - loss: 0.3956 - auc: 0.715 - 0s 22us/step - loss: 0.3894 - auc: 0.7172 - val_loss: 0.3919 - val_auc: 0.7139\n",
      "\n",
      "Epoch 01038: val_loss improved from 0.39201 to 0.39191, saving model to DeepFM.h5\n",
      "Epoch 1039/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3831 - auc: 0.630 - ETA: 0s - loss: 0.3841 - auc: 0.731 - 0s 22us/step - loss: 0.3879 - auc: 0.7284 - val_loss: 0.3919 - val_auc: 0.7129\n",
      "\n",
      "Epoch 01039: val_loss improved from 0.39191 to 0.39185, saving model to DeepFM.h5\n",
      "Epoch 1040/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4030 - auc: 0.770 - ETA: 0s - loss: 0.3903 - auc: 0.723 - 0s 22us/step - loss: 0.3874 - auc: 0.7252 - val_loss: 0.3918 - val_auc: 0.7132\n",
      "\n",
      "Epoch 01040: val_loss improved from 0.39185 to 0.39179, saving model to DeepFM.h5\n",
      "Epoch 1041/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3784 - auc: 0.736 - ETA: 0s - loss: 0.3866 - auc: 0.713 - 0s 22us/step - loss: 0.3913 - auc: 0.7139 - val_loss: 0.3917 - val_auc: 0.7156\n",
      "\n",
      "Epoch 01041: val_loss improved from 0.39179 to 0.39168, saving model to DeepFM.h5\n",
      "Epoch 1042/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3486 - auc: 0.730 - ETA: 0s - loss: 0.3865 - auc: 0.734 - 0s 22us/step - loss: 0.3856 - auc: 0.7362 - val_loss: 0.3917 - val_auc: 0.7140\n",
      "\n",
      "Epoch 01042: val_loss improved from 0.39168 to 0.39165, saving model to DeepFM.h5\n",
      "Epoch 1043/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3820 - auc: 0.753 - ETA: 0s - loss: 0.3881 - auc: 0.719 - 0s 23us/step - loss: 0.3866 - auc: 0.7278 - val_loss: 0.3916 - val_auc: 0.7137\n",
      "\n",
      "Epoch 01043: val_loss improved from 0.39165 to 0.39156, saving model to DeepFM.h5\n",
      "Epoch 1044/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3196 - auc: 0.749 - ETA: 0s - loss: 0.3844 - auc: 0.719 - 0s 24us/step - loss: 0.3880 - auc: 0.7205 - val_loss: 0.3915 - val_auc: 0.7140\n",
      "\n",
      "Epoch 01044: val_loss improved from 0.39156 to 0.39154, saving model to DeepFM.h5\n",
      "Epoch 1045/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4433 - auc: 0.772 - ETA: 0s - loss: 0.3801 - auc: 0.730 - 0s 22us/step - loss: 0.3883 - auc: 0.7214 - val_loss: 0.3915 - val_auc: 0.7137\n",
      "\n",
      "Epoch 01045: val_loss improved from 0.39154 to 0.39146, saving model to DeepFM.h5\n",
      "Epoch 1046/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4647 - auc: 0.671 - ETA: 0s - loss: 0.3982 - auc: 0.710 - 0s 21us/step - loss: 0.3905 - auc: 0.7128 - val_loss: 0.3915 - val_auc: 0.7146\n",
      "\n",
      "Epoch 01046: val_loss improved from 0.39146 to 0.39145, saving model to DeepFM.h5\n",
      "Epoch 1047/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3247 - auc: 0.731 - ETA: 0s - loss: 0.3848 - auc: 0.716 - 0s 23us/step - loss: 0.3887 - auc: 0.7231 - val_loss: 0.3913 - val_auc: 0.7148\n",
      "\n",
      "Epoch 01047: val_loss improved from 0.39145 to 0.39134, saving model to DeepFM.h5\n",
      "Epoch 1048/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3557 - auc: 0.740 - ETA: 0s - loss: 0.3879 - auc: 0.720 - 0s 24us/step - loss: 0.3885 - auc: 0.7219 - val_loss: 0.3913 - val_auc: 0.7142\n",
      "\n",
      "Epoch 01048: val_loss improved from 0.39134 to 0.39130, saving model to DeepFM.h5\n",
      "Epoch 1049/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3487 - auc: 0.787 - ETA: 0s - loss: 0.3776 - auc: 0.725 - 0s 23us/step - loss: 0.3915 - auc: 0.7031 - val_loss: 0.3912 - val_auc: 0.7148\n",
      "\n",
      "Epoch 01049: val_loss improved from 0.39130 to 0.39120, saving model to DeepFM.h5\n",
      "Epoch 1050/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3782 - auc: 0.763 - ETA: 0s - loss: 0.3942 - auc: 0.711 - 0s 22us/step - loss: 0.3884 - auc: 0.7163 - val_loss: 0.3911 - val_auc: 0.7152\n",
      "\n",
      "Epoch 01050: val_loss improved from 0.39120 to 0.39114, saving model to DeepFM.h5\n",
      "Epoch 1051/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3557 - auc: 0.672 - ETA: 0s - loss: 0.3711 - auc: 0.725 - 0s 21us/step - loss: 0.3876 - auc: 0.7228 - val_loss: 0.3910 - val_auc: 0.7141\n",
      "\n",
      "Epoch 01051: val_loss improved from 0.39114 to 0.39104, saving model to DeepFM.h5\n",
      "Epoch 1052/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3238 - auc: 0.736 - ETA: 0s - loss: 0.3761 - auc: 0.715 - 0s 23us/step - loss: 0.3861 - auc: 0.7255 - val_loss: 0.3910 - val_auc: 0.7155\n",
      "\n",
      "Epoch 01052: val_loss improved from 0.39104 to 0.39104, saving model to DeepFM.h5\n",
      "Epoch 1053/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3996 - auc: 0.667 - ETA: 0s - loss: 0.3802 - auc: 0.717 - 0s 23us/step - loss: 0.3891 - auc: 0.7190 - val_loss: 0.3909 - val_auc: 0.7144\n",
      "\n",
      "Epoch 01053: val_loss improved from 0.39104 to 0.39093, saving model to DeepFM.h5\n",
      "Epoch 1054/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3715 - auc: 0.771 - ETA: 0s - loss: 0.3935 - auc: 0.729 - 0s 22us/step - loss: 0.3846 - auc: 0.7344 - val_loss: 0.3909 - val_auc: 0.7154\n",
      "\n",
      "Epoch 01054: val_loss improved from 0.39093 to 0.39089, saving model to DeepFM.h5\n",
      "Epoch 1055/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4788 - auc: 0.599 - ETA: 0s - loss: 0.3852 - auc: 0.717 - 0s 21us/step - loss: 0.3879 - auc: 0.7161 - val_loss: 0.3908 - val_auc: 0.7155\n",
      "\n",
      "Epoch 01055: val_loss improved from 0.39089 to 0.39084, saving model to DeepFM.h5\n",
      "Epoch 1056/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4067 - auc: 0.694 - ETA: 0s - loss: 0.3800 - auc: 0.711 - 0s 22us/step - loss: 0.3888 - auc: 0.7184 - val_loss: 0.3907 - val_auc: 0.7155\n",
      "\n",
      "Epoch 01056: val_loss improved from 0.39084 to 0.39074, saving model to DeepFM.h5\n",
      "Epoch 1057/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3801 - auc: 0.724 - ETA: 0s - loss: 0.3856 - auc: 0.727 - 0s 24us/step - loss: 0.3875 - auc: 0.7237 - val_loss: 0.3907 - val_auc: 0.7156\n",
      "\n",
      "Epoch 01057: val_loss improved from 0.39074 to 0.39073, saving model to DeepFM.h5\n",
      "Epoch 1058/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4178 - auc: 0.743 - ETA: 0s - loss: 0.3873 - auc: 0.726 - 0s 22us/step - loss: 0.3883 - auc: 0.7198 - val_loss: 0.3907 - val_auc: 0.7167\n",
      "\n",
      "Epoch 01058: val_loss improved from 0.39073 to 0.39069, saving model to DeepFM.h5\n",
      "Epoch 1059/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4093 - auc: 0.625 - ETA: 0s - loss: 0.3911 - auc: 0.691 - 0s 23us/step - loss: 0.3904 - auc: 0.7104 - val_loss: 0.3906 - val_auc: 0.7179\n",
      "\n",
      "Epoch 01059: val_loss improved from 0.39069 to 0.39062, saving model to DeepFM.h5\n",
      "Epoch 1060/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4356 - auc: 0.743 - ETA: 0s - loss: 0.3977 - auc: 0.723 - 0s 24us/step - loss: 0.3856 - auc: 0.7280 - val_loss: 0.3906 - val_auc: 0.7177\n",
      "\n",
      "Epoch 01060: val_loss improved from 0.39062 to 0.39061, saving model to DeepFM.h5\n",
      "Epoch 1061/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4058 - auc: 0.717 - ETA: 0s - loss: 0.3936 - auc: 0.708 - 0s 22us/step - loss: 0.3870 - auc: 0.7141 - val_loss: 0.3906 - val_auc: 0.7182\n",
      "\n",
      "Epoch 01061: val_loss improved from 0.39061 to 0.39056, saving model to DeepFM.h5\n",
      "Epoch 1062/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3830 - auc: 0.733 - ETA: 0s - loss: 0.3946 - auc: 0.711 - 0s 21us/step - loss: 0.3882 - auc: 0.7177 - val_loss: 0.3905 - val_auc: 0.7180\n",
      "\n",
      "Epoch 01062: val_loss improved from 0.39056 to 0.39050, saving model to DeepFM.h5\n",
      "Epoch 1063/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3329 - auc: 0.734 - ETA: 0s - loss: 0.3865 - auc: 0.729 - 0s 21us/step - loss: 0.3863 - auc: 0.7281 - val_loss: 0.3904 - val_auc: 0.7189\n",
      "\n",
      "Epoch 01063: val_loss improved from 0.39050 to 0.39042, saving model to DeepFM.h5\n",
      "Epoch 1064/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3266 - auc: 0.782 - ETA: 0s - loss: 0.3844 - auc: 0.729 - 0s 21us/step - loss: 0.3856 - auc: 0.7244 - val_loss: 0.3903 - val_auc: 0.7187\n",
      "\n",
      "Epoch 01064: val_loss improved from 0.39042 to 0.39035, saving model to DeepFM.h5\n",
      "Epoch 1065/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3411 - auc: 0.670 - ETA: 0s - loss: 0.3926 - auc: 0.715 - 0s 22us/step - loss: 0.3873 - auc: 0.7181 - val_loss: 0.3903 - val_auc: 0.7191\n",
      "\n",
      "Epoch 01065: val_loss improved from 0.39035 to 0.39026, saving model to DeepFM.h5\n",
      "Epoch 1066/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4157 - auc: 0.662 - ETA: 0s - loss: 0.3884 - auc: 0.715 - 0s 20us/step - loss: 0.3864 - auc: 0.7255 - val_loss: 0.3902 - val_auc: 0.7185\n",
      "\n",
      "Epoch 01066: val_loss improved from 0.39026 to 0.39016, saving model to DeepFM.h5\n",
      "Epoch 1067/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4008 - auc: 0.758 - ETA: 0s - loss: 0.3768 - auc: 0.725 - 0s 21us/step - loss: 0.3853 - auc: 0.7295 - val_loss: 0.3901 - val_auc: 0.7181\n",
      "\n",
      "Epoch 01067: val_loss improved from 0.39016 to 0.39007, saving model to DeepFM.h5\n",
      "Epoch 1068/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3844 - auc: 0.705 - ETA: 0s - loss: 0.3770 - auc: 0.721 - 0s 22us/step - loss: 0.3840 - auc: 0.7249 - val_loss: 0.3900 - val_auc: 0.7186\n",
      "\n",
      "Epoch 01068: val_loss improved from 0.39007 to 0.39002, saving model to DeepFM.h5\n",
      "Epoch 1069/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3615 - auc: 0.732 - ETA: 0s - loss: 0.3873 - auc: 0.725 - 0s 21us/step - loss: 0.3850 - auc: 0.7280 - val_loss: 0.3900 - val_auc: 0.7188\n",
      "\n",
      "Epoch 01069: val_loss improved from 0.39002 to 0.38997, saving model to DeepFM.h5\n",
      "Epoch 1070/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3584 - auc: 0.719 - ETA: 0s - loss: 0.3752 - auc: 0.723 - 0s 25us/step - loss: 0.3847 - auc: 0.7294 - val_loss: 0.3899 - val_auc: 0.7190\n",
      "\n",
      "Epoch 01070: val_loss improved from 0.38997 to 0.38990, saving model to DeepFM.h5\n",
      "Epoch 1071/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3970 - auc: 0.741 - ETA: 0s - loss: 0.3847 - auc: 0.728 - 0s 22us/step - loss: 0.3843 - auc: 0.7307 - val_loss: 0.3899 - val_auc: 0.7176\n",
      "\n",
      "Epoch 01071: val_loss improved from 0.38990 to 0.38987, saving model to DeepFM.h5\n",
      "Epoch 1072/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.763 - ETA: 0s - loss: 0.3828 - auc: 0.720 - 0s 20us/step - loss: 0.3852 - auc: 0.7199 - val_loss: 0.3898 - val_auc: 0.7184\n",
      "\n",
      "Epoch 01072: val_loss improved from 0.38987 to 0.38980, saving model to DeepFM.h5\n",
      "Epoch 1073/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3644 - auc: 0.706 - ETA: 0s - loss: 0.3859 - auc: 0.731 - 0s 21us/step - loss: 0.3842 - auc: 0.7327 - val_loss: 0.3898 - val_auc: 0.7191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01073: val_loss improved from 0.38980 to 0.38978, saving model to DeepFM.h5\n",
      "Epoch 1074/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4335 - auc: 0.733 - ETA: 0s - loss: 0.3875 - auc: 0.718 - 0s 22us/step - loss: 0.3881 - auc: 0.7165 - val_loss: 0.3897 - val_auc: 0.7200\n",
      "\n",
      "Epoch 01074: val_loss improved from 0.38978 to 0.38972, saving model to DeepFM.h5\n",
      "Epoch 1075/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4078 - auc: 0.733 - ETA: 0s - loss: 0.3873 - auc: 0.729 - 0s 22us/step - loss: 0.3861 - auc: 0.7258 - val_loss: 0.3896 - val_auc: 0.7205\n",
      "\n",
      "Epoch 01075: val_loss improved from 0.38972 to 0.38961, saving model to DeepFM.h5\n",
      "Epoch 1076/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4138 - auc: 0.746 - ETA: 0s - loss: 0.3931 - auc: 0.720 - 0s 23us/step - loss: 0.3850 - auc: 0.7316 - val_loss: 0.3895 - val_auc: 0.7208\n",
      "\n",
      "Epoch 01076: val_loss improved from 0.38961 to 0.38955, saving model to DeepFM.h5\n",
      "Epoch 1077/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4112 - auc: 0.730 - ETA: 0s - loss: 0.3920 - auc: 0.714 - 0s 23us/step - loss: 0.3850 - auc: 0.7283 - val_loss: 0.3894 - val_auc: 0.7196\n",
      "\n",
      "Epoch 01077: val_loss improved from 0.38955 to 0.38944, saving model to DeepFM.h5\n",
      "Epoch 1078/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4006 - auc: 0.730 - ETA: 0s - loss: 0.3857 - auc: 0.733 - 0s 22us/step - loss: 0.3830 - auc: 0.7367 - val_loss: 0.3894 - val_auc: 0.7204\n",
      "\n",
      "Epoch 01078: val_loss improved from 0.38944 to 0.38940, saving model to DeepFM.h5\n",
      "Epoch 1079/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3729 - auc: 0.789 - ETA: 0s - loss: 0.3800 - auc: 0.725 - 0s 21us/step - loss: 0.3843 - auc: 0.7311 - val_loss: 0.3893 - val_auc: 0.7207\n",
      "\n",
      "Epoch 01079: val_loss improved from 0.38940 to 0.38930, saving model to DeepFM.h5\n",
      "Epoch 1080/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3719 - auc: 0.691 - ETA: 0s - loss: 0.3758 - auc: 0.743 - 0s 25us/step - loss: 0.3852 - auc: 0.7327 - val_loss: 0.3892 - val_auc: 0.7214\n",
      "\n",
      "Epoch 01080: val_loss improved from 0.38930 to 0.38922, saving model to DeepFM.h5\n",
      "Epoch 1081/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3896 - auc: 0.756 - ETA: 0s - loss: 0.3847 - auc: 0.744 - 0s 21us/step - loss: 0.3861 - auc: 0.7334 - val_loss: 0.3891 - val_auc: 0.7227\n",
      "\n",
      "Epoch 01081: val_loss improved from 0.38922 to 0.38909, saving model to DeepFM.h5\n",
      "Epoch 1082/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4842 - auc: 0.631 - ETA: 0s - loss: 0.4049 - auc: 0.728 - 0s 22us/step - loss: 0.3860 - auc: 0.7287 - val_loss: 0.3892 - val_auc: 0.7219\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.38909\n",
      "Epoch 1083/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3860 - auc: 0.696 - ETA: 0s - loss: 0.3789 - auc: 0.713 - 0s 22us/step - loss: 0.3864 - auc: 0.7215 - val_loss: 0.3890 - val_auc: 0.7222\n",
      "\n",
      "Epoch 01083: val_loss improved from 0.38909 to 0.38902, saving model to DeepFM.h5\n",
      "Epoch 1084/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3588 - auc: 0.818 - ETA: 0s - loss: 0.3822 - auc: 0.735 - 0s 22us/step - loss: 0.3867 - auc: 0.7204 - val_loss: 0.3889 - val_auc: 0.7230\n",
      "\n",
      "Epoch 01084: val_loss improved from 0.38902 to 0.38885, saving model to DeepFM.h5\n",
      "Epoch 1085/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3627 - auc: 0.720 - ETA: 0s - loss: 0.3908 - auc: 0.737 - 0s 22us/step - loss: 0.3852 - auc: 0.7347 - val_loss: 0.3888 - val_auc: 0.7225\n",
      "\n",
      "Epoch 01085: val_loss improved from 0.38885 to 0.38880, saving model to DeepFM.h5\n",
      "Epoch 1086/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3914 - auc: 0.768 - ETA: 0s - loss: 0.3847 - auc: 0.731 - 0s 22us/step - loss: 0.3864 - auc: 0.7262 - val_loss: 0.3888 - val_auc: 0.7226\n",
      "\n",
      "Epoch 01086: val_loss improved from 0.38880 to 0.38877, saving model to DeepFM.h5\n",
      "Epoch 1087/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4139 - auc: 0.706 - ETA: 0s - loss: 0.3846 - auc: 0.739 - 0s 21us/step - loss: 0.3847 - auc: 0.7311 - val_loss: 0.3888 - val_auc: 0.7232\n",
      "\n",
      "Epoch 01087: val_loss improved from 0.38877 to 0.38876, saving model to DeepFM.h5\n",
      "Epoch 1088/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.774 - ETA: 0s - loss: 0.3867 - auc: 0.729 - 0s 22us/step - loss: 0.3831 - auc: 0.7286 - val_loss: 0.3887 - val_auc: 0.7220\n",
      "\n",
      "Epoch 01088: val_loss improved from 0.38876 to 0.38873, saving model to DeepFM.h5\n",
      "Epoch 1089/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3920 - auc: 0.753 - ETA: 0s - loss: 0.3846 - auc: 0.735 - 0s 22us/step - loss: 0.3835 - auc: 0.7328 - val_loss: 0.3886 - val_auc: 0.7237\n",
      "\n",
      "Epoch 01089: val_loss improved from 0.38873 to 0.38863, saving model to DeepFM.h5\n",
      "Epoch 1090/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4562 - auc: 0.602 - ETA: 0s - loss: 0.3902 - auc: 0.715 - 0s 21us/step - loss: 0.3863 - auc: 0.7165 - val_loss: 0.3886 - val_auc: 0.7236\n",
      "\n",
      "Epoch 01090: val_loss improved from 0.38863 to 0.38860, saving model to DeepFM.h5\n",
      "Epoch 1091/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4137 - auc: 0.672 - ETA: 0s - loss: 0.3752 - auc: 0.736 - 0s 22us/step - loss: 0.3866 - auc: 0.7293 - val_loss: 0.3884 - val_auc: 0.7230\n",
      "\n",
      "Epoch 01091: val_loss improved from 0.38860 to 0.38837, saving model to DeepFM.h5\n",
      "Epoch 1092/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3632 - auc: 0.665 - ETA: 0s - loss: 0.3906 - auc: 0.732 - 0s 22us/step - loss: 0.3837 - auc: 0.7326 - val_loss: 0.3883 - val_auc: 0.7228\n",
      "\n",
      "Epoch 01092: val_loss improved from 0.38837 to 0.38829, saving model to DeepFM.h5\n",
      "Epoch 1093/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3960 - auc: 0.762 - ETA: 0s - loss: 0.3796 - auc: 0.743 - 0s 22us/step - loss: 0.3817 - auc: 0.7384 - val_loss: 0.3882 - val_auc: 0.7247\n",
      "\n",
      "Epoch 01093: val_loss improved from 0.38829 to 0.38823, saving model to DeepFM.h5\n",
      "Epoch 1094/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4205 - auc: 0.754 - ETA: 0s - loss: 0.3755 - auc: 0.751 - 0s 21us/step - loss: 0.3815 - auc: 0.7441 - val_loss: 0.3882 - val_auc: 0.7252\n",
      "\n",
      "Epoch 01094: val_loss improved from 0.38823 to 0.38819, saving model to DeepFM.h5\n",
      "Epoch 1095/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3976 - auc: 0.772 - ETA: 0s - loss: 0.3826 - auc: 0.731 - 0s 21us/step - loss: 0.3843 - auc: 0.7332 - val_loss: 0.3882 - val_auc: 0.7255\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.38819\n",
      "Epoch 1096/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3525 - auc: 0.807 - ETA: 0s - loss: 0.3868 - auc: 0.737 - 0s 21us/step - loss: 0.3841 - auc: 0.7358 - val_loss: 0.3881 - val_auc: 0.7258\n",
      "\n",
      "Epoch 01096: val_loss improved from 0.38819 to 0.38815, saving model to DeepFM.h5\n",
      "Epoch 1097/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3775 - auc: 0.763 - ETA: 0s - loss: 0.3844 - auc: 0.725 - 0s 22us/step - loss: 0.3839 - auc: 0.7321 - val_loss: 0.3881 - val_auc: 0.7260\n",
      "\n",
      "Epoch 01097: val_loss improved from 0.38815 to 0.38806, saving model to DeepFM.h5\n",
      "Epoch 1098/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3415 - auc: 0.758 - ETA: 0s - loss: 0.3826 - auc: 0.728 - 0s 22us/step - loss: 0.3845 - auc: 0.7261 - val_loss: 0.3879 - val_auc: 0.7261\n",
      "\n",
      "Epoch 01098: val_loss improved from 0.38806 to 0.38790, saving model to DeepFM.h5\n",
      "Epoch 1099/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4234 - auc: 0.707 - ETA: 0s - loss: 0.3870 - auc: 0.737 - 0s 23us/step - loss: 0.3833 - auc: 0.7314 - val_loss: 0.3878 - val_auc: 0.7259\n",
      "\n",
      "Epoch 01099: val_loss improved from 0.38790 to 0.38782, saving model to DeepFM.h5\n",
      "Epoch 1100/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4022 - auc: 0.758 - ETA: 0s - loss: 0.3835 - auc: 0.743 - 0s 22us/step - loss: 0.3825 - auc: 0.7384 - val_loss: 0.3878 - val_auc: 0.7267\n",
      "\n",
      "Epoch 01100: val_loss improved from 0.38782 to 0.38778, saving model to DeepFM.h5\n",
      "Epoch 1101/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3329 - auc: 0.770 - ETA: 0s - loss: 0.3850 - auc: 0.737 - 0s 21us/step - loss: 0.3841 - auc: 0.7338 - val_loss: 0.3878 - val_auc: 0.7265\n",
      "\n",
      "Epoch 01101: val_loss improved from 0.38778 to 0.38778, saving model to DeepFM.h5\n",
      "Epoch 1102/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4394 - auc: 0.748 - ETA: 0s - loss: 0.3884 - auc: 0.727 - 0s 22us/step - loss: 0.3826 - auc: 0.7302 - val_loss: 0.3877 - val_auc: 0.7269\n",
      "\n",
      "Epoch 01102: val_loss improved from 0.38778 to 0.38773, saving model to DeepFM.h5\n",
      "Epoch 1103/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3758 - auc: 0.750 - ETA: 0s - loss: 0.3770 - auc: 0.731 - 0s 22us/step - loss: 0.3830 - auc: 0.7366 - val_loss: 0.3876 - val_auc: 0.7270\n",
      "\n",
      "Epoch 01103: val_loss improved from 0.38773 to 0.38761, saving model to DeepFM.h5\n",
      "Epoch 1104/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4025 - auc: 0.760 - ETA: 0s - loss: 0.3795 - auc: 0.746 - 0s 20us/step - loss: 0.3794 - auc: 0.7477 - val_loss: 0.3875 - val_auc: 0.7266\n",
      "\n",
      "Epoch 01104: val_loss improved from 0.38761 to 0.38753, saving model to DeepFM.h5\n",
      "Epoch 1105/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3773 - auc: 0.732 - ETA: 0s - loss: 0.3849 - auc: 0.708 - 0s 22us/step - loss: 0.3860 - auc: 0.7132 - val_loss: 0.3875 - val_auc: 0.7271\n",
      "\n",
      "Epoch 01105: val_loss improved from 0.38753 to 0.38748, saving model to DeepFM.h5\n",
      "Epoch 1106/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.724 - ETA: 0s - loss: 0.3926 - auc: 0.729 - 0s 22us/step - loss: 0.3815 - auc: 0.7410 - val_loss: 0.3874 - val_auc: 0.7271\n",
      "\n",
      "Epoch 01106: val_loss improved from 0.38748 to 0.38745, saving model to DeepFM.h5\n",
      "Epoch 1107/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3597 - auc: 0.746 - ETA: 0s - loss: 0.3847 - auc: 0.737 - 0s 22us/step - loss: 0.3792 - auc: 0.7408 - val_loss: 0.3874 - val_auc: 0.7279\n",
      "\n",
      "Epoch 01107: val_loss improved from 0.38745 to 0.38739, saving model to DeepFM.h5\n",
      "Epoch 1108/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4204 - auc: 0.753 - ETA: 0s - loss: 0.3868 - auc: 0.729 - 0s 23us/step - loss: 0.3833 - auc: 0.7337 - val_loss: 0.3873 - val_auc: 0.7280\n",
      "\n",
      "Epoch 01108: val_loss improved from 0.38739 to 0.38726, saving model to DeepFM.h5\n",
      "Epoch 1109/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4389 - auc: 0.736 - ETA: 0s - loss: 0.3776 - auc: 0.739 - 0s 23us/step - loss: 0.3815 - auc: 0.7370 - val_loss: 0.3872 - val_auc: 0.7281\n",
      "\n",
      "Epoch 01109: val_loss improved from 0.38726 to 0.38718, saving model to DeepFM.h5\n",
      "Epoch 1110/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3861 - auc: 0.687 - ETA: 0s - loss: 0.3824 - auc: 0.722 - 0s 22us/step - loss: 0.3860 - auc: 0.7249 - val_loss: 0.3871 - val_auc: 0.7286\n",
      "\n",
      "Epoch 01110: val_loss improved from 0.38718 to 0.38710, saving model to DeepFM.h5\n",
      "Epoch 1111/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4343 - auc: 0.758 - ETA: 0s - loss: 0.3845 - auc: 0.733 - 0s 21us/step - loss: 0.3833 - auc: 0.7368 - val_loss: 0.3871 - val_auc: 0.7282\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.38710\n",
      "Epoch 1112/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4150 - auc: 0.699 - ETA: 0s - loss: 0.3880 - auc: 0.732 - 0s 23us/step - loss: 0.3819 - auc: 0.7396 - val_loss: 0.3870 - val_auc: 0.7284\n",
      "\n",
      "Epoch 01112: val_loss improved from 0.38710 to 0.38701, saving model to DeepFM.h5\n",
      "Epoch 1113/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3587 - auc: 0.782 - ETA: 0s - loss: 0.3805 - auc: 0.751 - 0s 20us/step - loss: 0.3786 - auc: 0.7549 - val_loss: 0.3870 - val_auc: 0.7272\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.38701\n",
      "Epoch 1114/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4043 - auc: 0.771 - ETA: 0s - loss: 0.3652 - auc: 0.748 - 0s 22us/step - loss: 0.3804 - auc: 0.7415 - val_loss: 0.3869 - val_auc: 0.7271\n",
      "\n",
      "Epoch 01114: val_loss improved from 0.38701 to 0.38694, saving model to DeepFM.h5\n",
      "Epoch 1115/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3904 - auc: 0.767 - ETA: 0s - loss: 0.3821 - auc: 0.763 - 0s 23us/step - loss: 0.3795 - auc: 0.7462 - val_loss: 0.3868 - val_auc: 0.7273\n",
      "\n",
      "Epoch 01115: val_loss improved from 0.38694 to 0.38684, saving model to DeepFM.h5\n",
      "Epoch 1116/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.732 - ETA: 0s - loss: 0.3878 - auc: 0.730 - 0s 23us/step - loss: 0.3829 - auc: 0.7310 - val_loss: 0.3868 - val_auc: 0.7270\n",
      "\n",
      "Epoch 01116: val_loss improved from 0.38684 to 0.38678, saving model to DeepFM.h5\n",
      "Epoch 1117/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3737 - auc: 0.764 - ETA: 0s - loss: 0.3713 - auc: 0.752 - 0s 22us/step - loss: 0.3772 - auc: 0.7569 - val_loss: 0.3867 - val_auc: 0.7282\n",
      "\n",
      "Epoch 01117: val_loss improved from 0.38678 to 0.38669, saving model to DeepFM.h5\n",
      "Epoch 1118/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.724 - ETA: 0s - loss: 0.3796 - auc: 0.750 - 0s 21us/step - loss: 0.3813 - auc: 0.7393 - val_loss: 0.3865 - val_auc: 0.7288\n",
      "\n",
      "Epoch 01118: val_loss improved from 0.38669 to 0.38648, saving model to DeepFM.h5\n",
      "Epoch 1119/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3616 - auc: 0.809 - ETA: 0s - loss: 0.3842 - auc: 0.746 - 0s 22us/step - loss: 0.3786 - auc: 0.7491 - val_loss: 0.3865 - val_auc: 0.7287\n",
      "\n",
      "Epoch 01119: val_loss improved from 0.38648 to 0.38647, saving model to DeepFM.h5\n",
      "Epoch 1120/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3463 - auc: 0.738 - ETA: 0s - loss: 0.3760 - auc: 0.755 - 0s 22us/step - loss: 0.3802 - auc: 0.7420 - val_loss: 0.3864 - val_auc: 0.7293\n",
      "\n",
      "Epoch 01120: val_loss improved from 0.38647 to 0.38642, saving model to DeepFM.h5\n",
      "Epoch 1121/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4187 - auc: 0.734 - ETA: 0s - loss: 0.3792 - auc: 0.741 - 0s 22us/step - loss: 0.3815 - auc: 0.7422 - val_loss: 0.3864 - val_auc: 0.7298\n",
      "\n",
      "Epoch 01121: val_loss improved from 0.38642 to 0.38636, saving model to DeepFM.h5\n",
      "Epoch 1122/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3882 - auc: 0.698 - ETA: 0s - loss: 0.3769 - auc: 0.736 - 0s 21us/step - loss: 0.3826 - auc: 0.7330 - val_loss: 0.3862 - val_auc: 0.7301\n",
      "\n",
      "Epoch 01122: val_loss improved from 0.38636 to 0.38620, saving model to DeepFM.h5\n",
      "Epoch 1123/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4393 - auc: 0.703 - ETA: 0s - loss: 0.3774 - auc: 0.740 - 0s 22us/step - loss: 0.3825 - auc: 0.7316 - val_loss: 0.3862 - val_auc: 0.7314\n",
      "\n",
      "Epoch 01123: val_loss improved from 0.38620 to 0.38617, saving model to DeepFM.h5\n",
      "Epoch 1124/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3420 - auc: 0.792 - ETA: 0s - loss: 0.3803 - auc: 0.746 - 0s 24us/step - loss: 0.3793 - auc: 0.7445 - val_loss: 0.3861 - val_auc: 0.7299\n",
      "\n",
      "Epoch 01124: val_loss improved from 0.38617 to 0.38609, saving model to DeepFM.h5\n",
      "Epoch 1125/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3809 - auc: 0.705 - ETA: 0s - loss: 0.3843 - auc: 0.732 - 0s 23us/step - loss: 0.3816 - auc: 0.7336 - val_loss: 0.3860 - val_auc: 0.7299\n",
      "\n",
      "Epoch 01125: val_loss improved from 0.38609 to 0.38600, saving model to DeepFM.h5\n",
      "Epoch 1126/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4054 - auc: 0.649 - ETA: 0s - loss: 0.3824 - auc: 0.725 - 0s 23us/step - loss: 0.3838 - auc: 0.7334 - val_loss: 0.3859 - val_auc: 0.7314\n",
      "\n",
      "Epoch 01126: val_loss improved from 0.38600 to 0.38591, saving model to DeepFM.h5\n",
      "Epoch 1127/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3940 - auc: 0.658 - ETA: 0s - loss: 0.3824 - auc: 0.730 - 0s 23us/step - loss: 0.3840 - auc: 0.7296 - val_loss: 0.3859 - val_auc: 0.7309\n",
      "\n",
      "Epoch 01127: val_loss improved from 0.38591 to 0.38586, saving model to DeepFM.h5\n",
      "Epoch 1128/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3855 - auc: 0.759 - ETA: 0s - loss: 0.3769 - auc: 0.737 - 0s 25us/step - loss: 0.3837 - auc: 0.7279 - val_loss: 0.3859 - val_auc: 0.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01128: val_loss improved from 0.38586 to 0.38586, saving model to DeepFM.h5\n",
      "Epoch 1129/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2876 - auc: 0.735 - ETA: 0s - loss: 0.3856 - auc: 0.735 - 0s 22us/step - loss: 0.3819 - auc: 0.7350 - val_loss: 0.3858 - val_auc: 0.7320\n",
      "\n",
      "Epoch 01129: val_loss improved from 0.38586 to 0.38575, saving model to DeepFM.h5\n",
      "Epoch 1130/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4102 - auc: 0.747 - ETA: 0s - loss: 0.3912 - auc: 0.735 - 0s 22us/step - loss: 0.3782 - auc: 0.7440 - val_loss: 0.3858 - val_auc: 0.7308\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.38575\n",
      "Epoch 1131/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3554 - auc: 0.752 - ETA: 0s - loss: 0.3916 - auc: 0.726 - 0s 23us/step - loss: 0.3839 - auc: 0.7280 - val_loss: 0.3857 - val_auc: 0.7311\n",
      "\n",
      "Epoch 01131: val_loss improved from 0.38575 to 0.38571, saving model to DeepFM.h5\n",
      "Epoch 1132/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3466 - auc: 0.768 - ETA: 0s - loss: 0.3830 - auc: 0.737 - 0s 25us/step - loss: 0.3801 - auc: 0.7367 - val_loss: 0.3856 - val_auc: 0.7313\n",
      "\n",
      "Epoch 01132: val_loss improved from 0.38571 to 0.38558, saving model to DeepFM.h5\n",
      "Epoch 1133/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4086 - auc: 0.733 - ETA: 0s - loss: 0.3722 - auc: 0.753 - 0s 23us/step - loss: 0.3781 - auc: 0.7487 - val_loss: 0.3855 - val_auc: 0.7312\n",
      "\n",
      "Epoch 01133: val_loss improved from 0.38558 to 0.38552, saving model to DeepFM.h5\n",
      "Epoch 1134/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4188 - auc: 0.768 - ETA: 0s - loss: 0.3879 - auc: 0.750 - 0s 21us/step - loss: 0.3782 - auc: 0.7526 - val_loss: 0.3855 - val_auc: 0.7320\n",
      "\n",
      "Epoch 01134: val_loss improved from 0.38552 to 0.38547, saving model to DeepFM.h5\n",
      "Epoch 1135/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3362 - auc: 0.767 - ETA: 0s - loss: 0.3677 - auc: 0.750 - 0s 23us/step - loss: 0.3786 - auc: 0.7401 - val_loss: 0.3853 - val_auc: 0.7320\n",
      "\n",
      "Epoch 01135: val_loss improved from 0.38547 to 0.38534, saving model to DeepFM.h5\n",
      "Epoch 1136/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3623 - auc: 0.711 - ETA: 0s - loss: 0.3806 - auc: 0.748 - 0s 22us/step - loss: 0.3799 - auc: 0.7422 - val_loss: 0.3853 - val_auc: 0.7316\n",
      "\n",
      "Epoch 01136: val_loss improved from 0.38534 to 0.38531, saving model to DeepFM.h5\n",
      "Epoch 1137/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3978 - auc: 0.677 - ETA: 0s - loss: 0.3968 - auc: 0.714 - 0s 23us/step - loss: 0.3793 - auc: 0.7406 - val_loss: 0.3853 - val_auc: 0.7324\n",
      "\n",
      "Epoch 01137: val_loss improved from 0.38531 to 0.38526, saving model to DeepFM.h5\n",
      "Epoch 1138/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3414 - auc: 0.717 - ETA: 0s - loss: 0.3646 - auc: 0.745 - 0s 22us/step - loss: 0.3773 - auc: 0.7450 - val_loss: 0.3851 - val_auc: 0.7325\n",
      "\n",
      "Epoch 01138: val_loss improved from 0.38526 to 0.38512, saving model to DeepFM.h5\n",
      "Epoch 1139/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4084 - auc: 0.697 - ETA: 0s - loss: 0.3691 - auc: 0.749 - 0s 21us/step - loss: 0.3781 - auc: 0.7517 - val_loss: 0.3850 - val_auc: 0.7324\n",
      "\n",
      "Epoch 01139: val_loss improved from 0.38512 to 0.38503, saving model to DeepFM.h5\n",
      "Epoch 1140/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3356 - auc: 0.718 - ETA: 0s - loss: 0.3704 - auc: 0.731 - 0s 20us/step - loss: 0.3844 - auc: 0.7258 - val_loss: 0.3849 - val_auc: 0.7342\n",
      "\n",
      "Epoch 01140: val_loss improved from 0.38503 to 0.38493, saving model to DeepFM.h5\n",
      "Epoch 1141/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4066 - auc: 0.767 - ETA: 0s - loss: 0.3748 - auc: 0.738 - 0s 23us/step - loss: 0.3769 - auc: 0.7484 - val_loss: 0.3849 - val_auc: 0.7338\n",
      "\n",
      "Epoch 01141: val_loss improved from 0.38493 to 0.38492, saving model to DeepFM.h5\n",
      "Epoch 1142/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4306 - auc: 0.725 - ETA: 0s - loss: 0.3910 - auc: 0.742 - 0s 22us/step - loss: 0.3758 - auc: 0.7499 - val_loss: 0.3848 - val_auc: 0.7334\n",
      "\n",
      "Epoch 01142: val_loss improved from 0.38492 to 0.38485, saving model to DeepFM.h5\n",
      "Epoch 1143/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3867 - auc: 0.736 - ETA: 0s - loss: 0.3835 - auc: 0.734 - 0s 23us/step - loss: 0.3798 - auc: 0.7393 - val_loss: 0.3848 - val_auc: 0.7339\n",
      "\n",
      "Epoch 01143: val_loss improved from 0.38485 to 0.38477, saving model to DeepFM.h5\n",
      "Epoch 1144/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3832 - auc: 0.767 - ETA: 0s - loss: 0.3805 - auc: 0.758 - 0s 22us/step - loss: 0.3764 - auc: 0.7553 - val_loss: 0.3846 - val_auc: 0.7339\n",
      "\n",
      "Epoch 01144: val_loss improved from 0.38477 to 0.38458, saving model to DeepFM.h5\n",
      "Epoch 1145/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3246 - auc: 0.836 - ETA: 0s - loss: 0.3798 - auc: 0.749 - 0s 21us/step - loss: 0.3786 - auc: 0.7409 - val_loss: 0.3846 - val_auc: 0.7342\n",
      "\n",
      "Epoch 01145: val_loss improved from 0.38458 to 0.38457, saving model to DeepFM.h5\n",
      "Epoch 1146/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3368 - auc: 0.741 - ETA: 0s - loss: 0.3692 - auc: 0.737 - 0s 23us/step - loss: 0.3750 - auc: 0.7527 - val_loss: 0.3845 - val_auc: 0.7343\n",
      "\n",
      "Epoch 01146: val_loss improved from 0.38457 to 0.38451, saving model to DeepFM.h5\n",
      "Epoch 1147/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3841 - auc: 0.676 - ETA: 0s - loss: 0.3810 - auc: 0.739 - 0s 21us/step - loss: 0.3798 - auc: 0.7390 - val_loss: 0.3844 - val_auc: 0.7351\n",
      "\n",
      "Epoch 01147: val_loss improved from 0.38451 to 0.38443, saving model to DeepFM.h5\n",
      "Epoch 1148/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3709 - auc: 0.719 - ETA: 0s - loss: 0.3777 - auc: 0.754 - 0s 21us/step - loss: 0.3770 - auc: 0.7560 - val_loss: 0.3844 - val_auc: 0.7349\n",
      "\n",
      "Epoch 01148: val_loss improved from 0.38443 to 0.38436, saving model to DeepFM.h5\n",
      "Epoch 1149/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4502 - auc: 0.826 - ETA: 0s - loss: 0.3723 - auc: 0.741 - 0s 22us/step - loss: 0.3827 - auc: 0.7360 - val_loss: 0.3842 - val_auc: 0.7359\n",
      "\n",
      "Epoch 01149: val_loss improved from 0.38436 to 0.38421, saving model to DeepFM.h5\n",
      "Epoch 1150/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4229 - auc: 0.733 - ETA: 0s - loss: 0.3970 - auc: 0.746 - 0s 23us/step - loss: 0.3793 - auc: 0.7438 - val_loss: 0.3842 - val_auc: 0.7351\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.38421\n",
      "Epoch 1151/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4249 - auc: 0.760 - ETA: 0s - loss: 0.3598 - auc: 0.749 - 0s 22us/step - loss: 0.3758 - auc: 0.7563 - val_loss: 0.3841 - val_auc: 0.7354\n",
      "\n",
      "Epoch 01151: val_loss improved from 0.38421 to 0.38413, saving model to DeepFM.h5\n",
      "Epoch 1152/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3417 - auc: 0.727 - ETA: 0s - loss: 0.3811 - auc: 0.747 - 0s 21us/step - loss: 0.3811 - auc: 0.7378 - val_loss: 0.3840 - val_auc: 0.7363\n",
      "\n",
      "Epoch 01152: val_loss improved from 0.38413 to 0.38399, saving model to DeepFM.h5\n",
      "Epoch 1153/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3991 - auc: 0.722 - ETA: 0s - loss: 0.3721 - auc: 0.750 - 0s 25us/step - loss: 0.3755 - auc: 0.7526 - val_loss: 0.3839 - val_auc: 0.7367\n",
      "\n",
      "Epoch 01153: val_loss improved from 0.38399 to 0.38391, saving model to DeepFM.h5\n",
      "Epoch 1154/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3362 - auc: 0.825 - ETA: 0s - loss: 0.3770 - auc: 0.750 - 0s 20us/step - loss: 0.3769 - auc: 0.7521 - val_loss: 0.3839 - val_auc: 0.7361\n",
      "\n",
      "Epoch 01154: val_loss improved from 0.38391 to 0.38389, saving model to DeepFM.h5\n",
      "Epoch 1155/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3660 - auc: 0.694 - ETA: 0s - loss: 0.3602 - auc: 0.763 - 0s 20us/step - loss: 0.3739 - auc: 0.7566 - val_loss: 0.3839 - val_auc: 0.7359\n",
      "\n",
      "Epoch 01155: val_loss improved from 0.38389 to 0.38386, saving model to DeepFM.h5\n",
      "Epoch 1156/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3826 - auc: 0.784 - ETA: 0s - loss: 0.3848 - auc: 0.747 - 0s 22us/step - loss: 0.3762 - auc: 0.7531 - val_loss: 0.3838 - val_auc: 0.7356\n",
      "\n",
      "Epoch 01156: val_loss improved from 0.38386 to 0.38382, saving model to DeepFM.h5\n",
      "Epoch 1157/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3772 - auc: 0.758 - ETA: 0s - loss: 0.3901 - auc: 0.745 - 0s 21us/step - loss: 0.3787 - auc: 0.7479 - val_loss: 0.3838 - val_auc: 0.7364\n",
      "\n",
      "Epoch 01157: val_loss improved from 0.38382 to 0.38381, saving model to DeepFM.h5\n",
      "Epoch 1158/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3588 - auc: 0.712 - ETA: 0s - loss: 0.3734 - auc: 0.760 - 0s 23us/step - loss: 0.3766 - auc: 0.7500 - val_loss: 0.3838 - val_auc: 0.7377\n",
      "\n",
      "Epoch 01158: val_loss improved from 0.38381 to 0.38380, saving model to DeepFM.h5\n",
      "Epoch 1159/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3740 - auc: 0.842 - ETA: 0s - loss: 0.3734 - auc: 0.767 - 0s 22us/step - loss: 0.3762 - auc: 0.7557 - val_loss: 0.3836 - val_auc: 0.7370\n",
      "\n",
      "Epoch 01159: val_loss improved from 0.38380 to 0.38361, saving model to DeepFM.h5\n",
      "Epoch 1160/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3944 - auc: 0.666 - ETA: 0s - loss: 0.3783 - auc: 0.742 - 0s 22us/step - loss: 0.3763 - auc: 0.7477 - val_loss: 0.3835 - val_auc: 0.7377\n",
      "\n",
      "Epoch 01160: val_loss improved from 0.38361 to 0.38351, saving model to DeepFM.h5\n",
      "Epoch 1161/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4386 - auc: 0.664 - ETA: 0s - loss: 0.3799 - auc: 0.750 - 0s 22us/step - loss: 0.3748 - auc: 0.7559 - val_loss: 0.3834 - val_auc: 0.7385\n",
      "\n",
      "Epoch 01161: val_loss improved from 0.38351 to 0.38342, saving model to DeepFM.h5\n",
      "Epoch 1162/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4452 - auc: 0.678 - ETA: 0s - loss: 0.3773 - auc: 0.764 - 0s 22us/step - loss: 0.3758 - auc: 0.7567 - val_loss: 0.3834 - val_auc: 0.7377\n",
      "\n",
      "Epoch 01162: val_loss improved from 0.38342 to 0.38338, saving model to DeepFM.h5\n",
      "Epoch 1163/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3285 - auc: 0.743 - ETA: 0s - loss: 0.3856 - auc: 0.737 - 0s 22us/step - loss: 0.3799 - auc: 0.7371 - val_loss: 0.3833 - val_auc: 0.7384\n",
      "\n",
      "Epoch 01163: val_loss improved from 0.38338 to 0.38334, saving model to DeepFM.h5\n",
      "Epoch 1164/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3365 - auc: 0.748 - ETA: 0s - loss: 0.3678 - auc: 0.766 - 0s 23us/step - loss: 0.3746 - auc: 0.7581 - val_loss: 0.3833 - val_auc: 0.7386\n",
      "\n",
      "Epoch 01164: val_loss improved from 0.38334 to 0.38326, saving model to DeepFM.h5\n",
      "Epoch 1165/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3675 - auc: 0.742 - ETA: 0s - loss: 0.3826 - auc: 0.744 - 0s 23us/step - loss: 0.3788 - auc: 0.7379 - val_loss: 0.3832 - val_auc: 0.7388\n",
      "\n",
      "Epoch 01165: val_loss improved from 0.38326 to 0.38318, saving model to DeepFM.h5\n",
      "Epoch 1166/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3170 - auc: 0.723 - ETA: 0s - loss: 0.3705 - auc: 0.751 - 0s 21us/step - loss: 0.3777 - auc: 0.7433 - val_loss: 0.3831 - val_auc: 0.7393\n",
      "\n",
      "Epoch 01166: val_loss improved from 0.38318 to 0.38306, saving model to DeepFM.h5\n",
      "Epoch 1167/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3132 - auc: 0.784 - ETA: 0s - loss: 0.3678 - auc: 0.753 - 0s 22us/step - loss: 0.3748 - auc: 0.7471 - val_loss: 0.3830 - val_auc: 0.7383\n",
      "\n",
      "Epoch 01167: val_loss improved from 0.38306 to 0.38297, saving model to DeepFM.h5\n",
      "Epoch 1168/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4308 - auc: 0.729 - ETA: 0s - loss: 0.3854 - auc: 0.751 - 0s 23us/step - loss: 0.3750 - auc: 0.7521 - val_loss: 0.3829 - val_auc: 0.7391\n",
      "\n",
      "Epoch 01168: val_loss improved from 0.38297 to 0.38292, saving model to DeepFM.h5\n",
      "Epoch 1169/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3534 - auc: 0.702 - ETA: 0s - loss: 0.3819 - auc: 0.737 - 0s 21us/step - loss: 0.3798 - auc: 0.7362 - val_loss: 0.3828 - val_auc: 0.7389\n",
      "\n",
      "Epoch 01169: val_loss improved from 0.38292 to 0.38282, saving model to DeepFM.h5\n",
      "Epoch 1170/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3395 - auc: 0.765 - ETA: 0s - loss: 0.3767 - auc: 0.744 - 0s 23us/step - loss: 0.3771 - auc: 0.7491 - val_loss: 0.3828 - val_auc: 0.7397\n",
      "\n",
      "Epoch 01170: val_loss improved from 0.38282 to 0.38280, saving model to DeepFM.h5\n",
      "Epoch 1171/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3729 - auc: 0.820 - ETA: 0s - loss: 0.3912 - auc: 0.767 - 0s 23us/step - loss: 0.3727 - auc: 0.7619 - val_loss: 0.3828 - val_auc: 0.7386\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.38280\n",
      "Epoch 1172/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3430 - auc: 0.822 - ETA: 0s - loss: 0.3840 - auc: 0.744 - 0s 22us/step - loss: 0.3807 - auc: 0.7411 - val_loss: 0.3827 - val_auc: 0.7393\n",
      "\n",
      "Epoch 01172: val_loss improved from 0.38280 to 0.38266, saving model to DeepFM.h5\n",
      "Epoch 1173/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3864 - auc: 0.758 - ETA: 0s - loss: 0.3746 - auc: 0.756 - 0s 21us/step - loss: 0.3730 - auc: 0.7548 - val_loss: 0.3826 - val_auc: 0.7397\n",
      "\n",
      "Epoch 01173: val_loss improved from 0.38266 to 0.38260, saving model to DeepFM.h5\n",
      "Epoch 1174/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3067 - auc: 0.788 - ETA: 0s - loss: 0.3761 - auc: 0.744 - 0s 20us/step - loss: 0.3770 - auc: 0.7486 - val_loss: 0.3825 - val_auc: 0.7400\n",
      "\n",
      "Epoch 01174: val_loss improved from 0.38260 to 0.38252, saving model to DeepFM.h5\n",
      "Epoch 1175/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4341 - auc: 0.764 - ETA: 0s - loss: 0.3728 - auc: 0.760 - 0s 22us/step - loss: 0.3763 - auc: 0.7460 - val_loss: 0.3824 - val_auc: 0.7397\n",
      "\n",
      "Epoch 01175: val_loss improved from 0.38252 to 0.38244, saving model to DeepFM.h5\n",
      "Epoch 1176/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3649 - auc: 0.785 - ETA: 0s - loss: 0.3697 - auc: 0.776 - 0s 23us/step - loss: 0.3733 - auc: 0.7645 - val_loss: 0.3825 - val_auc: 0.7391\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.38244\n",
      "Epoch 1177/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3525 - auc: 0.783 - ETA: 0s - loss: 0.3637 - auc: 0.759 - 0s 22us/step - loss: 0.3751 - auc: 0.7516 - val_loss: 0.3823 - val_auc: 0.7400\n",
      "\n",
      "Epoch 01177: val_loss improved from 0.38244 to 0.38232, saving model to DeepFM.h5\n",
      "Epoch 1178/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3531 - auc: 0.784 - ETA: 0s - loss: 0.3710 - auc: 0.765 - 0s 23us/step - loss: 0.3699 - auc: 0.7689 - val_loss: 0.3823 - val_auc: 0.7389\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.38232\n",
      "Epoch 1179/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4373 - auc: 0.708 - ETA: 0s - loss: 0.3753 - auc: 0.735 - 0s 21us/step - loss: 0.3778 - auc: 0.7424 - val_loss: 0.3822 - val_auc: 0.7394\n",
      "\n",
      "Epoch 01179: val_loss improved from 0.38232 to 0.38221, saving model to DeepFM.h5\n",
      "Epoch 1180/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3378 - auc: 0.774 - ETA: 0s - loss: 0.3668 - auc: 0.757 - 0s 21us/step - loss: 0.3750 - auc: 0.7522 - val_loss: 0.3822 - val_auc: 0.7394\n",
      "\n",
      "Epoch 01180: val_loss improved from 0.38221 to 0.38218, saving model to DeepFM.h5\n",
      "Epoch 1181/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3551 - auc: 0.756 - ETA: 0s - loss: 0.3688 - auc: 0.758 - 0s 22us/step - loss: 0.3746 - auc: 0.7580 - val_loss: 0.3821 - val_auc: 0.7406\n",
      "\n",
      "Epoch 01181: val_loss improved from 0.38218 to 0.38209, saving model to DeepFM.h5\n",
      "Epoch 1182/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3708 - auc: 0.716 - ETA: 0s - loss: 0.3708 - auc: 0.757 - 0s 22us/step - loss: 0.3725 - auc: 0.7584 - val_loss: 0.3821 - val_auc: 0.7398\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.38209\n",
      "Epoch 1183/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3284 - auc: 0.771 - ETA: 0s - loss: 0.3675 - auc: 0.754 - 0s 23us/step - loss: 0.3702 - auc: 0.7649 - val_loss: 0.3819 - val_auc: 0.7404\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01183: val_loss improved from 0.38209 to 0.38194, saving model to DeepFM.h5\n",
      "Epoch 1184/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3850 - auc: 0.770 - ETA: 0s - loss: 0.3673 - auc: 0.754 - 0s 21us/step - loss: 0.3723 - auc: 0.7605 - val_loss: 0.3818 - val_auc: 0.7408\n",
      "\n",
      "Epoch 01184: val_loss improved from 0.38194 to 0.38179, saving model to DeepFM.h5\n",
      "Epoch 1185/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3812 - auc: 0.789 - ETA: 0s - loss: 0.3815 - auc: 0.770 - 0s 22us/step - loss: 0.3704 - auc: 0.7666 - val_loss: 0.3818 - val_auc: 0.7410\n",
      "\n",
      "Epoch 01185: val_loss improved from 0.38179 to 0.38176, saving model to DeepFM.h5\n",
      "Epoch 1186/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3581 - auc: 0.754 - ETA: 0s - loss: 0.3609 - auc: 0.777 - 0s 23us/step - loss: 0.3696 - auc: 0.7672 - val_loss: 0.3816 - val_auc: 0.7407\n",
      "\n",
      "Epoch 01186: val_loss improved from 0.38176 to 0.38164, saving model to DeepFM.h5\n",
      "Epoch 1187/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3230 - auc: 0.823 - ETA: 0s - loss: 0.3749 - auc: 0.762 - 0s 22us/step - loss: 0.3736 - auc: 0.7586 - val_loss: 0.3815 - val_auc: 0.7403\n",
      "\n",
      "Epoch 01187: val_loss improved from 0.38164 to 0.38153, saving model to DeepFM.h5\n",
      "Epoch 1188/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3584 - auc: 0.766 - ETA: 0s - loss: 0.3781 - auc: 0.751 - 0s 23us/step - loss: 0.3737 - auc: 0.7554 - val_loss: 0.3814 - val_auc: 0.7408\n",
      "\n",
      "Epoch 01188: val_loss improved from 0.38153 to 0.38137, saving model to DeepFM.h5\n",
      "Epoch 1189/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3937 - auc: 0.804 - ETA: 0s - loss: 0.3740 - auc: 0.741 - 0s 21us/step - loss: 0.3754 - auc: 0.7504 - val_loss: 0.3813 - val_auc: 0.7399\n",
      "\n",
      "Epoch 01189: val_loss improved from 0.38137 to 0.38127, saving model to DeepFM.h5\n",
      "Epoch 1190/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3886 - auc: 0.669 - ETA: 0s - loss: 0.3802 - auc: 0.736 - 0s 23us/step - loss: 0.3739 - auc: 0.7558 - val_loss: 0.3813 - val_auc: 0.7414\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.38127\n",
      "Epoch 1191/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3940 - auc: 0.809 - ETA: 0s - loss: 0.3766 - auc: 0.752 - 0s 21us/step - loss: 0.3744 - auc: 0.7574 - val_loss: 0.3813 - val_auc: 0.7406\n",
      "\n",
      "Epoch 01191: val_loss improved from 0.38127 to 0.38125, saving model to DeepFM.h5\n",
      "Epoch 1192/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4031 - auc: 0.738 - ETA: 0s - loss: 0.3760 - auc: 0.748 - 0s 23us/step - loss: 0.3750 - auc: 0.7531 - val_loss: 0.3812 - val_auc: 0.7404\n",
      "\n",
      "Epoch 01192: val_loss improved from 0.38125 to 0.38118, saving model to DeepFM.h5\n",
      "Epoch 1193/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.742 - ETA: 0s - loss: 0.3680 - auc: 0.779 - 0s 22us/step - loss: 0.3703 - auc: 0.7629 - val_loss: 0.3812 - val_auc: 0.7413\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.38118\n",
      "Epoch 1194/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3298 - auc: 0.742 - ETA: 0s - loss: 0.3720 - auc: 0.748 - 0s 22us/step - loss: 0.3723 - auc: 0.7613 - val_loss: 0.3811 - val_auc: 0.7418\n",
      "\n",
      "Epoch 01194: val_loss improved from 0.38118 to 0.38113, saving model to DeepFM.h5\n",
      "Epoch 1195/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4222 - auc: 0.762 - ETA: 0s - loss: 0.3600 - auc: 0.765 - 0s 22us/step - loss: 0.3696 - auc: 0.7674 - val_loss: 0.3810 - val_auc: 0.7425\n",
      "\n",
      "Epoch 01195: val_loss improved from 0.38113 to 0.38100, saving model to DeepFM.h5\n",
      "Epoch 1196/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4197 - auc: 0.745 - ETA: 0s - loss: 0.3680 - auc: 0.757 - 0s 21us/step - loss: 0.3697 - auc: 0.7613 - val_loss: 0.3809 - val_auc: 0.7425\n",
      "\n",
      "Epoch 01196: val_loss improved from 0.38100 to 0.38091, saving model to DeepFM.h5\n",
      "Epoch 1197/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4000 - auc: 0.768 - ETA: 0s - loss: 0.3711 - auc: 0.759 - 0s 22us/step - loss: 0.3743 - auc: 0.7491 - val_loss: 0.3809 - val_auc: 0.7420\n",
      "\n",
      "Epoch 01197: val_loss improved from 0.38091 to 0.38087, saving model to DeepFM.h5\n",
      "Epoch 1198/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.775 - ETA: 0s - loss: 0.3774 - auc: 0.759 - 0s 22us/step - loss: 0.3753 - auc: 0.7537 - val_loss: 0.3809 - val_auc: 0.7428\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.38087\n",
      "Epoch 1199/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3517 - auc: 0.734 - ETA: 0s - loss: 0.3666 - auc: 0.752 - 0s 22us/step - loss: 0.3736 - auc: 0.7536 - val_loss: 0.3807 - val_auc: 0.7432\n",
      "\n",
      "Epoch 01199: val_loss improved from 0.38087 to 0.38066, saving model to DeepFM.h5\n",
      "Epoch 1200/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3855 - auc: 0.763 - ETA: 0s - loss: 0.3743 - auc: 0.765 - 0s 22us/step - loss: 0.3720 - auc: 0.7600 - val_loss: 0.3806 - val_auc: 0.7434\n",
      "\n",
      "Epoch 01200: val_loss improved from 0.38066 to 0.38057, saving model to DeepFM.h5\n",
      "Epoch 1201/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3297 - auc: 0.760 - ETA: 0s - loss: 0.3760 - auc: 0.775 - 0s 22us/step - loss: 0.3704 - auc: 0.7691 - val_loss: 0.3805 - val_auc: 0.7431\n",
      "\n",
      "Epoch 01201: val_loss improved from 0.38057 to 0.38053, saving model to DeepFM.h5\n",
      "Epoch 1202/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3635 - auc: 0.784 - ETA: 0s - loss: 0.3636 - auc: 0.778 - 0s 22us/step - loss: 0.3719 - auc: 0.7628 - val_loss: 0.3805 - val_auc: 0.7429\n",
      "\n",
      "Epoch 01202: val_loss improved from 0.38053 to 0.38045, saving model to DeepFM.h5\n",
      "Epoch 1203/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3779 - auc: 0.776 - ETA: 0s - loss: 0.3761 - auc: 0.753 - 0s 21us/step - loss: 0.3726 - auc: 0.7578 - val_loss: 0.3804 - val_auc: 0.7430\n",
      "\n",
      "Epoch 01203: val_loss improved from 0.38045 to 0.38036, saving model to DeepFM.h5\n",
      "Epoch 1204/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3868 - auc: 0.789 - ETA: 0s - loss: 0.3762 - auc: 0.776 - 0s 23us/step - loss: 0.3696 - auc: 0.7725 - val_loss: 0.3802 - val_auc: 0.7432\n",
      "\n",
      "Epoch 01204: val_loss improved from 0.38036 to 0.38023, saving model to DeepFM.h5\n",
      "Epoch 1205/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4282 - auc: 0.734 - ETA: 0s - loss: 0.3678 - auc: 0.763 - 0s 22us/step - loss: 0.3716 - auc: 0.7592 - val_loss: 0.3802 - val_auc: 0.7445\n",
      "\n",
      "Epoch 01205: val_loss improved from 0.38023 to 0.38019, saving model to DeepFM.h5\n",
      "Epoch 1206/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3800 - auc: 0.772 - ETA: 0s - loss: 0.3692 - auc: 0.768 - 0s 22us/step - loss: 0.3710 - auc: 0.7610 - val_loss: 0.3802 - val_auc: 0.7438\n",
      "\n",
      "Epoch 01206: val_loss improved from 0.38019 to 0.38015, saving model to DeepFM.h5\n",
      "Epoch 1207/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3663 - auc: 0.674 - ETA: 0s - loss: 0.3654 - auc: 0.755 - 0s 21us/step - loss: 0.3717 - auc: 0.7583 - val_loss: 0.3801 - val_auc: 0.7439\n",
      "\n",
      "Epoch 01207: val_loss improved from 0.38015 to 0.38009, saving model to DeepFM.h5\n",
      "Epoch 1208/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3893 - auc: 0.771 - ETA: 0s - loss: 0.3833 - auc: 0.763 - 0s 21us/step - loss: 0.3711 - auc: 0.7688 - val_loss: 0.3801 - val_auc: 0.7442\n",
      "\n",
      "Epoch 01208: val_loss improved from 0.38009 to 0.38008, saving model to DeepFM.h5\n",
      "Epoch 1209/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3303 - auc: 0.832 - ETA: 0s - loss: 0.3673 - auc: 0.770 - 0s 23us/step - loss: 0.3683 - auc: 0.7742 - val_loss: 0.3799 - val_auc: 0.7444\n",
      "\n",
      "Epoch 01209: val_loss improved from 0.38008 to 0.37995, saving model to DeepFM.h5\n",
      "Epoch 1210/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3715 - auc: 0.735 - ETA: 0s - loss: 0.3763 - auc: 0.741 - 0s 23us/step - loss: 0.3735 - auc: 0.7528 - val_loss: 0.3798 - val_auc: 0.7440\n",
      "\n",
      "Epoch 01210: val_loss improved from 0.37995 to 0.37979, saving model to DeepFM.h5\n",
      "Epoch 1211/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4394 - auc: 0.775 - ETA: 0s - loss: 0.3797 - auc: 0.762 - 0s 22us/step - loss: 0.3725 - auc: 0.7566 - val_loss: 0.3798 - val_auc: 0.7456\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.37979\n",
      "Epoch 1212/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3463 - auc: 0.778 - ETA: 0s - loss: 0.3674 - auc: 0.770 - 0s 23us/step - loss: 0.3701 - auc: 0.7662 - val_loss: 0.3797 - val_auc: 0.7449\n",
      "\n",
      "Epoch 01212: val_loss improved from 0.37979 to 0.37967, saving model to DeepFM.h5\n",
      "Epoch 1213/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3912 - auc: 0.787 - ETA: 0s - loss: 0.3675 - auc: 0.754 - 0s 21us/step - loss: 0.3703 - auc: 0.7609 - val_loss: 0.3795 - val_auc: 0.7439\n",
      "\n",
      "Epoch 01213: val_loss improved from 0.37967 to 0.37951, saving model to DeepFM.h5\n",
      "Epoch 1214/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3468 - auc: 0.727 - ETA: 0s - loss: 0.3721 - auc: 0.764 - 0s 21us/step - loss: 0.3699 - auc: 0.7618 - val_loss: 0.3795 - val_auc: 0.7460\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.37951\n",
      "Epoch 1215/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3675 - auc: 0.719 - ETA: 0s - loss: 0.3776 - auc: 0.750 - 0s 22us/step - loss: 0.3710 - auc: 0.7587 - val_loss: 0.3795 - val_auc: 0.7455\n",
      "\n",
      "Epoch 01215: val_loss improved from 0.37951 to 0.37947, saving model to DeepFM.h5\n",
      "Epoch 1216/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4006 - auc: 0.739 - ETA: 0s - loss: 0.3751 - auc: 0.747 - 0s 22us/step - loss: 0.3664 - auc: 0.7753 - val_loss: 0.3794 - val_auc: 0.7463\n",
      "\n",
      "Epoch 01216: val_loss improved from 0.37947 to 0.37939, saving model to DeepFM.h5\n",
      "Epoch 1217/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3922 - auc: 0.796 - ETA: 0s - loss: 0.3694 - auc: 0.776 - 0s 23us/step - loss: 0.3691 - auc: 0.7673 - val_loss: 0.3793 - val_auc: 0.7463\n",
      "\n",
      "Epoch 01217: val_loss improved from 0.37939 to 0.37930, saving model to DeepFM.h5\n",
      "Epoch 1218/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3415 - auc: 0.832 - ETA: 0s - loss: 0.3604 - auc: 0.769 - 0s 23us/step - loss: 0.3697 - auc: 0.7624 - val_loss: 0.3792 - val_auc: 0.7460\n",
      "\n",
      "Epoch 01218: val_loss improved from 0.37930 to 0.37922, saving model to DeepFM.h5\n",
      "Epoch 1219/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3670 - auc: 0.711 - ETA: 0s - loss: 0.3618 - auc: 0.768 - 0s 21us/step - loss: 0.3677 - auc: 0.7681 - val_loss: 0.3791 - val_auc: 0.7468\n",
      "\n",
      "Epoch 01219: val_loss improved from 0.37922 to 0.37909, saving model to DeepFM.h5\n",
      "Epoch 1220/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4352 - auc: 0.740 - ETA: 0s - loss: 0.3676 - auc: 0.766 - 0s 22us/step - loss: 0.3709 - auc: 0.7649 - val_loss: 0.3790 - val_auc: 0.7465\n",
      "\n",
      "Epoch 01220: val_loss improved from 0.37909 to 0.37903, saving model to DeepFM.h5\n",
      "Epoch 1221/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3906 - auc: 0.804 - ETA: 0s - loss: 0.3797 - auc: 0.763 - 0s 23us/step - loss: 0.3671 - auc: 0.7712 - val_loss: 0.3790 - val_auc: 0.7472\n",
      "\n",
      "Epoch 01221: val_loss improved from 0.37903 to 0.37896, saving model to DeepFM.h5\n",
      "Epoch 1222/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3756 - auc: 0.765 - ETA: 0s - loss: 0.3792 - auc: 0.769 - 0s 21us/step - loss: 0.3700 - auc: 0.7662 - val_loss: 0.3790 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.37896\n",
      "Epoch 1223/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3330 - auc: 0.820 - ETA: 0s - loss: 0.3665 - auc: 0.759 - 0s 21us/step - loss: 0.3710 - auc: 0.7566 - val_loss: 0.3787 - val_auc: 0.7463\n",
      "\n",
      "Epoch 01223: val_loss improved from 0.37896 to 0.37873, saving model to DeepFM.h5\n",
      "Epoch 1224/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3919 - auc: 0.788 - ETA: 0s - loss: 0.3615 - auc: 0.766 - 0s 23us/step - loss: 0.3729 - auc: 0.7577 - val_loss: 0.3787 - val_auc: 0.7461\n",
      "\n",
      "Epoch 01224: val_loss improved from 0.37873 to 0.37867, saving model to DeepFM.h5\n",
      "Epoch 1225/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3502 - auc: 0.773 - ETA: 0s - loss: 0.3735 - auc: 0.771 - 0s 22us/step - loss: 0.3709 - auc: 0.7576 - val_loss: 0.3787 - val_auc: 0.7466\n",
      "\n",
      "Epoch 01225: val_loss improved from 0.37867 to 0.37866, saving model to DeepFM.h5\n",
      "Epoch 1226/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2847 - auc: 0.789 - ETA: 0s - loss: 0.3637 - auc: 0.777 - 0s 22us/step - loss: 0.3655 - auc: 0.7783 - val_loss: 0.3786 - val_auc: 0.7467\n",
      "\n",
      "Epoch 01226: val_loss improved from 0.37866 to 0.37860, saving model to DeepFM.h5\n",
      "Epoch 1227/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2849 - auc: 0.731 - ETA: 0s - loss: 0.3685 - auc: 0.766 - 0s 22us/step - loss: 0.3680 - auc: 0.7715 - val_loss: 0.3785 - val_auc: 0.7467\n",
      "\n",
      "Epoch 01227: val_loss improved from 0.37860 to 0.37852, saving model to DeepFM.h5\n",
      "Epoch 1228/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3951 - auc: 0.777 - ETA: 0s - loss: 0.3689 - auc: 0.762 - 0s 22us/step - loss: 0.3715 - auc: 0.7614 - val_loss: 0.3783 - val_auc: 0.7476\n",
      "\n",
      "Epoch 01228: val_loss improved from 0.37852 to 0.37835, saving model to DeepFM.h5\n",
      "Epoch 1229/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3851 - auc: 0.761 - ETA: 0s - loss: 0.3648 - auc: 0.768 - 0s 21us/step - loss: 0.3698 - auc: 0.7655 - val_loss: 0.3783 - val_auc: 0.7472\n",
      "\n",
      "Epoch 01229: val_loss improved from 0.37835 to 0.37830, saving model to DeepFM.h5\n",
      "Epoch 1230/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3090 - auc: 0.733 - ETA: 0s - loss: 0.3706 - auc: 0.765 - 0s 22us/step - loss: 0.3679 - auc: 0.7671 - val_loss: 0.3782 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01230: val_loss improved from 0.37830 to 0.37824, saving model to DeepFM.h5\n",
      "Epoch 1231/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3412 - auc: 0.803 - ETA: 0s - loss: 0.3619 - auc: 0.762 - 0s 22us/step - loss: 0.3690 - auc: 0.7682 - val_loss: 0.3781 - val_auc: 0.7484\n",
      "\n",
      "Epoch 01231: val_loss improved from 0.37824 to 0.37809, saving model to DeepFM.h5\n",
      "Epoch 1232/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3398 - auc: 0.789 - ETA: 0s - loss: 0.3681 - auc: 0.759 - 0s 21us/step - loss: 0.3670 - auc: 0.7693 - val_loss: 0.3780 - val_auc: 0.7476\n",
      "\n",
      "Epoch 01232: val_loss improved from 0.37809 to 0.37804, saving model to DeepFM.h5\n",
      "Epoch 1233/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3383 - auc: 0.720 - ETA: 0s - loss: 0.3741 - auc: 0.763 - 0s 25us/step - loss: 0.3661 - auc: 0.7726 - val_loss: 0.3781 - val_auc: 0.7487\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.37804\n",
      "Epoch 1234/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4271 - auc: 0.710 - ETA: 0s - loss: 0.3673 - auc: 0.767 - 0s 23us/step - loss: 0.3692 - auc: 0.7674 - val_loss: 0.3781 - val_auc: 0.7492\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.37804\n",
      "Epoch 1235/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3681 - auc: 0.807 - ETA: 0s - loss: 0.3598 - auc: 0.779 - 0s 22us/step - loss: 0.3652 - auc: 0.7757 - val_loss: 0.3779 - val_auc: 0.7489\n",
      "\n",
      "Epoch 01235: val_loss improved from 0.37804 to 0.37787, saving model to DeepFM.h5\n",
      "Epoch 1236/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3276 - auc: 0.810 - ETA: 0s - loss: 0.3668 - auc: 0.768 - 0s 22us/step - loss: 0.3681 - auc: 0.7714 - val_loss: 0.3777 - val_auc: 0.7503\n",
      "\n",
      "Epoch 01236: val_loss improved from 0.37787 to 0.37771, saving model to DeepFM.h5\n",
      "Epoch 1237/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3518 - auc: 0.799 - ETA: 0s - loss: 0.3626 - auc: 0.772 - 0s 26us/step - loss: 0.3616 - auc: 0.7827 - val_loss: 0.3778 - val_auc: 0.7495\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.37771\n",
      "Epoch 1238/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3648 - auc: 0.822 - ETA: 0s - loss: 0.3662 - auc: 0.765 - 0s 24us/step - loss: 0.3664 - auc: 0.7758 - val_loss: 0.3775 - val_auc: 0.7500\n",
      "\n",
      "Epoch 01238: val_loss improved from 0.37771 to 0.37753, saving model to DeepFM.h5\n",
      "Epoch 1239/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.742 - ETA: 0s - loss: 0.3703 - auc: 0.758 - 0s 22us/step - loss: 0.3722 - auc: 0.7659 - val_loss: 0.3775 - val_auc: 0.7504\n",
      "\n",
      "Epoch 01239: val_loss improved from 0.37753 to 0.37746, saving model to DeepFM.h5\n",
      "Epoch 1240/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3173 - auc: 0.773 - ETA: 0s - loss: 0.3611 - auc: 0.768 - 0s 21us/step - loss: 0.3683 - auc: 0.7633 - val_loss: 0.3773 - val_auc: 0.7500\n",
      "\n",
      "Epoch 01240: val_loss improved from 0.37746 to 0.37734, saving model to DeepFM.h5\n",
      "Epoch 1241/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3488 - auc: 0.781 - ETA: 0s - loss: 0.3715 - auc: 0.753 - 0s 23us/step - loss: 0.3717 - auc: 0.7589 - val_loss: 0.3774 - val_auc: 0.7513\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.37734\n",
      "Epoch 1242/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3733 - auc: 0.799 - ETA: 0s - loss: 0.3669 - auc: 0.771 - 0s 22us/step - loss: 0.3704 - auc: 0.7627 - val_loss: 0.3772 - val_auc: 0.7504\n",
      "\n",
      "Epoch 01242: val_loss improved from 0.37734 to 0.37721, saving model to DeepFM.h5\n",
      "Epoch 1243/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3403 - auc: 0.716 - ETA: 0s - loss: 0.3604 - auc: 0.770 - 0s 25us/step - loss: 0.3713 - auc: 0.7601 - val_loss: 0.3771 - val_auc: 0.7509\n",
      "\n",
      "Epoch 01243: val_loss improved from 0.37721 to 0.37712, saving model to DeepFM.h5\n",
      "Epoch 1244/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3451 - auc: 0.850 - ETA: 0s - loss: 0.3644 - auc: 0.769 - 0s 22us/step - loss: 0.3716 - auc: 0.7560 - val_loss: 0.3770 - val_auc: 0.7502\n",
      "\n",
      "Epoch 01244: val_loss improved from 0.37712 to 0.37704, saving model to DeepFM.h5\n",
      "Epoch 1245/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.774 - ETA: 0s - loss: 0.3679 - auc: 0.771 - 0s 21us/step - loss: 0.3648 - auc: 0.7704 - val_loss: 0.3770 - val_auc: 0.7515\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.37704\n",
      "Epoch 1246/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3892 - auc: 0.718 - ETA: 0s - loss: 0.3702 - auc: 0.765 - 0s 21us/step - loss: 0.3699 - auc: 0.7582 - val_loss: 0.3770 - val_auc: 0.7518\n",
      "\n",
      "Epoch 01246: val_loss improved from 0.37704 to 0.37697, saving model to DeepFM.h5\n",
      "Epoch 1247/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3579 - auc: 0.685 - ETA: 0s - loss: 0.3664 - auc: 0.768 - 0s 23us/step - loss: 0.3678 - auc: 0.7664 - val_loss: 0.3770 - val_auc: 0.7519\n",
      "\n",
      "Epoch 01247: val_loss improved from 0.37697 to 0.37696, saving model to DeepFM.h5\n",
      "Epoch 1248/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3633 - auc: 0.760 - ETA: 0s - loss: 0.3730 - auc: 0.756 - 0s 21us/step - loss: 0.3699 - auc: 0.7648 - val_loss: 0.3769 - val_auc: 0.7517\n",
      "\n",
      "Epoch 01248: val_loss improved from 0.37696 to 0.37694, saving model to DeepFM.h5\n",
      "Epoch 1249/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3792 - auc: 0.744 - ETA: 0s - loss: 0.3795 - auc: 0.740 - 0s 24us/step - loss: 0.3682 - auc: 0.7647 - val_loss: 0.3769 - val_auc: 0.7516\n",
      "\n",
      "Epoch 01249: val_loss improved from 0.37694 to 0.37690, saving model to DeepFM.h5\n",
      "Epoch 1250/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3530 - auc: 0.801 - ETA: 0s - loss: 0.3580 - auc: 0.769 - 0s 22us/step - loss: 0.3661 - auc: 0.7653 - val_loss: 0.3768 - val_auc: 0.7525\n",
      "\n",
      "Epoch 01250: val_loss improved from 0.37690 to 0.37682, saving model to DeepFM.h5\n",
      "Epoch 1251/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4678 - auc: 0.676 - ETA: 0s - loss: 0.3666 - auc: 0.766 - 0s 21us/step - loss: 0.3650 - auc: 0.7711 - val_loss: 0.3768 - val_auc: 0.7522\n",
      "\n",
      "Epoch 01251: val_loss improved from 0.37682 to 0.37677, saving model to DeepFM.h5\n",
      "Epoch 1252/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.744 - ETA: 0s - loss: 0.3714 - auc: 0.758 - 0s 23us/step - loss: 0.3688 - auc: 0.7594 - val_loss: 0.3767 - val_auc: 0.7525\n",
      "\n",
      "Epoch 01252: val_loss improved from 0.37677 to 0.37667, saving model to DeepFM.h5\n",
      "Epoch 1253/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3528 - auc: 0.827 - ETA: 0s - loss: 0.3689 - auc: 0.767 - 0s 21us/step - loss: 0.3660 - auc: 0.7738 - val_loss: 0.3766 - val_auc: 0.7524\n",
      "\n",
      "Epoch 01253: val_loss improved from 0.37667 to 0.37657, saving model to DeepFM.h5\n",
      "Epoch 1254/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3524 - auc: 0.791 - ETA: 0s - loss: 0.3663 - auc: 0.766 - 0s 23us/step - loss: 0.3664 - auc: 0.7701 - val_loss: 0.3765 - val_auc: 0.7517\n",
      "\n",
      "Epoch 01254: val_loss improved from 0.37657 to 0.37651, saving model to DeepFM.h5\n",
      "Epoch 1255/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3544 - auc: 0.780 - ETA: 0s - loss: 0.3654 - auc: 0.771 - 0s 22us/step - loss: 0.3652 - auc: 0.7742 - val_loss: 0.3765 - val_auc: 0.7519\n",
      "\n",
      "Epoch 01255: val_loss improved from 0.37651 to 0.37648, saving model to DeepFM.h5\n",
      "Epoch 1256/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3807 - auc: 0.743 - ETA: 0s - loss: 0.3682 - auc: 0.753 - 0s 22us/step - loss: 0.3738 - auc: 0.7526 - val_loss: 0.3763 - val_auc: 0.7537\n",
      "\n",
      "Epoch 01256: val_loss improved from 0.37648 to 0.37628, saving model to DeepFM.h5\n",
      "Epoch 1257/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3479 - auc: 0.754 - ETA: 0s - loss: 0.3589 - auc: 0.759 - 0s 24us/step - loss: 0.3674 - auc: 0.7694 - val_loss: 0.3761 - val_auc: 0.7524\n",
      "\n",
      "Epoch 01257: val_loss improved from 0.37628 to 0.37609, saving model to DeepFM.h5\n",
      "Epoch 1258/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3670 - auc: 0.728 - ETA: 0s - loss: 0.3688 - auc: 0.764 - 0s 21us/step - loss: 0.3674 - auc: 0.7622 - val_loss: 0.3761 - val_auc: 0.7533\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.37609\n",
      "Epoch 1259/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3416 - auc: 0.765 - ETA: 0s - loss: 0.3696 - auc: 0.768 - 0s 23us/step - loss: 0.3641 - auc: 0.7783 - val_loss: 0.3762 - val_auc: 0.7525\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.37609\n",
      "Epoch 1260/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3619 - auc: 0.750 - ETA: 0s - loss: 0.3586 - auc: 0.773 - 0s 22us/step - loss: 0.3675 - auc: 0.7683 - val_loss: 0.3761 - val_auc: 0.7538\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.37609\n",
      "Epoch 1261/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3765 - auc: 0.791 - ETA: 0s - loss: 0.3701 - auc: 0.765 - 0s 22us/step - loss: 0.3657 - auc: 0.7696 - val_loss: 0.3761 - val_auc: 0.7537\n",
      "\n",
      "Epoch 01261: val_loss improved from 0.37609 to 0.37606, saving model to DeepFM.h5\n",
      "Epoch 1262/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3727 - auc: 0.764 - ETA: 0s - loss: 0.3637 - auc: 0.796 - 0s 23us/step - loss: 0.3642 - auc: 0.7732 - val_loss: 0.3761 - val_auc: 0.7526\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.37606\n",
      "Epoch 1263/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3244 - auc: 0.714 - ETA: 0s - loss: 0.3773 - auc: 0.768 - 0s 21us/step - loss: 0.3680 - auc: 0.7669 - val_loss: 0.3760 - val_auc: 0.7534\n",
      "\n",
      "Epoch 01263: val_loss improved from 0.37606 to 0.37601, saving model to DeepFM.h5\n",
      "Epoch 1264/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3356 - auc: 0.738 - ETA: 0s - loss: 0.3666 - auc: 0.762 - 0s 23us/step - loss: 0.3639 - auc: 0.7761 - val_loss: 0.3759 - val_auc: 0.7541\n",
      "\n",
      "Epoch 01264: val_loss improved from 0.37601 to 0.37592, saving model to DeepFM.h5\n",
      "Epoch 1265/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3574 - auc: 0.655 - ETA: 0s - loss: 0.3638 - auc: 0.766 - 0s 21us/step - loss: 0.3654 - auc: 0.7736 - val_loss: 0.3758 - val_auc: 0.7543\n",
      "\n",
      "Epoch 01265: val_loss improved from 0.37592 to 0.37581, saving model to DeepFM.h5\n",
      "Epoch 1266/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3942 - auc: 0.760 - ETA: 0s - loss: 0.3773 - auc: 0.752 - 0s 22us/step - loss: 0.3694 - auc: 0.7611 - val_loss: 0.3757 - val_auc: 0.7547\n",
      "\n",
      "Epoch 01266: val_loss improved from 0.37581 to 0.37572, saving model to DeepFM.h5\n",
      "Epoch 1267/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3307 - auc: 0.701 - ETA: 0s - loss: 0.3602 - auc: 0.784 - 0s 22us/step - loss: 0.3678 - auc: 0.7696 - val_loss: 0.3756 - val_auc: 0.7541\n",
      "\n",
      "Epoch 01267: val_loss improved from 0.37572 to 0.37558, saving model to DeepFM.h5\n",
      "Epoch 1268/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3806 - auc: 0.773 - ETA: 0s - loss: 0.3626 - auc: 0.775 - 0s 22us/step - loss: 0.3650 - auc: 0.7775 - val_loss: 0.3756 - val_auc: 0.7555\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.37558\n",
      "Epoch 1269/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4337 - auc: 0.688 - ETA: 0s - loss: 0.3595 - auc: 0.777 - 0s 21us/step - loss: 0.3648 - auc: 0.7744 - val_loss: 0.3755 - val_auc: 0.7547\n",
      "\n",
      "Epoch 01269: val_loss improved from 0.37558 to 0.37546, saving model to DeepFM.h5\n",
      "Epoch 1270/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3735 - auc: 0.697 - ETA: 0s - loss: 0.3529 - auc: 0.779 - 0s 21us/step - loss: 0.3631 - auc: 0.7802 - val_loss: 0.3755 - val_auc: 0.7549\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 0.37546\n",
      "Epoch 1271/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3607 - auc: 0.832 - ETA: 0s - loss: 0.3526 - auc: 0.782 - 0s 22us/step - loss: 0.3614 - auc: 0.7849 - val_loss: 0.3754 - val_auc: 0.7557\n",
      "\n",
      "Epoch 01271: val_loss improved from 0.37546 to 0.37538, saving model to DeepFM.h5\n",
      "Epoch 1272/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3511 - auc: 0.836 - ETA: 0s - loss: 0.3610 - auc: 0.777 - 0s 20us/step - loss: 0.3652 - auc: 0.7747 - val_loss: 0.3754 - val_auc: 0.7546\n",
      "\n",
      "Epoch 01272: val_loss improved from 0.37538 to 0.37537, saving model to DeepFM.h5\n",
      "Epoch 1273/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3421 - auc: 0.759 - ETA: 0s - loss: 0.3736 - auc: 0.781 - 0s 22us/step - loss: 0.3642 - auc: 0.7812 - val_loss: 0.3753 - val_auc: 0.7530\n",
      "\n",
      "Epoch 01273: val_loss improved from 0.37537 to 0.37533, saving model to DeepFM.h5\n",
      "Epoch 1274/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5019 - auc: 0.782 - ETA: 0s - loss: 0.3852 - auc: 0.773 - 0s 23us/step - loss: 0.3668 - auc: 0.7705 - val_loss: 0.3754 - val_auc: 0.7532\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.37533\n",
      "Epoch 1275/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3410 - auc: 0.776 - ETA: 0s - loss: 0.3643 - auc: 0.779 - 0s 22us/step - loss: 0.3614 - auc: 0.7806 - val_loss: 0.3752 - val_auc: 0.7526\n",
      "\n",
      "Epoch 01275: val_loss improved from 0.37533 to 0.37522, saving model to DeepFM.h5\n",
      "Epoch 1276/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4139 - auc: 0.753 - ETA: 0s - loss: 0.3578 - auc: 0.785 - 0s 24us/step - loss: 0.3596 - auc: 0.7832 - val_loss: 0.3752 - val_auc: 0.7527\n",
      "\n",
      "Epoch 01276: val_loss improved from 0.37522 to 0.37517, saving model to DeepFM.h5\n",
      "Epoch 1277/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4307 - auc: 0.704 - ETA: 0s - loss: 0.3625 - auc: 0.795 - 0s 24us/step - loss: 0.3573 - auc: 0.7915 - val_loss: 0.3752 - val_auc: 0.7530\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 0.37517\n",
      "Epoch 1278/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3531 - auc: 0.749 - ETA: 0s - loss: 0.3554 - auc: 0.791 - 0s 22us/step - loss: 0.3613 - auc: 0.7875 - val_loss: 0.3750 - val_auc: 0.7533\n",
      "\n",
      "Epoch 01278: val_loss improved from 0.37517 to 0.37499, saving model to DeepFM.h5\n",
      "Epoch 1279/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4002 - auc: 0.796 - ETA: 0s - loss: 0.3626 - auc: 0.794 - 0s 20us/step - loss: 0.3608 - auc: 0.7883 - val_loss: 0.3749 - val_auc: 0.7533\n",
      "\n",
      "Epoch 01279: val_loss improved from 0.37499 to 0.37494, saving model to DeepFM.h5\n",
      "Epoch 1280/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.701 - ETA: 0s - loss: 0.3651 - auc: 0.768 - 0s 23us/step - loss: 0.3623 - auc: 0.7753 - val_loss: 0.3748 - val_auc: 0.7535\n",
      "\n",
      "Epoch 01280: val_loss improved from 0.37494 to 0.37478, saving model to DeepFM.h5\n",
      "Epoch 1281/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3530 - auc: 0.794 - ETA: 0s - loss: 0.3753 - auc: 0.766 - 0s 22us/step - loss: 0.3638 - auc: 0.7750 - val_loss: 0.3748 - val_auc: 0.7534\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 0.37478\n",
      "Epoch 1282/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3771 - auc: 0.827 - ETA: 0s - loss: 0.3510 - auc: 0.782 - 0s 25us/step - loss: 0.3641 - auc: 0.7773 - val_loss: 0.3746 - val_auc: 0.7535\n",
      "\n",
      "Epoch 01282: val_loss improved from 0.37478 to 0.37462, saving model to DeepFM.h5\n",
      "Epoch 1283/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3831 - auc: 0.775 - ETA: 0s - loss: 0.3572 - auc: 0.794 - 0s 24us/step - loss: 0.3615 - auc: 0.7824 - val_loss: 0.3747 - val_auc: 0.7535\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.37462\n",
      "Epoch 1284/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3446 - auc: 0.830 - ETA: 0s - loss: 0.3621 - auc: 0.767 - 0s 23us/step - loss: 0.3655 - auc: 0.7701 - val_loss: 0.3745 - val_auc: 0.7527\n",
      "\n",
      "Epoch 01284: val_loss improved from 0.37462 to 0.37451, saving model to DeepFM.h5\n",
      "Epoch 1285/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3649 - auc: 0.775 - ETA: 0s - loss: 0.3780 - auc: 0.776 - 0s 24us/step - loss: 0.3640 - auc: 0.7743 - val_loss: 0.3746 - val_auc: 0.7550\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.37451\n",
      "Epoch 1286/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3110 - auc: 0.796 - ETA: 0s - loss: 0.3650 - auc: 0.766 - 0s 24us/step - loss: 0.3633 - auc: 0.7793 - val_loss: 0.3745 - val_auc: 0.7533\n",
      "\n",
      "Epoch 01286: val_loss improved from 0.37451 to 0.37447, saving model to DeepFM.h5\n",
      "Epoch 1287/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3485 - auc: 0.815 - ETA: 0s - loss: 0.3544 - auc: 0.764 - 0s 25us/step - loss: 0.3662 - auc: 0.7693 - val_loss: 0.3743 - val_auc: 0.7553\n",
      "\n",
      "Epoch 01287: val_loss improved from 0.37447 to 0.37427, saving model to DeepFM.h5\n",
      "Epoch 1288/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3851 - auc: 0.806 - ETA: 0s - loss: 0.3568 - auc: 0.788 - 0s 24us/step - loss: 0.3626 - auc: 0.7828 - val_loss: 0.3742 - val_auc: 0.7542\n",
      "\n",
      "Epoch 01288: val_loss improved from 0.37427 to 0.37422, saving model to DeepFM.h5\n",
      "Epoch 1289/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3564 - auc: 0.808 - ETA: 0s - loss: 0.3721 - auc: 0.780 - 0s 24us/step - loss: 0.3635 - auc: 0.7775 - val_loss: 0.3743 - val_auc: 0.7559\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.37422\n",
      "Epoch 1290/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3358 - auc: 0.814 - ETA: 0s - loss: 0.3705 - auc: 0.765 - 0s 22us/step - loss: 0.3653 - auc: 0.7713 - val_loss: 0.3743 - val_auc: 0.7559\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.37422\n",
      "Epoch 1291/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3996 - auc: 0.756 - ETA: 0s - loss: 0.3628 - auc: 0.780 - 0s 24us/step - loss: 0.3618 - auc: 0.7782 - val_loss: 0.3741 - val_auc: 0.7546\n",
      "\n",
      "Epoch 01291: val_loss improved from 0.37422 to 0.37414, saving model to DeepFM.h5\n",
      "Epoch 1292/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3362 - auc: 0.768 - ETA: 0s - loss: 0.3617 - auc: 0.785 - 0s 23us/step - loss: 0.3597 - auc: 0.7842 - val_loss: 0.3743 - val_auc: 0.7563\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.37414\n",
      "Epoch 1293/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3531 - auc: 0.849 - ETA: 0s - loss: 0.3669 - auc: 0.775 - 0s 22us/step - loss: 0.3629 - auc: 0.7776 - val_loss: 0.3741 - val_auc: 0.7564\n",
      "\n",
      "Epoch 01293: val_loss improved from 0.37414 to 0.37406, saving model to DeepFM.h5\n",
      "Epoch 1294/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3799 - auc: 0.737 - ETA: 0s - loss: 0.3544 - auc: 0.772 - 0s 23us/step - loss: 0.3640 - auc: 0.7739 - val_loss: 0.3738 - val_auc: 0.7546\n",
      "\n",
      "Epoch 01294: val_loss improved from 0.37406 to 0.37381, saving model to DeepFM.h5\n",
      "Epoch 1295/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3497 - auc: 0.732 - ETA: 0s - loss: 0.3674 - auc: 0.760 - 0s 21us/step - loss: 0.3657 - auc: 0.7625 - val_loss: 0.3738 - val_auc: 0.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01295: val_loss did not improve from 0.37381\n",
      "Epoch 1296/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4019 - auc: 0.755 - ETA: 0s - loss: 0.3552 - auc: 0.783 - 0s 22us/step - loss: 0.3631 - auc: 0.7742 - val_loss: 0.3738 - val_auc: 0.7558\n",
      "\n",
      "Epoch 01296: val_loss improved from 0.37381 to 0.37379, saving model to DeepFM.h5\n",
      "Epoch 1297/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2957 - auc: 0.890 - ETA: 0s - loss: 0.3650 - auc: 0.780 - 0s 21us/step - loss: 0.3654 - auc: 0.7681 - val_loss: 0.3738 - val_auc: 0.7555\n",
      "\n",
      "Epoch 01297: val_loss improved from 0.37379 to 0.37377, saving model to DeepFM.h5\n",
      "Epoch 1298/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3719 - auc: 0.712 - ETA: 0s - loss: 0.3696 - auc: 0.787 - 0s 23us/step - loss: 0.3610 - auc: 0.7779 - val_loss: 0.3738 - val_auc: 0.7566\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.37377\n",
      "Epoch 1299/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3812 - auc: 0.820 - ETA: 0s - loss: 0.3641 - auc: 0.778 - 0s 24us/step - loss: 0.3604 - auc: 0.7844 - val_loss: 0.3737 - val_auc: 0.7559\n",
      "\n",
      "Epoch 01299: val_loss improved from 0.37377 to 0.37368, saving model to DeepFM.h5\n",
      "Epoch 1300/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4098 - auc: 0.743 - ETA: 0s - loss: 0.3580 - auc: 0.781 - 0s 23us/step - loss: 0.3587 - auc: 0.7835 - val_loss: 0.3738 - val_auc: 0.7559\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.37368\n",
      "Epoch 1301/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3393 - auc: 0.838 - ETA: 0s - loss: 0.3688 - auc: 0.771 - 0s 21us/step - loss: 0.3600 - auc: 0.7836 - val_loss: 0.3736 - val_auc: 0.7571\n",
      "\n",
      "Epoch 01301: val_loss improved from 0.37368 to 0.37357, saving model to DeepFM.h5\n",
      "Epoch 1302/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3558 - auc: 0.758 - ETA: 0s - loss: 0.3686 - auc: 0.765 - 0s 25us/step - loss: 0.3643 - auc: 0.7735 - val_loss: 0.3736 - val_auc: 0.7572\n",
      "\n",
      "Epoch 01302: val_loss improved from 0.37357 to 0.37355, saving model to DeepFM.h5\n",
      "Epoch 1303/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3261 - auc: 0.793 - ETA: 0s - loss: 0.3551 - auc: 0.790 - 0s 23us/step - loss: 0.3594 - auc: 0.7851 - val_loss: 0.3734 - val_auc: 0.7550\n",
      "\n",
      "Epoch 01303: val_loss improved from 0.37355 to 0.37338, saving model to DeepFM.h5\n",
      "Epoch 1304/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4306 - auc: 0.757 - ETA: 0s - loss: 0.3591 - auc: 0.784 - 0s 25us/step - loss: 0.3573 - auc: 0.7933 - val_loss: 0.3733 - val_auc: 0.7563\n",
      "\n",
      "Epoch 01304: val_loss improved from 0.37338 to 0.37334, saving model to DeepFM.h5\n",
      "Epoch 1305/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3105 - auc: 0.753 - ETA: 0s - loss: 0.3716 - auc: 0.792 - 0s 24us/step - loss: 0.3538 - auc: 0.7983 - val_loss: 0.3737 - val_auc: 0.7560\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.37334\n",
      "Epoch 1306/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3838 - auc: 0.748 - ETA: 0s - loss: 0.3720 - auc: 0.771 - 0s 27us/step - loss: 0.3599 - auc: 0.7818 - val_loss: 0.3735 - val_auc: 0.7568\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.37334\n",
      "Epoch 1307/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3910 - auc: 0.786 - ETA: 0s - loss: 0.3522 - auc: 0.785 - 0s 23us/step - loss: 0.3611 - auc: 0.7819 - val_loss: 0.3733 - val_auc: 0.7581\n",
      "\n",
      "Epoch 01307: val_loss improved from 0.37334 to 0.37330, saving model to DeepFM.h5\n",
      "Epoch 1308/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3736 - auc: 0.780 - ETA: 0s - loss: 0.3603 - auc: 0.775 - 0s 23us/step - loss: 0.3568 - auc: 0.7879 - val_loss: 0.3731 - val_auc: 0.7573\n",
      "\n",
      "Epoch 01308: val_loss improved from 0.37330 to 0.37310, saving model to DeepFM.h5\n",
      "Epoch 1309/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4046 - auc: 0.779 - ETA: 0s - loss: 0.3790 - auc: 0.773 - 0s 23us/step - loss: 0.3638 - auc: 0.7764 - val_loss: 0.3730 - val_auc: 0.7573\n",
      "\n",
      "Epoch 01309: val_loss improved from 0.37310 to 0.37303, saving model to DeepFM.h5\n",
      "Epoch 1310/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3505 - auc: 0.832 - ETA: 0s - loss: 0.3638 - auc: 0.786 - 0s 21us/step - loss: 0.3608 - auc: 0.7786 - val_loss: 0.3728 - val_auc: 0.7567\n",
      "\n",
      "Epoch 01310: val_loss improved from 0.37303 to 0.37278, saving model to DeepFM.h5\n",
      "Epoch 1311/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3808 - auc: 0.728 - ETA: 0s - loss: 0.3538 - auc: 0.783 - 0s 22us/step - loss: 0.3578 - auc: 0.7906 - val_loss: 0.3727 - val_auc: 0.7572\n",
      "\n",
      "Epoch 01311: val_loss improved from 0.37278 to 0.37272, saving model to DeepFM.h5\n",
      "Epoch 1312/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3236 - auc: 0.857 - ETA: 0s - loss: 0.3462 - auc: 0.785 - 0s 25us/step - loss: 0.3579 - auc: 0.7886 - val_loss: 0.3727 - val_auc: 0.7573\n",
      "\n",
      "Epoch 01312: val_loss improved from 0.37272 to 0.37269, saving model to DeepFM.h5\n",
      "Epoch 1313/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3460 - auc: 0.744 - ETA: 0s - loss: 0.3624 - auc: 0.786 - 0s 23us/step - loss: 0.3601 - auc: 0.7837 - val_loss: 0.3727 - val_auc: 0.7580\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.37269\n",
      "Epoch 1314/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3573 - auc: 0.836 - ETA: 0s - loss: 0.3620 - auc: 0.794 - 0s 22us/step - loss: 0.3559 - auc: 0.7952 - val_loss: 0.3728 - val_auc: 0.7577\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.37269\n",
      "Epoch 1315/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3880 - auc: 0.727 - ETA: 0s - loss: 0.3668 - auc: 0.775 - 0s 22us/step - loss: 0.3616 - auc: 0.7767 - val_loss: 0.3726 - val_auc: 0.7581\n",
      "\n",
      "Epoch 01315: val_loss improved from 0.37269 to 0.37263, saving model to DeepFM.h5\n",
      "Epoch 1316/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3501 - auc: 0.788 - ETA: 0s - loss: 0.3631 - auc: 0.777 - 0s 22us/step - loss: 0.3636 - auc: 0.7732 - val_loss: 0.3724 - val_auc: 0.7569\n",
      "\n",
      "Epoch 01316: val_loss improved from 0.37263 to 0.37240, saving model to DeepFM.h5\n",
      "Epoch 1317/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3758 - auc: 0.809 - ETA: 0s - loss: 0.3529 - auc: 0.796 - 0s 22us/step - loss: 0.3541 - auc: 0.7949 - val_loss: 0.3724 - val_auc: 0.7582\n",
      "\n",
      "Epoch 01317: val_loss improved from 0.37240 to 0.37239, saving model to DeepFM.h5\n",
      "Epoch 1318/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3713 - auc: 0.776 - ETA: 0s - loss: 0.3599 - auc: 0.803 - 0s 22us/step - loss: 0.3564 - auc: 0.7969 - val_loss: 0.3725 - val_auc: 0.7582\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 0.37239\n",
      "Epoch 1319/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3745 - auc: 0.812 - ETA: 0s - loss: 0.3763 - auc: 0.773 - 0s 24us/step - loss: 0.3612 - auc: 0.7752 - val_loss: 0.3724 - val_auc: 0.7585\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 0.37239\n",
      "Epoch 1320/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3476 - auc: 0.792 - ETA: 0s - loss: 0.3611 - auc: 0.785 - 0s 23us/step - loss: 0.3574 - auc: 0.7886 - val_loss: 0.3723 - val_auc: 0.7574\n",
      "\n",
      "Epoch 01320: val_loss improved from 0.37239 to 0.37228, saving model to DeepFM.h5\n",
      "Epoch 1321/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3594 - auc: 0.824 - ETA: 0s - loss: 0.3658 - auc: 0.788 - 0s 23us/step - loss: 0.3580 - auc: 0.7855 - val_loss: 0.3724 - val_auc: 0.7571\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.37228\n",
      "Epoch 1322/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3262 - auc: 0.779 - ETA: 0s - loss: 0.3670 - auc: 0.790 - 0s 23us/step - loss: 0.3602 - auc: 0.7802 - val_loss: 0.3725 - val_auc: 0.7571\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.37228\n",
      "Epoch 1323/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3693 - auc: 0.728 - ETA: 0s - loss: 0.3506 - auc: 0.789 - 0s 23us/step - loss: 0.3577 - auc: 0.7881 - val_loss: 0.3725 - val_auc: 0.7574\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.37228\n",
      "Epoch 1324/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3951 - auc: 0.807 - ETA: 0s - loss: 0.3579 - auc: 0.781 - 0s 23us/step - loss: 0.3543 - auc: 0.7917 - val_loss: 0.3725 - val_auc: 0.7574\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 0.37228\n",
      "Epoch 1325/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3675 - auc: 0.750 - ETA: 0s - loss: 0.3628 - auc: 0.764 - 0s 23us/step - loss: 0.3644 - auc: 0.7676 - val_loss: 0.3720 - val_auc: 0.7590\n",
      "\n",
      "Epoch 01325: val_loss improved from 0.37228 to 0.37199, saving model to DeepFM.h5\n",
      "Epoch 1326/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3235 - auc: 0.804 - ETA: 0s - loss: 0.3632 - auc: 0.796 - 0s 23us/step - loss: 0.3547 - auc: 0.7943 - val_loss: 0.3719 - val_auc: 0.7586\n",
      "\n",
      "Epoch 01326: val_loss improved from 0.37199 to 0.37188, saving model to DeepFM.h5\n",
      "Epoch 1327/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3163 - auc: 0.760 - ETA: 0s - loss: 0.3622 - auc: 0.784 - 0s 24us/step - loss: 0.3569 - auc: 0.7869 - val_loss: 0.3718 - val_auc: 0.7589\n",
      "\n",
      "Epoch 01327: val_loss improved from 0.37188 to 0.37177, saving model to DeepFM.h5\n",
      "Epoch 1328/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3456 - auc: 0.811 - ETA: 0s - loss: 0.3598 - auc: 0.791 - 0s 23us/step - loss: 0.3600 - auc: 0.7853 - val_loss: 0.3718 - val_auc: 0.7592\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.37177\n",
      "Epoch 1329/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3186 - auc: 0.784 - ETA: 0s - loss: 0.3708 - auc: 0.777 - 0s 23us/step - loss: 0.3616 - auc: 0.7779 - val_loss: 0.3717 - val_auc: 0.7586\n",
      "\n",
      "Epoch 01329: val_loss improved from 0.37177 to 0.37166, saving model to DeepFM.h5\n",
      "Epoch 1330/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3464 - auc: 0.847 - ETA: 0s - loss: 0.3648 - auc: 0.798 - 0s 23us/step - loss: 0.3553 - auc: 0.7915 - val_loss: 0.3718 - val_auc: 0.7595\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.37166\n",
      "Epoch 1331/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3840 - auc: 0.799 - ETA: 0s - loss: 0.3651 - auc: 0.787 - 0s 22us/step - loss: 0.3603 - auc: 0.7796 - val_loss: 0.3719 - val_auc: 0.7576\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 0.37166\n",
      "Epoch 1332/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3687 - auc: 0.795 - ETA: 0s - loss: 0.3591 - auc: 0.777 - 0s 24us/step - loss: 0.3613 - auc: 0.7804 - val_loss: 0.3717 - val_auc: 0.7589\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.37166\n",
      "Epoch 1333/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3889 - auc: 0.744 - ETA: 0s - loss: 0.3573 - auc: 0.788 - 0s 23us/step - loss: 0.3579 - auc: 0.7869 - val_loss: 0.3718 - val_auc: 0.7584\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.37166\n",
      "Epoch 1334/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3766 - auc: 0.708 - ETA: 0s - loss: 0.3516 - auc: 0.785 - 0s 23us/step - loss: 0.3592 - auc: 0.7847 - val_loss: 0.3716 - val_auc: 0.7580\n",
      "\n",
      "Epoch 01334: val_loss improved from 0.37166 to 0.37155, saving model to DeepFM.h5\n",
      "Epoch 1335/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3088 - auc: 0.804 - ETA: 0s - loss: 0.3780 - auc: 0.760 - 0s 22us/step - loss: 0.3608 - auc: 0.7759 - val_loss: 0.3717 - val_auc: 0.7586\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 0.37155\n",
      "Epoch 1336/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4326 - auc: 0.697 - ETA: 0s - loss: 0.3534 - auc: 0.790 - 0s 23us/step - loss: 0.3562 - auc: 0.7877 - val_loss: 0.3715 - val_auc: 0.7585\n",
      "\n",
      "Epoch 01336: val_loss improved from 0.37155 to 0.37154, saving model to DeepFM.h5\n",
      "Epoch 1337/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3795 - auc: 0.790 - ETA: 0s - loss: 0.3557 - auc: 0.787 - 0s 23us/step - loss: 0.3555 - auc: 0.7857 - val_loss: 0.3717 - val_auc: 0.7584\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.37154\n",
      "Epoch 1338/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3429 - auc: 0.724 - ETA: 0s - loss: 0.3552 - auc: 0.795 - 0s 23us/step - loss: 0.3573 - auc: 0.7834 - val_loss: 0.3715 - val_auc: 0.7585\n",
      "\n",
      "Epoch 01338: val_loss improved from 0.37154 to 0.37151, saving model to DeepFM.h5\n",
      "Epoch 1339/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3650 - auc: 0.761 - ETA: 0s - loss: 0.3611 - auc: 0.770 - 0s 23us/step - loss: 0.3590 - auc: 0.7820 - val_loss: 0.3714 - val_auc: 0.7589\n",
      "\n",
      "Epoch 01339: val_loss improved from 0.37151 to 0.37143, saving model to DeepFM.h5\n",
      "Epoch 1340/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3793 - auc: 0.808 - ETA: 0s - loss: 0.3677 - auc: 0.774 - 0s 23us/step - loss: 0.3571 - auc: 0.7867 - val_loss: 0.3714 - val_auc: 0.7589\n",
      "\n",
      "Epoch 01340: val_loss improved from 0.37143 to 0.37140, saving model to DeepFM.h5\n",
      "Epoch 1341/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3313 - auc: 0.847 - ETA: 0s - loss: 0.3549 - auc: 0.783 - 0s 22us/step - loss: 0.3585 - auc: 0.7797 - val_loss: 0.3714 - val_auc: 0.7595\n",
      "\n",
      "Epoch 01341: val_loss improved from 0.37140 to 0.37137, saving model to DeepFM.h5\n",
      "Epoch 1342/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3378 - auc: 0.786 - ETA: 0s - loss: 0.3462 - auc: 0.792 - 0s 23us/step - loss: 0.3558 - auc: 0.7862 - val_loss: 0.3712 - val_auc: 0.7586\n",
      "\n",
      "Epoch 01342: val_loss improved from 0.37137 to 0.37119, saving model to DeepFM.h5\n",
      "Epoch 1343/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3475 - auc: 0.836 - ETA: 0s - loss: 0.3563 - auc: 0.789 - 0s 23us/step - loss: 0.3570 - auc: 0.7897 - val_loss: 0.3713 - val_auc: 0.7597\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.37119\n",
      "Epoch 1344/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3763 - auc: 0.767 - ETA: 0s - loss: 0.3482 - auc: 0.790 - 0s 22us/step - loss: 0.3537 - auc: 0.7969 - val_loss: 0.3712 - val_auc: 0.7596\n",
      "\n",
      "Epoch 01344: val_loss improved from 0.37119 to 0.37116, saving model to DeepFM.h5\n",
      "Epoch 1345/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3410 - auc: 0.810 - ETA: 0s - loss: 0.3498 - auc: 0.797 - 0s 24us/step - loss: 0.3516 - auc: 0.8001 - val_loss: 0.3710 - val_auc: 0.7595\n",
      "\n",
      "Epoch 01345: val_loss improved from 0.37116 to 0.37105, saving model to DeepFM.h5\n",
      "Epoch 1346/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3986 - auc: 0.694 - ETA: 0s - loss: 0.3743 - auc: 0.786 - 0s 23us/step - loss: 0.3567 - auc: 0.7864 - val_loss: 0.3712 - val_auc: 0.7585\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.37105\n",
      "Epoch 1347/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3442 - auc: 0.764 - ETA: 0s - loss: 0.3529 - auc: 0.795 - 0s 22us/step - loss: 0.3515 - auc: 0.7985 - val_loss: 0.3710 - val_auc: 0.7599\n",
      "\n",
      "Epoch 01347: val_loss improved from 0.37105 to 0.37099, saving model to DeepFM.h5\n",
      "Epoch 1348/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3885 - auc: 0.730 - ETA: 0s - loss: 0.3687 - auc: 0.776 - 0s 25us/step - loss: 0.3573 - auc: 0.7892 - val_loss: 0.3711 - val_auc: 0.7598\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.37099\n",
      "Epoch 1349/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3245 - auc: 0.839 - ETA: 0s - loss: 0.3518 - auc: 0.795 - 0s 24us/step - loss: 0.3588 - auc: 0.7811 - val_loss: 0.3709 - val_auc: 0.7601\n",
      "\n",
      "Epoch 01349: val_loss improved from 0.37099 to 0.37088, saving model to DeepFM.h5\n",
      "Epoch 1350/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3167 - auc: 0.846 - ETA: 0s - loss: 0.3397 - auc: 0.809 - 0s 24us/step - loss: 0.3520 - auc: 0.7978 - val_loss: 0.3707 - val_auc: 0.7609\n",
      "\n",
      "Epoch 01350: val_loss improved from 0.37088 to 0.37075, saving model to DeepFM.h5\n",
      "Epoch 1351/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3033 - auc: 0.820 - ETA: 0s - loss: 0.3601 - auc: 0.794 - 0s 23us/step - loss: 0.3543 - auc: 0.7932 - val_loss: 0.3707 - val_auc: 0.7604\n",
      "\n",
      "Epoch 01351: val_loss improved from 0.37075 to 0.37069, saving model to DeepFM.h5\n",
      "Epoch 1352/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3667 - auc: 0.781 - ETA: 0s - loss: 0.3574 - auc: 0.794 - 0s 23us/step - loss: 0.3528 - auc: 0.8023 - val_loss: 0.3707 - val_auc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01352: val_loss improved from 0.37069 to 0.37068, saving model to DeepFM.h5\n",
      "Epoch 1353/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3743 - auc: 0.740 - ETA: 0s - loss: 0.3695 - auc: 0.775 - 0s 25us/step - loss: 0.3563 - auc: 0.7848 - val_loss: 0.3707 - val_auc: 0.7596\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 0.37068\n",
      "Epoch 1354/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3480 - auc: 0.835 - ETA: 0s - loss: 0.3622 - auc: 0.795 - 0s 21us/step - loss: 0.3554 - auc: 0.7917 - val_loss: 0.3707 - val_auc: 0.7597\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.37068\n",
      "Epoch 1355/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3360 - auc: 0.816 - ETA: 0s - loss: 0.3459 - auc: 0.799 - 0s 24us/step - loss: 0.3530 - auc: 0.7981 - val_loss: 0.3707 - val_auc: 0.7595\n",
      "\n",
      "Epoch 01355: val_loss improved from 0.37068 to 0.37067, saving model to DeepFM.h5\n",
      "Epoch 1356/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3748 - auc: 0.817 - ETA: 0s - loss: 0.3568 - auc: 0.789 - 0s 24us/step - loss: 0.3541 - auc: 0.7902 - val_loss: 0.3705 - val_auc: 0.7604\n",
      "\n",
      "Epoch 01356: val_loss improved from 0.37067 to 0.37053, saving model to DeepFM.h5\n",
      "Epoch 1357/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3646 - auc: 0.753 - ETA: 0s - loss: 0.3516 - auc: 0.798 - 0s 23us/step - loss: 0.3504 - auc: 0.7965 - val_loss: 0.3706 - val_auc: 0.7600\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 0.37053\n",
      "Epoch 1358/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3133 - auc: 0.784 - ETA: 0s - loss: 0.3419 - auc: 0.796 - 0s 26us/step - loss: 0.3545 - auc: 0.7893 - val_loss: 0.3704 - val_auc: 0.7594\n",
      "\n",
      "Epoch 01358: val_loss improved from 0.37053 to 0.37040, saving model to DeepFM.h5\n",
      "Epoch 1359/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3942 - auc: 0.735 - ETA: 0s - loss: 0.3496 - auc: 0.805 - 0s 24us/step - loss: 0.3564 - auc: 0.7933 - val_loss: 0.3704 - val_auc: 0.7609\n",
      "\n",
      "Epoch 01359: val_loss improved from 0.37040 to 0.37037, saving model to DeepFM.h5\n",
      "Epoch 1360/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3968 - auc: 0.782 - ETA: 0s - loss: 0.3588 - auc: 0.797 - 0s 23us/step - loss: 0.3540 - auc: 0.7947 - val_loss: 0.3704 - val_auc: 0.7591\n",
      "\n",
      "Epoch 01360: val_loss improved from 0.37037 to 0.37035, saving model to DeepFM.h5\n",
      "Epoch 1361/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3357 - auc: 0.857 - ETA: 0s - loss: 0.3486 - auc: 0.802 - 0s 23us/step - loss: 0.3509 - auc: 0.7993 - val_loss: 0.3703 - val_auc: 0.7592\n",
      "\n",
      "Epoch 01361: val_loss improved from 0.37035 to 0.37032, saving model to DeepFM.h5\n",
      "Epoch 1362/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3064 - auc: 0.837 - ETA: 0s - loss: 0.3517 - auc: 0.796 - 0s 24us/step - loss: 0.3522 - auc: 0.7943 - val_loss: 0.3701 - val_auc: 0.7604\n",
      "\n",
      "Epoch 01362: val_loss improved from 0.37032 to 0.37015, saving model to DeepFM.h5\n",
      "Epoch 1363/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3732 - auc: 0.794 - ETA: 0s - loss: 0.3531 - auc: 0.794 - 0s 22us/step - loss: 0.3543 - auc: 0.7911 - val_loss: 0.3702 - val_auc: 0.7593\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.37015\n",
      "Epoch 1364/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3596 - auc: 0.826 - ETA: 0s - loss: 0.3631 - auc: 0.789 - 0s 21us/step - loss: 0.3551 - auc: 0.7937 - val_loss: 0.3702 - val_auc: 0.7588\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.37015\n",
      "Epoch 1365/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3705 - auc: 0.810 - ETA: 0s - loss: 0.3499 - auc: 0.795 - 0s 23us/step - loss: 0.3569 - auc: 0.7903 - val_loss: 0.3701 - val_auc: 0.7600\n",
      "\n",
      "Epoch 01365: val_loss improved from 0.37015 to 0.37006, saving model to DeepFM.h5\n",
      "Epoch 1366/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3526 - auc: 0.710 - ETA: 0s - loss: 0.3556 - auc: 0.783 - 0s 21us/step - loss: 0.3542 - auc: 0.7924 - val_loss: 0.3700 - val_auc: 0.7599\n",
      "\n",
      "Epoch 01366: val_loss improved from 0.37006 to 0.37005, saving model to DeepFM.h5\n",
      "Epoch 1367/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3484 - auc: 0.769 - ETA: 0s - loss: 0.3467 - auc: 0.804 - 0s 21us/step - loss: 0.3512 - auc: 0.8033 - val_loss: 0.3700 - val_auc: 0.7598\n",
      "\n",
      "Epoch 01367: val_loss improved from 0.37005 to 0.37002, saving model to DeepFM.h5\n",
      "Epoch 1368/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4074 - auc: 0.710 - ETA: 0s - loss: 0.3526 - auc: 0.800 - 0s 22us/step - loss: 0.3543 - auc: 0.7927 - val_loss: 0.3703 - val_auc: 0.7595\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.37002\n",
      "Epoch 1369/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3702 - auc: 0.841 - ETA: 0s - loss: 0.3605 - auc: 0.767 - 0s 23us/step - loss: 0.3608 - auc: 0.7739 - val_loss: 0.3701 - val_auc: 0.7591\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.37002\n",
      "Epoch 1370/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3564 - auc: 0.799 - ETA: 0s - loss: 0.3472 - auc: 0.787 - 0s 22us/step - loss: 0.3505 - auc: 0.8029 - val_loss: 0.3699 - val_auc: 0.7596\n",
      "\n",
      "Epoch 01370: val_loss improved from 0.37002 to 0.36987, saving model to DeepFM.h5\n",
      "Epoch 1371/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3164 - auc: 0.836 - ETA: 0s - loss: 0.3575 - auc: 0.805 - 0s 23us/step - loss: 0.3492 - auc: 0.8043 - val_loss: 0.3699 - val_auc: 0.7592\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.36987\n",
      "Epoch 1372/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3240 - auc: 0.806 - ETA: 0s - loss: 0.3458 - auc: 0.803 - 0s 23us/step - loss: 0.3512 - auc: 0.7967 - val_loss: 0.3697 - val_auc: 0.7600\n",
      "\n",
      "Epoch 01372: val_loss improved from 0.36987 to 0.36971, saving model to DeepFM.h5\n",
      "Epoch 1373/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3133 - auc: 0.793 - ETA: 0s - loss: 0.3569 - auc: 0.798 - 0s 23us/step - loss: 0.3526 - auc: 0.7937 - val_loss: 0.3698 - val_auc: 0.7592\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 0.36971\n",
      "Epoch 1374/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3242 - auc: 0.876 - ETA: 0s - loss: 0.3619 - auc: 0.783 - 0s 22us/step - loss: 0.3529 - auc: 0.7988 - val_loss: 0.3697 - val_auc: 0.7592\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 0.36971\n",
      "Epoch 1375/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3759 - auc: 0.769 - ETA: 0s - loss: 0.3432 - auc: 0.798 - 0s 22us/step - loss: 0.3548 - auc: 0.7886 - val_loss: 0.3697 - val_auc: 0.7605\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 0.36971\n",
      "Epoch 1376/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3438 - auc: 0.836 - ETA: 0s - loss: 0.3454 - auc: 0.798 - 0s 22us/step - loss: 0.3513 - auc: 0.8004 - val_loss: 0.3696 - val_auc: 0.7600\n",
      "\n",
      "Epoch 01376: val_loss improved from 0.36971 to 0.36956, saving model to DeepFM.h5\n",
      "Epoch 1377/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3488 - auc: 0.734 - ETA: 0s - loss: 0.3543 - auc: 0.783 - 0s 23us/step - loss: 0.3556 - auc: 0.7871 - val_loss: 0.3695 - val_auc: 0.7597\n",
      "\n",
      "Epoch 01377: val_loss improved from 0.36956 to 0.36950, saving model to DeepFM.h5\n",
      "Epoch 1378/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3612 - auc: 0.839 - ETA: 0s - loss: 0.3552 - auc: 0.799 - 0s 22us/step - loss: 0.3517 - auc: 0.8002 - val_loss: 0.3696 - val_auc: 0.7603\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 0.36950\n",
      "Epoch 1379/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3357 - auc: 0.815 - ETA: 0s - loss: 0.3574 - auc: 0.791 - 0s 24us/step - loss: 0.3532 - auc: 0.7949 - val_loss: 0.3695 - val_auc: 0.7602\n",
      "\n",
      "Epoch 01379: val_loss improved from 0.36950 to 0.36946, saving model to DeepFM.h5\n",
      "Epoch 1380/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4020 - auc: 0.806 - ETA: 0s - loss: 0.3492 - auc: 0.789 - 0s 26us/step - loss: 0.3494 - auc: 0.7994 - val_loss: 0.3696 - val_auc: 0.7601\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 0.36946\n",
      "Epoch 1381/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3589 - auc: 0.787 - ETA: 0s - loss: 0.3412 - auc: 0.798 - 0s 24us/step - loss: 0.3501 - auc: 0.7997 - val_loss: 0.3694 - val_auc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01381: val_loss improved from 0.36946 to 0.36941, saving model to DeepFM.h5\n",
      "Epoch 1382/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3798 - auc: 0.740 - ETA: 0s - loss: 0.3585 - auc: 0.781 - 0s 23us/step - loss: 0.3514 - auc: 0.7964 - val_loss: 0.3695 - val_auc: 0.7603\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 0.36941\n",
      "Epoch 1383/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4151 - auc: 0.767 - ETA: 0s - loss: 0.3554 - auc: 0.782 - 0s 23us/step - loss: 0.3543 - auc: 0.7871 - val_loss: 0.3694 - val_auc: 0.7605\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 0.36941\n",
      "Epoch 1384/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3218 - auc: 0.836 - ETA: 0s - loss: 0.3466 - auc: 0.796 - 0s 21us/step - loss: 0.3518 - auc: 0.7955 - val_loss: 0.3693 - val_auc: 0.7601\n",
      "\n",
      "Epoch 01384: val_loss improved from 0.36941 to 0.36933, saving model to DeepFM.h5\n",
      "Epoch 1385/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.812 - ETA: 0s - loss: 0.3388 - auc: 0.798 - 0s 23us/step - loss: 0.3498 - auc: 0.7976 - val_loss: 0.3691 - val_auc: 0.7598\n",
      "\n",
      "Epoch 01385: val_loss improved from 0.36933 to 0.36911, saving model to DeepFM.h5\n",
      "Epoch 1386/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3193 - auc: 0.823 - ETA: 0s - loss: 0.3293 - auc: 0.809 - 0s 23us/step - loss: 0.3465 - auc: 0.8069 - val_loss: 0.3691 - val_auc: 0.7601\n",
      "\n",
      "Epoch 01386: val_loss improved from 0.36911 to 0.36911, saving model to DeepFM.h5\n",
      "Epoch 1387/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4299 - auc: 0.788 - ETA: 0s - loss: 0.3486 - auc: 0.794 - 0s 23us/step - loss: 0.3476 - auc: 0.8021 - val_loss: 0.3691 - val_auc: 0.7607\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 0.36911\n",
      "Epoch 1388/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3592 - auc: 0.821 - ETA: 0s - loss: 0.3462 - auc: 0.813 - 0s 23us/step - loss: 0.3462 - auc: 0.8112 - val_loss: 0.3692 - val_auc: 0.7615\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 0.36911\n",
      "Epoch 1389/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3479 - auc: 0.866 - ETA: 0s - loss: 0.3476 - auc: 0.789 - 0s 22us/step - loss: 0.3525 - auc: 0.7936 - val_loss: 0.3691 - val_auc: 0.7615\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 0.36911\n",
      "Epoch 1390/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4081 - auc: 0.739 - ETA: 0s - loss: 0.3578 - auc: 0.795 - 0s 21us/step - loss: 0.3508 - auc: 0.7953 - val_loss: 0.3694 - val_auc: 0.7611\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 0.36911\n",
      "Epoch 1391/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3956 - auc: 0.822 - ETA: 0s - loss: 0.3467 - auc: 0.810 - 0s 22us/step - loss: 0.3470 - auc: 0.8081 - val_loss: 0.3693 - val_auc: 0.7612\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 0.36911\n",
      "Epoch 1392/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3788 - auc: 0.785 - ETA: 0s - loss: 0.3496 - auc: 0.803 - 0s 23us/step - loss: 0.3549 - auc: 0.7916 - val_loss: 0.3691 - val_auc: 0.7622\n",
      "\n",
      "Epoch 01392: val_loss improved from 0.36911 to 0.36905, saving model to DeepFM.h5\n",
      "Epoch 1393/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2823 - auc: 0.888 - ETA: 0s - loss: 0.3414 - auc: 0.811 - 0s 24us/step - loss: 0.3492 - auc: 0.8013 - val_loss: 0.3689 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01393: val_loss improved from 0.36905 to 0.36890, saving model to DeepFM.h5\n",
      "Epoch 1394/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3375 - auc: 0.815 - ETA: 0s - loss: 0.3585 - auc: 0.809 - 0s 22us/step - loss: 0.3433 - auc: 0.8146 - val_loss: 0.3690 - val_auc: 0.7614\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 0.36890\n",
      "Epoch 1395/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3800 - auc: 0.746 - ETA: 0s - loss: 0.3415 - auc: 0.800 - 0s 23us/step - loss: 0.3550 - auc: 0.7935 - val_loss: 0.3686 - val_auc: 0.7603\n",
      "\n",
      "Epoch 01395: val_loss improved from 0.36890 to 0.36864, saving model to DeepFM.h5\n",
      "Epoch 1396/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3321 - auc: 0.799 - ETA: 0s - loss: 0.3460 - auc: 0.798 - 0s 22us/step - loss: 0.3492 - auc: 0.8013 - val_loss: 0.3685 - val_auc: 0.7616\n",
      "\n",
      "Epoch 01396: val_loss improved from 0.36864 to 0.36846, saving model to DeepFM.h5\n",
      "Epoch 1397/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3285 - auc: 0.800 - ETA: 0s - loss: 0.3455 - auc: 0.798 - 0s 22us/step - loss: 0.3516 - auc: 0.7943 - val_loss: 0.3686 - val_auc: 0.7614\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 0.36846\n",
      "Epoch 1398/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3296 - auc: 0.827 - ETA: 0s - loss: 0.3367 - auc: 0.823 - 0s 24us/step - loss: 0.3445 - auc: 0.8127 - val_loss: 0.3687 - val_auc: 0.7623\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 0.36846\n",
      "Epoch 1399/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3599 - auc: 0.771 - ETA: 0s - loss: 0.3418 - auc: 0.805 - 0s 22us/step - loss: 0.3488 - auc: 0.7997 - val_loss: 0.3686 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 0.36846\n",
      "Epoch 1400/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2938 - auc: 0.847 - ETA: 0s - loss: 0.3544 - auc: 0.796 - 0s 23us/step - loss: 0.3488 - auc: 0.8051 - val_loss: 0.3685 - val_auc: 0.7613\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 0.36846\n",
      "Epoch 1401/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3635 - auc: 0.779 - ETA: 0s - loss: 0.3468 - auc: 0.804 - 0s 23us/step - loss: 0.3488 - auc: 0.8022 - val_loss: 0.3684 - val_auc: 0.7618\n",
      "\n",
      "Epoch 01401: val_loss improved from 0.36846 to 0.36837, saving model to DeepFM.h5\n",
      "Epoch 1402/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3281 - auc: 0.827 - ETA: 0s - loss: 0.3514 - auc: 0.794 - 0s 23us/step - loss: 0.3516 - auc: 0.7954 - val_loss: 0.3684 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 0.36837\n",
      "Epoch 1403/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3404 - auc: 0.869 - ETA: 0s - loss: 0.3507 - auc: 0.808 - 0s 23us/step - loss: 0.3466 - auc: 0.8051 - val_loss: 0.3686 - val_auc: 0.7621\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 0.36837\n",
      "Epoch 1404/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3167 - auc: 0.782 - ETA: 0s - loss: 0.3520 - auc: 0.810 - 0s 23us/step - loss: 0.3472 - auc: 0.8053 - val_loss: 0.3686 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 0.36837\n",
      "Epoch 1405/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3150 - auc: 0.814 - ETA: 0s - loss: 0.3396 - auc: 0.804 - 0s 23us/step - loss: 0.3523 - auc: 0.7885 - val_loss: 0.3684 - val_auc: 0.7612\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 0.36837\n",
      "Epoch 1406/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3290 - auc: 0.791 - ETA: 0s - loss: 0.3624 - auc: 0.774 - 0s 22us/step - loss: 0.3558 - auc: 0.7848 - val_loss: 0.3682 - val_auc: 0.7616\n",
      "\n",
      "Epoch 01406: val_loss improved from 0.36837 to 0.36821, saving model to DeepFM.h5\n",
      "Epoch 1407/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3519 - auc: 0.813 - ETA: 0s - loss: 0.3368 - auc: 0.801 - 0s 23us/step - loss: 0.3508 - auc: 0.7993 - val_loss: 0.3680 - val_auc: 0.7626\n",
      "\n",
      "Epoch 01407: val_loss improved from 0.36821 to 0.36801, saving model to DeepFM.h5\n",
      "Epoch 1408/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3130 - auc: 0.858 - ETA: 0s - loss: 0.3427 - auc: 0.803 - 0s 24us/step - loss: 0.3494 - auc: 0.8006 - val_loss: 0.3680 - val_auc: 0.7622\n",
      "\n",
      "Epoch 01408: val_loss improved from 0.36801 to 0.36799, saving model to DeepFM.h5\n",
      "Epoch 1409/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3536 - auc: 0.784 - ETA: 0s - loss: 0.3449 - auc: 0.806 - 0s 24us/step - loss: 0.3501 - auc: 0.7994 - val_loss: 0.3680 - val_auc: 0.7618\n",
      "\n",
      "Epoch 01409: val_loss improved from 0.36799 to 0.36798, saving model to DeepFM.h5\n",
      "Epoch 1410/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3459 - auc: 0.845 - ETA: 0s - loss: 0.3573 - auc: 0.789 - 0s 24us/step - loss: 0.3449 - auc: 0.8111 - val_loss: 0.3683 - val_auc: 0.7625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01410: val_loss did not improve from 0.36798\n",
      "Epoch 1411/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2994 - auc: 0.829 - ETA: 0s - loss: 0.3368 - auc: 0.813 - 0s 23us/step - loss: 0.3472 - auc: 0.8036 - val_loss: 0.3681 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 0.36798\n",
      "Epoch 1412/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3335 - auc: 0.819 - ETA: 0s - loss: 0.3509 - auc: 0.791 - 0s 24us/step - loss: 0.3494 - auc: 0.7956 - val_loss: 0.3680 - val_auc: 0.7621\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 0.36798\n",
      "Epoch 1413/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3527 - auc: 0.807 - ETA: 0s - loss: 0.3506 - auc: 0.794 - 0s 23us/step - loss: 0.3491 - auc: 0.7972 - val_loss: 0.3681 - val_auc: 0.7618\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 0.36798\n",
      "Epoch 1414/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3259 - auc: 0.810 - ETA: 0s - loss: 0.3482 - auc: 0.807 - 0s 21us/step - loss: 0.3473 - auc: 0.8073 - val_loss: 0.3682 - val_auc: 0.7623\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 0.36798\n",
      "Epoch 1415/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3983 - auc: 0.757 - ETA: 0s - loss: 0.3461 - auc: 0.801 - 0s 22us/step - loss: 0.3493 - auc: 0.7968 - val_loss: 0.3680 - val_auc: 0.7618\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 0.36798\n",
      "Epoch 1416/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3310 - auc: 0.835 - ETA: 0s - loss: 0.3513 - auc: 0.803 - 0s 23us/step - loss: 0.3510 - auc: 0.7988 - val_loss: 0.3680 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 0.36798\n",
      "Epoch 1417/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.838 - ETA: 0s - loss: 0.3342 - auc: 0.805 - 0s 22us/step - loss: 0.3424 - auc: 0.8131 - val_loss: 0.3679 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01417: val_loss improved from 0.36798 to 0.36791, saving model to DeepFM.h5\n",
      "Epoch 1418/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3309 - auc: 0.814 - ETA: 0s - loss: 0.3377 - auc: 0.820 - 0s 22us/step - loss: 0.3493 - auc: 0.8028 - val_loss: 0.3679 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 0.36791\n",
      "Epoch 1419/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3545 - auc: 0.760 - ETA: 0s - loss: 0.3504 - auc: 0.795 - 0s 21us/step - loss: 0.3486 - auc: 0.8002 - val_loss: 0.3679 - val_auc: 0.7617\n",
      "\n",
      "Epoch 01419: val_loss improved from 0.36791 to 0.36789, saving model to DeepFM.h5\n",
      "Epoch 1420/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3237 - auc: 0.818 - ETA: 0s - loss: 0.3500 - auc: 0.808 - 0s 23us/step - loss: 0.3478 - auc: 0.8023 - val_loss: 0.3678 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01420: val_loss improved from 0.36789 to 0.36782, saving model to DeepFM.h5\n",
      "Epoch 1421/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3790 - auc: 0.748 - ETA: 0s - loss: 0.3518 - auc: 0.807 - 0s 22us/step - loss: 0.3518 - auc: 0.7980 - val_loss: 0.3679 - val_auc: 0.7614\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 0.36782\n",
      "Epoch 1422/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3405 - auc: 0.832 - ETA: 0s - loss: 0.3449 - auc: 0.812 - 0s 23us/step - loss: 0.3508 - auc: 0.8016 - val_loss: 0.3677 - val_auc: 0.7615\n",
      "\n",
      "Epoch 01422: val_loss improved from 0.36782 to 0.36774, saving model to DeepFM.h5\n",
      "Epoch 1423/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2998 - auc: 0.851 - ETA: 0s - loss: 0.3381 - auc: 0.821 - 0s 24us/step - loss: 0.3414 - auc: 0.8218 - val_loss: 0.3677 - val_auc: 0.7617\n",
      "\n",
      "Epoch 01423: val_loss improved from 0.36774 to 0.36773, saving model to DeepFM.h5\n",
      "Epoch 1424/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3033 - auc: 0.785 - ETA: 0s - loss: 0.3384 - auc: 0.815 - 0s 23us/step - loss: 0.3467 - auc: 0.8066 - val_loss: 0.3678 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 0.36773\n",
      "Epoch 1425/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3902 - auc: 0.783 - ETA: 0s - loss: 0.3570 - auc: 0.809 - 0s 23us/step - loss: 0.3455 - auc: 0.8048 - val_loss: 0.3681 - val_auc: 0.7631\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 0.36773\n",
      "Epoch 1426/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3329 - auc: 0.808 - ETA: 0s - loss: 0.3511 - auc: 0.810 - 0s 23us/step - loss: 0.3438 - auc: 0.8091 - val_loss: 0.3680 - val_auc: 0.7631\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 0.36773\n",
      "Epoch 1427/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3407 - auc: 0.774 - ETA: 0s - loss: 0.3558 - auc: 0.809 - 0s 25us/step - loss: 0.3477 - auc: 0.8067 - val_loss: 0.3678 - val_auc: 0.7624\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 0.36773\n",
      "Epoch 1428/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3499 - auc: 0.764 - ETA: 0s - loss: 0.3624 - auc: 0.792 - 0s 23us/step - loss: 0.3515 - auc: 0.7969 - val_loss: 0.3676 - val_auc: 0.7627\n",
      "\n",
      "Epoch 01428: val_loss improved from 0.36773 to 0.36762, saving model to DeepFM.h5\n",
      "Epoch 1429/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3255 - auc: 0.830 - ETA: 0s - loss: 0.3405 - auc: 0.821 - 0s 23us/step - loss: 0.3448 - auc: 0.8118 - val_loss: 0.3676 - val_auc: 0.7626\n",
      "\n",
      "Epoch 01429: val_loss improved from 0.36762 to 0.36756, saving model to DeepFM.h5\n",
      "Epoch 1430/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3252 - auc: 0.809 - ETA: 0s - loss: 0.3485 - auc: 0.806 - 0s 23us/step - loss: 0.3479 - auc: 0.8046 - val_loss: 0.3676 - val_auc: 0.7631\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 0.36756\n",
      "Epoch 1431/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3057 - auc: 0.843 - ETA: 0s - loss: 0.3501 - auc: 0.798 - 0s 22us/step - loss: 0.3525 - auc: 0.7951 - val_loss: 0.3674 - val_auc: 0.7612\n",
      "\n",
      "Epoch 01431: val_loss improved from 0.36756 to 0.36742, saving model to DeepFM.h5\n",
      "Epoch 1432/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2791 - auc: 0.862 - ETA: 0s - loss: 0.3466 - auc: 0.810 - 0s 22us/step - loss: 0.3455 - auc: 0.8145 - val_loss: 0.3674 - val_auc: 0.7613\n",
      "\n",
      "Epoch 01432: val_loss improved from 0.36742 to 0.36740, saving model to DeepFM.h5\n",
      "Epoch 1433/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2894 - auc: 0.838 - ETA: 0s - loss: 0.3441 - auc: 0.823 - 0s 23us/step - loss: 0.3415 - auc: 0.8166 - val_loss: 0.3676 - val_auc: 0.7637\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 0.36740\n",
      "Epoch 1434/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3303 - auc: 0.804 - ETA: 0s - loss: 0.3436 - auc: 0.826 - 0s 24us/step - loss: 0.3470 - auc: 0.8028 - val_loss: 0.3675 - val_auc: 0.7638\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 0.36740\n",
      "Epoch 1435/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2967 - auc: 0.862 - ETA: 0s - loss: 0.3532 - auc: 0.806 - 0s 22us/step - loss: 0.3462 - auc: 0.8046 - val_loss: 0.3675 - val_auc: 0.7645\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 0.36740\n",
      "Epoch 1436/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3214 - auc: 0.826 - ETA: 0s - loss: 0.3417 - auc: 0.802 - 0s 25us/step - loss: 0.3469 - auc: 0.8046 - val_loss: 0.3673 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01436: val_loss improved from 0.36740 to 0.36727, saving model to DeepFM.h5\n",
      "Epoch 1437/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3908 - auc: 0.761 - ETA: 0s - loss: 0.3422 - auc: 0.809 - 0s 23us/step - loss: 0.3457 - auc: 0.8104 - val_loss: 0.3672 - val_auc: 0.7618\n",
      "\n",
      "Epoch 01437: val_loss improved from 0.36727 to 0.36715, saving model to DeepFM.h5\n",
      "Epoch 1438/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3672 - auc: 0.825 - ETA: 0s - loss: 0.3498 - auc: 0.802 - 0s 23us/step - loss: 0.3446 - auc: 0.8110 - val_loss: 0.3674 - val_auc: 0.7640\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 0.36715\n",
      "Epoch 1439/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3131 - auc: 0.864 - ETA: 0s - loss: 0.3423 - auc: 0.808 - 0s 23us/step - loss: 0.3452 - auc: 0.8095 - val_loss: 0.3672 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 0.36715\n",
      "Epoch 1440/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3676 - auc: 0.819 - ETA: 0s - loss: 0.3366 - auc: 0.817 - 0s 24us/step - loss: 0.3436 - auc: 0.8117 - val_loss: 0.3673 - val_auc: 0.7638\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 0.36715\n",
      "Epoch 1441/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3524 - auc: 0.785 - ETA: 0s - loss: 0.3458 - auc: 0.803 - 0s 24us/step - loss: 0.3432 - auc: 0.8107 - val_loss: 0.3672 - val_auc: 0.7632\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 0.36715\n",
      "Epoch 1442/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3309 - auc: 0.810 - ETA: 0s - loss: 0.3356 - auc: 0.826 - 0s 21us/step - loss: 0.3436 - auc: 0.8098 - val_loss: 0.3673 - val_auc: 0.7630\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 0.36715\n",
      "Epoch 1443/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3348 - auc: 0.835 - ETA: 0s - loss: 0.3552 - auc: 0.800 - 0s 21us/step - loss: 0.3451 - auc: 0.8083 - val_loss: 0.3672 - val_auc: 0.7629\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 0.36715\n",
      "Epoch 1444/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3366 - auc: 0.841 - ETA: 0s - loss: 0.3411 - auc: 0.816 - 0s 23us/step - loss: 0.3439 - auc: 0.8113 - val_loss: 0.3672 - val_auc: 0.7619\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 0.36715\n",
      "Epoch 1445/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3145 - auc: 0.793 - ETA: 0s - loss: 0.3404 - auc: 0.822 - 0s 23us/step - loss: 0.3411 - auc: 0.8190 - val_loss: 0.3676 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 0.36715\n",
      "Epoch 1446/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3454 - auc: 0.804 - ETA: 0s - loss: 0.3436 - auc: 0.800 - 0s 23us/step - loss: 0.3465 - auc: 0.8029 - val_loss: 0.3671 - val_auc: 0.7621\n",
      "\n",
      "Epoch 01446: val_loss improved from 0.36715 to 0.36710, saving model to DeepFM.h5\n",
      "Epoch 1447/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3853 - auc: 0.809 - ETA: 0s - loss: 0.3509 - auc: 0.810 - 0s 23us/step - loss: 0.3440 - auc: 0.8105 - val_loss: 0.3672 - val_auc: 0.7631\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 0.36710\n",
      "Epoch 1448/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3634 - auc: 0.780 - ETA: 0s - loss: 0.3441 - auc: 0.797 - 0s 23us/step - loss: 0.3469 - auc: 0.8024 - val_loss: 0.3671 - val_auc: 0.7629\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 0.36710\n",
      "Epoch 1449/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3084 - auc: 0.846 - ETA: 0s - loss: 0.3468 - auc: 0.802 - 0s 23us/step - loss: 0.3449 - auc: 0.8064 - val_loss: 0.3671 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 0.36710\n",
      "Epoch 1450/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3963 - auc: 0.799 - ETA: 0s - loss: 0.3485 - auc: 0.805 - 0s 23us/step - loss: 0.3441 - auc: 0.8082 - val_loss: 0.3670 - val_auc: 0.7626\n",
      "\n",
      "Epoch 01450: val_loss improved from 0.36710 to 0.36704, saving model to DeepFM.h5\n",
      "Epoch 1451/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3997 - auc: 0.772 - ETA: 0s - loss: 0.3461 - auc: 0.819 - 0s 23us/step - loss: 0.3417 - auc: 0.8172 - val_loss: 0.3672 - val_auc: 0.7624\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 0.36704\n",
      "Epoch 1452/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3812 - auc: 0.718 - ETA: 0s - loss: 0.3447 - auc: 0.799 - 0s 23us/step - loss: 0.3442 - auc: 0.8043 - val_loss: 0.3670 - val_auc: 0.7622\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 0.36704\n",
      "Epoch 1453/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3899 - auc: 0.785 - ETA: 0s - loss: 0.3365 - auc: 0.811 - 0s 22us/step - loss: 0.3397 - auc: 0.8150 - val_loss: 0.3672 - val_auc: 0.7624\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 0.36704\n",
      "Epoch 1454/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3344 - auc: 0.797 - ETA: 0s - loss: 0.3355 - auc: 0.817 - 0s 23us/step - loss: 0.3431 - auc: 0.8079 - val_loss: 0.3671 - val_auc: 0.7625\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 0.36704\n",
      "Epoch 1455/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3798 - auc: 0.806 - ETA: 0s - loss: 0.3346 - auc: 0.821 - 0s 23us/step - loss: 0.3398 - auc: 0.8195 - val_loss: 0.3670 - val_auc: 0.7620\n",
      "\n",
      "Epoch 01455: val_loss improved from 0.36704 to 0.36699, saving model to DeepFM.h5\n",
      "Epoch 1456/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3227 - auc: 0.817 - ETA: 0s - loss: 0.3468 - auc: 0.789 - 0s 23us/step - loss: 0.3505 - auc: 0.7914 - val_loss: 0.3669 - val_auc: 0.7630\n",
      "\n",
      "Epoch 01456: val_loss improved from 0.36699 to 0.36686, saving model to DeepFM.h5\n",
      "Epoch 1457/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3415 - auc: 0.841 - ETA: 0s - loss: 0.3459 - auc: 0.802 - 0s 23us/step - loss: 0.3409 - auc: 0.8158 - val_loss: 0.3670 - val_auc: 0.7624\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 0.36686\n",
      "Epoch 1458/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3225 - auc: 0.834 - ETA: 0s - loss: 0.3409 - auc: 0.810 - 0s 24us/step - loss: 0.3415 - auc: 0.8165 - val_loss: 0.3670 - val_auc: 0.7627\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 0.36686\n",
      "Epoch 1459/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3496 - auc: 0.791 - ETA: 0s - loss: 0.3356 - auc: 0.825 - 0s 21us/step - loss: 0.3381 - auc: 0.8178 - val_loss: 0.3671 - val_auc: 0.7623\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 0.36686\n",
      "Epoch 1460/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3734 - auc: 0.768 - ETA: 0s - loss: 0.3465 - auc: 0.800 - 0s 21us/step - loss: 0.3457 - auc: 0.8074 - val_loss: 0.3670 - val_auc: 0.7629\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 0.36686\n",
      "Epoch 1461/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3838 - auc: 0.763 - ETA: 0s - loss: 0.3353 - auc: 0.806 - 0s 23us/step - loss: 0.3427 - auc: 0.8105 - val_loss: 0.3666 - val_auc: 0.7625\n",
      "\n",
      "Epoch 01461: val_loss improved from 0.36686 to 0.36662, saving model to DeepFM.h5\n",
      "Epoch 1462/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3371 - auc: 0.787 - ETA: 0s - loss: 0.3423 - auc: 0.805 - 0s 23us/step - loss: 0.3401 - auc: 0.8174 - val_loss: 0.3670 - val_auc: 0.7627\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 0.36662\n",
      "Epoch 1463/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4082 - auc: 0.785 - ETA: 0s - loss: 0.3432 - auc: 0.806 - 0s 24us/step - loss: 0.3468 - auc: 0.8053 - val_loss: 0.3669 - val_auc: 0.7632\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 0.36662\n",
      "Epoch 1464/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2984 - auc: 0.852 - ETA: 0s - loss: 0.3377 - auc: 0.815 - 0s 23us/step - loss: 0.3433 - auc: 0.8108 - val_loss: 0.3667 - val_auc: 0.7627\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 0.36662\n",
      "Epoch 1465/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3856 - auc: 0.728 - ETA: 0s - loss: 0.3629 - auc: 0.789 - 0s 24us/step - loss: 0.3477 - auc: 0.8039 - val_loss: 0.3671 - val_auc: 0.7634\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 0.36662\n",
      "Epoch 1466/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2567 - auc: 0.836 - ETA: 0s - loss: 0.3458 - auc: 0.819 - 0s 23us/step - loss: 0.3413 - auc: 0.8167 - val_loss: 0.3670 - val_auc: 0.7631\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 0.36662\n",
      "Epoch 1467/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3723 - auc: 0.823 - ETA: 0s - loss: 0.3367 - auc: 0.809 - 0s 23us/step - loss: 0.3431 - auc: 0.8123 - val_loss: 0.3667 - val_auc: 0.7631\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 0.36662\n",
      "Epoch 1468/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3368 - auc: 0.833 - ETA: 0s - loss: 0.3512 - auc: 0.812 - 0s 21us/step - loss: 0.3429 - auc: 0.8115 - val_loss: 0.3668 - val_auc: 0.7640\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 0.36662\n",
      "Epoch 1469/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3268 - auc: 0.860 - ETA: 0s - loss: 0.3369 - auc: 0.808 - 0s 23us/step - loss: 0.3430 - auc: 0.8077 - val_loss: 0.3667 - val_auc: 0.7633\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 0.36662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1470/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3184 - auc: 0.823 - ETA: 0s - loss: 0.3449 - auc: 0.811 - 0s 23us/step - loss: 0.3427 - auc: 0.8162 - val_loss: 0.3664 - val_auc: 0.7628\n",
      "\n",
      "Epoch 01470: val_loss improved from 0.36662 to 0.36638, saving model to DeepFM.h5\n",
      "Epoch 1471/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3575 - auc: 0.773 - ETA: 0s - loss: 0.3370 - auc: 0.819 - 0s 24us/step - loss: 0.3408 - auc: 0.8158 - val_loss: 0.3663 - val_auc: 0.7628\n",
      "\n",
      "Epoch 01471: val_loss improved from 0.36638 to 0.36633, saving model to DeepFM.h5\n",
      "Epoch 1472/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3353 - auc: 0.816 - ETA: 0s - loss: 0.3442 - auc: 0.813 - 0s 22us/step - loss: 0.3424 - auc: 0.8145 - val_loss: 0.3663 - val_auc: 0.7629\n",
      "\n",
      "Epoch 01472: val_loss improved from 0.36633 to 0.36632, saving model to DeepFM.h5\n",
      "Epoch 1473/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3014 - auc: 0.826 - ETA: 0s - loss: 0.3289 - auc: 0.813 - 0s 22us/step - loss: 0.3408 - auc: 0.8140 - val_loss: 0.3663 - val_auc: 0.7629\n",
      "\n",
      "Epoch 01473: val_loss improved from 0.36632 to 0.36631, saving model to DeepFM.h5\n",
      "Epoch 1474/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2920 - auc: 0.795 - ETA: 0s - loss: 0.3404 - auc: 0.812 - 0s 25us/step - loss: 0.3481 - auc: 0.8044 - val_loss: 0.3664 - val_auc: 0.7633\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 0.36631\n",
      "Epoch 1475/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3212 - auc: 0.779 - ETA: 0s - loss: 0.3388 - auc: 0.805 - 0s 25us/step - loss: 0.3446 - auc: 0.8066 - val_loss: 0.3663 - val_auc: 0.7633\n",
      "\n",
      "Epoch 01475: val_loss improved from 0.36631 to 0.36629, saving model to DeepFM.h5\n",
      "Epoch 1476/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3460 - auc: 0.804 - ETA: 0s - loss: 0.3459 - auc: 0.804 - 0s 23us/step - loss: 0.3382 - auc: 0.8205 - val_loss: 0.3665 - val_auc: 0.7642\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 0.36629\n",
      "Epoch 1477/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3461 - auc: 0.797 - ETA: 0s - loss: 0.3544 - auc: 0.807 - 0s 25us/step - loss: 0.3521 - auc: 0.7957 - val_loss: 0.3665 - val_auc: 0.7641\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 0.36629\n",
      "Epoch 1478/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2851 - auc: 0.803 - ETA: 0s - loss: 0.3460 - auc: 0.792 - 0s 25us/step - loss: 0.3411 - auc: 0.8137 - val_loss: 0.3663 - val_auc: 0.7630\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 0.36629\n",
      "Epoch 1479/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3010 - auc: 0.824 - ETA: 0s - loss: 0.3341 - auc: 0.819 - 0s 24us/step - loss: 0.3361 - auc: 0.8213 - val_loss: 0.3665 - val_auc: 0.7638\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 0.36629\n",
      "Epoch 1480/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2982 - auc: 0.847 - ETA: 0s - loss: 0.3428 - auc: 0.814 - 0s 23us/step - loss: 0.3381 - auc: 0.8150 - val_loss: 0.3664 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 0.36629\n",
      "Epoch 1481/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4005 - auc: 0.794 - ETA: 0s - loss: 0.3353 - auc: 0.831 - 0s 24us/step - loss: 0.3334 - auc: 0.8274 - val_loss: 0.3666 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 0.36629\n",
      "Epoch 1482/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3577 - auc: 0.846 - ETA: 0s - loss: 0.3328 - auc: 0.829 - 0s 22us/step - loss: 0.3359 - auc: 0.8235 - val_loss: 0.3664 - val_auc: 0.7635\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 0.36629\n",
      "Epoch 1483/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3482 - auc: 0.885 - ETA: 0s - loss: 0.3323 - auc: 0.828 - 0s 23us/step - loss: 0.3405 - auc: 0.8149 - val_loss: 0.3663 - val_auc: 0.7638\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 0.36629\n",
      "Epoch 1484/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2963 - auc: 0.814 - ETA: 0s - loss: 0.3324 - auc: 0.822 - 0s 22us/step - loss: 0.3357 - auc: 0.8201 - val_loss: 0.3661 - val_auc: 0.7641\n",
      "\n",
      "Epoch 01484: val_loss improved from 0.36629 to 0.36612, saving model to DeepFM.h5\n",
      "Epoch 1485/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3373 - auc: 0.869 - ETA: 0s - loss: 0.3426 - auc: 0.822 - 0s 24us/step - loss: 0.3389 - auc: 0.8158 - val_loss: 0.3663 - val_auc: 0.7645\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 0.36612\n",
      "Epoch 1486/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3497 - auc: 0.835 - ETA: 0s - loss: 0.3291 - auc: 0.820 - 0s 25us/step - loss: 0.3342 - auc: 0.8285 - val_loss: 0.3662 - val_auc: 0.7643\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 0.36612\n",
      "Epoch 1487/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3335 - auc: 0.870 - ETA: 0s - loss: 0.3532 - auc: 0.796 - 0s 25us/step - loss: 0.3464 - auc: 0.8018 - val_loss: 0.3662 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 0.36612\n",
      "Epoch 1488/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3383 - auc: 0.820 - ETA: 0s - loss: 0.3401 - auc: 0.802 - 0s 27us/step - loss: 0.3422 - auc: 0.8093 - val_loss: 0.3659 - val_auc: 0.7643\n",
      "\n",
      "Epoch 01488: val_loss improved from 0.36612 to 0.36594, saving model to DeepFM.h5\n",
      "Epoch 1489/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3371 - auc: 0.843 - ETA: 0s - loss: 0.3473 - auc: 0.802 - 0s 24us/step - loss: 0.3425 - auc: 0.8076 - val_loss: 0.3661 - val_auc: 0.7644\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 0.36594\n",
      "Epoch 1490/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3833 - auc: 0.772 - ETA: 0s - loss: 0.3425 - auc: 0.808 - 0s 23us/step - loss: 0.3437 - auc: 0.8125 - val_loss: 0.3659 - val_auc: 0.7647\n",
      "\n",
      "Epoch 01490: val_loss improved from 0.36594 to 0.36591, saving model to DeepFM.h5\n",
      "Epoch 1491/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3519 - auc: 0.805 - ETA: 0s - loss: 0.3387 - auc: 0.803 - 0s 24us/step - loss: 0.3417 - auc: 0.8077 - val_loss: 0.3660 - val_auc: 0.7655\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 0.36591\n",
      "Epoch 1492/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3176 - auc: 0.802 - ETA: 0s - loss: 0.3399 - auc: 0.812 - 0s 25us/step - loss: 0.3385 - auc: 0.8200 - val_loss: 0.3662 - val_auc: 0.7645\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 0.36591\n",
      "Epoch 1493/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3371 - auc: 0.837 - ETA: 0s - loss: 0.3372 - auc: 0.818 - 0s 23us/step - loss: 0.3382 - auc: 0.8199 - val_loss: 0.3662 - val_auc: 0.7641\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 0.36591\n",
      "Epoch 1494/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3598 - auc: 0.759 - ETA: 0s - loss: 0.3400 - auc: 0.820 - 0s 23us/step - loss: 0.3411 - auc: 0.8165 - val_loss: 0.3660 - val_auc: 0.7656\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 0.36591\n",
      "Epoch 1495/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3033 - auc: 0.846 - ETA: 0s - loss: 0.3361 - auc: 0.816 - 0s 25us/step - loss: 0.3369 - auc: 0.8212 - val_loss: 0.3660 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 0.36591\n",
      "Epoch 1496/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3373 - auc: 0.807 - ETA: 0s - loss: 0.3410 - auc: 0.819 - 0s 22us/step - loss: 0.3423 - auc: 0.8152 - val_loss: 0.3658 - val_auc: 0.7653\n",
      "\n",
      "Epoch 01496: val_loss improved from 0.36591 to 0.36576, saving model to DeepFM.h5\n",
      "Epoch 1497/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3164 - auc: 0.831 - ETA: 0s - loss: 0.3254 - auc: 0.816 - 0s 23us/step - loss: 0.3405 - auc: 0.8155 - val_loss: 0.3657 - val_auc: 0.7650\n",
      "\n",
      "Epoch 01497: val_loss improved from 0.36576 to 0.36571, saving model to DeepFM.h5\n",
      "Epoch 1498/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3706 - auc: 0.821 - ETA: 0s - loss: 0.3470 - auc: 0.817 - 0s 23us/step - loss: 0.3384 - auc: 0.8197 - val_loss: 0.3660 - val_auc: 0.7651\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 0.36571\n",
      "Epoch 1499/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3300 - auc: 0.838 - ETA: 0s - loss: 0.3292 - auc: 0.825 - 0s 23us/step - loss: 0.3390 - auc: 0.8177 - val_loss: 0.3657 - val_auc: 0.7651\n",
      "\n",
      "Epoch 01499: val_loss improved from 0.36571 to 0.36567, saving model to DeepFM.h5\n",
      "Epoch 1500/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3241 - auc: 0.863 - ETA: 0s - loss: 0.3429 - auc: 0.832 - 0s 23us/step - loss: 0.3376 - auc: 0.8216 - val_loss: 0.3659 - val_auc: 0.7645\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 0.36567\n",
      "Epoch 1501/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.768 - ETA: 0s - loss: 0.3459 - auc: 0.812 - 0s 23us/step - loss: 0.3411 - auc: 0.8143 - val_loss: 0.3658 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 0.36567\n",
      "Epoch 1502/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3293 - auc: 0.835 - ETA: 0s - loss: 0.3299 - auc: 0.826 - 0s 23us/step - loss: 0.3400 - auc: 0.8164 - val_loss: 0.3656 - val_auc: 0.7654\n",
      "\n",
      "Epoch 01502: val_loss improved from 0.36567 to 0.36557, saving model to DeepFM.h5\n",
      "Epoch 1503/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3196 - auc: 0.884 - ETA: 0s - loss: 0.3302 - auc: 0.833 - 0s 23us/step - loss: 0.3355 - auc: 0.8255 - val_loss: 0.3656 - val_auc: 0.7659\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 0.36557\n",
      "Epoch 1504/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3163 - auc: 0.812 - ETA: 0s - loss: 0.3472 - auc: 0.801 - 0s 23us/step - loss: 0.3416 - auc: 0.8171 - val_loss: 0.3655 - val_auc: 0.7651\n",
      "\n",
      "Epoch 01504: val_loss improved from 0.36557 to 0.36551, saving model to DeepFM.h5\n",
      "Epoch 1505/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3078 - auc: 0.872 - ETA: 0s - loss: 0.3363 - auc: 0.823 - 0s 23us/step - loss: 0.3375 - auc: 0.8181 - val_loss: 0.3657 - val_auc: 0.7641\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 0.36551\n",
      "Epoch 1506/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3064 - auc: 0.805 - ETA: 0s - loss: 0.3340 - auc: 0.814 - 0s 23us/step - loss: 0.3340 - auc: 0.8250 - val_loss: 0.3658 - val_auc: 0.7643\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 0.36551\n",
      "Epoch 1507/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3254 - auc: 0.844 - ETA: 0s - loss: 0.3443 - auc: 0.814 - 0s 24us/step - loss: 0.3397 - auc: 0.8196 - val_loss: 0.3659 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 0.36551\n",
      "Epoch 1508/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3208 - auc: 0.778 - ETA: 0s - loss: 0.3420 - auc: 0.811 - 0s 23us/step - loss: 0.3415 - auc: 0.8101 - val_loss: 0.3656 - val_auc: 0.7657\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 0.36551\n",
      "Epoch 1509/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3362 - auc: 0.742 - ETA: 0s - loss: 0.3400 - auc: 0.807 - 0s 24us/step - loss: 0.3396 - auc: 0.8143 - val_loss: 0.3657 - val_auc: 0.7643\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 0.36551\n",
      "Epoch 1510/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3372 - auc: 0.847 - ETA: 0s - loss: 0.3392 - auc: 0.812 - 0s 22us/step - loss: 0.3388 - auc: 0.8127 - val_loss: 0.3657 - val_auc: 0.7636\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 0.36551\n",
      "Epoch 1511/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3374 - auc: 0.806 - ETA: 0s - loss: 0.3412 - auc: 0.815 - 0s 23us/step - loss: 0.3426 - auc: 0.8101 - val_loss: 0.3655 - val_auc: 0.7644\n",
      "\n",
      "Epoch 01511: val_loss improved from 0.36551 to 0.36550, saving model to DeepFM.h5\n",
      "Epoch 1512/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3035 - auc: 0.835 - ETA: 0s - loss: 0.3426 - auc: 0.807 - 0s 23us/step - loss: 0.3359 - auc: 0.8192 - val_loss: 0.3656 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 0.36550\n",
      "Epoch 1513/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3540 - auc: 0.867 - ETA: 0s - loss: 0.3449 - auc: 0.814 - 0s 23us/step - loss: 0.3393 - auc: 0.8157 - val_loss: 0.3656 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 0.36550\n",
      "Epoch 1514/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3160 - auc: 0.842 - ETA: 0s - loss: 0.3405 - auc: 0.815 - 0s 23us/step - loss: 0.3363 - auc: 0.8240 - val_loss: 0.3660 - val_auc: 0.7640\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 0.36550\n",
      "Epoch 1515/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2868 - auc: 0.863 - ETA: 0s - loss: 0.3384 - auc: 0.813 - 0s 25us/step - loss: 0.3384 - auc: 0.8169 - val_loss: 0.3656 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 0.36550\n",
      "Epoch 1516/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3184 - auc: 0.815 - ETA: 0s - loss: 0.3423 - auc: 0.820 - 0s 23us/step - loss: 0.3379 - auc: 0.8206 - val_loss: 0.3658 - val_auc: 0.7642\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 0.36550\n",
      "Epoch 1517/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3635 - auc: 0.771 - ETA: 0s - loss: 0.3441 - auc: 0.806 - 0s 21us/step - loss: 0.3450 - auc: 0.8040 - val_loss: 0.3656 - val_auc: 0.7643\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 0.36550\n",
      "Epoch 1518/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3587 - auc: 0.760 - ETA: 0s - loss: 0.3496 - auc: 0.808 - 0s 21us/step - loss: 0.3424 - auc: 0.8107 - val_loss: 0.3657 - val_auc: 0.7642\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 0.36550\n",
      "Epoch 1519/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3112 - auc: 0.800 - ETA: 0s - loss: 0.3313 - auc: 0.818 - 0s 22us/step - loss: 0.3411 - auc: 0.8120 - val_loss: 0.3655 - val_auc: 0.7650\n",
      "\n",
      "Epoch 01519: val_loss improved from 0.36550 to 0.36548, saving model to DeepFM.h5\n",
      "Epoch 1520/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2937 - auc: 0.864 - ETA: 0s - loss: 0.3307 - auc: 0.820 - 0s 22us/step - loss: 0.3344 - auc: 0.8228 - val_loss: 0.3656 - val_auc: 0.7637\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 0.36548\n",
      "Epoch 1521/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3979 - auc: 0.794 - ETA: 0s - loss: 0.3380 - auc: 0.818 - 0s 24us/step - loss: 0.3409 - auc: 0.8136 - val_loss: 0.3657 - val_auc: 0.7634\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 0.36548\n",
      "Epoch 1522/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3296 - auc: 0.816 - ETA: 0s - loss: 0.3381 - auc: 0.822 - 0s 23us/step - loss: 0.3351 - auc: 0.8215 - val_loss: 0.3656 - val_auc: 0.7642\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 0.36548\n",
      "Epoch 1523/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3401 - auc: 0.746 - ETA: 0s - loss: 0.3431 - auc: 0.807 - 0s 23us/step - loss: 0.3418 - auc: 0.8107 - val_loss: 0.3656 - val_auc: 0.7637\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 0.36548\n",
      "Epoch 1524/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3675 - auc: 0.795 - ETA: 0s - loss: 0.3476 - auc: 0.810 - 0s 28us/step - loss: 0.3392 - auc: 0.8122 - val_loss: 0.3658 - val_auc: 0.7642\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 0.36548\n",
      "Epoch 1525/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3740 - auc: 0.747 - ETA: 0s - loss: 0.3428 - auc: 0.809 - 0s 23us/step - loss: 0.3388 - auc: 0.8147 - val_loss: 0.3659 - val_auc: 0.7651\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 0.36548\n",
      "Epoch 1526/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3477 - auc: 0.827 - ETA: 0s - loss: 0.3265 - auc: 0.834 - 0s 23us/step - loss: 0.3324 - auc: 0.8273 - val_loss: 0.3659 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 0.36548\n",
      "Epoch 1527/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3781 - auc: 0.754 - ETA: 0s - loss: 0.3526 - auc: 0.801 - 0s 25us/step - loss: 0.3393 - auc: 0.8175 - val_loss: 0.3659 - val_auc: 0.7647\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 0.36548\n",
      "Epoch 1528/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3788 - auc: 0.789 - ETA: 0s - loss: 0.3268 - auc: 0.831 - 0s 23us/step - loss: 0.3348 - auc: 0.8248 - val_loss: 0.3657 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 0.36548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1529/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3558 - auc: 0.779 - ETA: 0s - loss: 0.3273 - auc: 0.840 - 0s 23us/step - loss: 0.3347 - auc: 0.8253 - val_loss: 0.3655 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01529: val_loss improved from 0.36548 to 0.36546, saving model to DeepFM.h5\n",
      "Epoch 1530/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3292 - auc: 0.843 - ETA: 0s - loss: 0.3385 - auc: 0.806 - 0s 22us/step - loss: 0.3428 - auc: 0.8072 - val_loss: 0.3657 - val_auc: 0.7653\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 0.36546\n",
      "Epoch 1531/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3456 - auc: 0.822 - ETA: 0s - loss: 0.3402 - auc: 0.804 - 0s 21us/step - loss: 0.3422 - auc: 0.8088 - val_loss: 0.3656 - val_auc: 0.7644\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 0.36546\n",
      "Epoch 1532/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3093 - auc: 0.827 - ETA: 0s - loss: 0.3273 - auc: 0.820 - 0s 23us/step - loss: 0.3332 - auc: 0.8257 - val_loss: 0.3658 - val_auc: 0.7647\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 0.36546\n",
      "Epoch 1533/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3820 - auc: 0.791 - ETA: 0s - loss: 0.3403 - auc: 0.814 - 0s 22us/step - loss: 0.3387 - auc: 0.8166 - val_loss: 0.3658 - val_auc: 0.7649\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 0.36546\n",
      "Epoch 1534/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3422 - auc: 0.865 - ETA: 0s - loss: 0.3347 - auc: 0.825 - 0s 22us/step - loss: 0.3358 - auc: 0.8231 - val_loss: 0.3658 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 0.36546\n",
      "Epoch 1535/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3422 - auc: 0.760 - ETA: 0s - loss: 0.3325 - auc: 0.810 - 0s 23us/step - loss: 0.3431 - auc: 0.8067 - val_loss: 0.3654 - val_auc: 0.7638\n",
      "\n",
      "Epoch 01535: val_loss improved from 0.36546 to 0.36541, saving model to DeepFM.h5\n",
      "Epoch 1536/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3416 - auc: 0.798 - ETA: 0s - loss: 0.3240 - auc: 0.830 - 0s 22us/step - loss: 0.3339 - auc: 0.8264 - val_loss: 0.3656 - val_auc: 0.7656\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 0.36541\n",
      "Epoch 1537/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3532 - auc: 0.834 - ETA: 0s - loss: 0.3404 - auc: 0.811 - 0s 22us/step - loss: 0.3373 - auc: 0.8197 - val_loss: 0.3654 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01537: val_loss improved from 0.36541 to 0.36538, saving model to DeepFM.h5\n",
      "Epoch 1538/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3482 - auc: 0.849 - ETA: 0s - loss: 0.3357 - auc: 0.814 - 0s 23us/step - loss: 0.3397 - auc: 0.8159 - val_loss: 0.3655 - val_auc: 0.7655\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 0.36538\n",
      "Epoch 1539/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2926 - auc: 0.841 - ETA: 0s - loss: 0.3149 - auc: 0.841 - 0s 23us/step - loss: 0.3304 - auc: 0.8270 - val_loss: 0.3656 - val_auc: 0.7656\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 0.36538\n",
      "Epoch 1540/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3812 - auc: 0.764 - ETA: 0s - loss: 0.3345 - auc: 0.826 - 0s 23us/step - loss: 0.3367 - auc: 0.8231 - val_loss: 0.3655 - val_auc: 0.7653\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 0.36538\n",
      "Epoch 1541/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3137 - auc: 0.748 - ETA: 0s - loss: 0.3239 - auc: 0.828 - 0s 23us/step - loss: 0.3366 - auc: 0.8214 - val_loss: 0.3653 - val_auc: 0.7650\n",
      "\n",
      "Epoch 01541: val_loss improved from 0.36538 to 0.36532, saving model to DeepFM.h5\n",
      "Epoch 1542/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2925 - auc: 0.834 - ETA: 0s - loss: 0.3371 - auc: 0.826 - 0s 21us/step - loss: 0.3362 - auc: 0.8200 - val_loss: 0.3656 - val_auc: 0.7656\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 0.36532\n",
      "Epoch 1543/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3514 - auc: 0.857 - ETA: 0s - loss: 0.3363 - auc: 0.837 - 0s 23us/step - loss: 0.3315 - auc: 0.8320 - val_loss: 0.3657 - val_auc: 0.7650\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 0.36532\n",
      "Epoch 1544/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3202 - auc: 0.812 - ETA: 0s - loss: 0.3368 - auc: 0.830 - 0s 22us/step - loss: 0.3338 - auc: 0.8290 - val_loss: 0.3655 - val_auc: 0.7656\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 0.36532\n",
      "Epoch 1545/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3321 - auc: 0.826 - ETA: 0s - loss: 0.3402 - auc: 0.818 - 0s 22us/step - loss: 0.3386 - auc: 0.8183 - val_loss: 0.3653 - val_auc: 0.7652\n",
      "\n",
      "Epoch 01545: val_loss improved from 0.36532 to 0.36529, saving model to DeepFM.h5\n",
      "Epoch 1546/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3355 - auc: 0.840 - ETA: 0s - loss: 0.3324 - auc: 0.823 - 0s 23us/step - loss: 0.3345 - auc: 0.8301 - val_loss: 0.3652 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01546: val_loss improved from 0.36529 to 0.36523, saving model to DeepFM.h5\n",
      "Epoch 1547/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3249 - auc: 0.825 - ETA: 0s - loss: 0.3363 - auc: 0.829 - 0s 24us/step - loss: 0.3339 - auc: 0.8232 - val_loss: 0.3655 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 0.36523\n",
      "Epoch 1548/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4237 - auc: 0.758 - ETA: 0s - loss: 0.3477 - auc: 0.818 - 0s 22us/step - loss: 0.3361 - auc: 0.8268 - val_loss: 0.3658 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 0.36523\n",
      "Epoch 1549/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3053 - auc: 0.861 - ETA: 0s - loss: 0.3392 - auc: 0.819 - 0s 22us/step - loss: 0.3412 - auc: 0.8119 - val_loss: 0.3655 - val_auc: 0.7647\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 0.36523\n",
      "Epoch 1550/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2848 - auc: 0.819 - ETA: 0s - loss: 0.3381 - auc: 0.816 - 0s 22us/step - loss: 0.3382 - auc: 0.8170 - val_loss: 0.3655 - val_auc: 0.7649\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 0.36523\n",
      "Epoch 1551/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3025 - auc: 0.843 - ETA: 0s - loss: 0.3309 - auc: 0.829 - 0s 24us/step - loss: 0.3291 - auc: 0.8336 - val_loss: 0.3658 - val_auc: 0.7644\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 0.36523\n",
      "Epoch 1552/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3538 - auc: 0.815 - ETA: 0s - loss: 0.3472 - auc: 0.815 - 0s 23us/step - loss: 0.3364 - auc: 0.8215 - val_loss: 0.3657 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 0.36523\n",
      "Epoch 1553/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3813 - auc: 0.808 - ETA: 0s - loss: 0.3474 - auc: 0.825 - 0s 22us/step - loss: 0.3323 - auc: 0.8279 - val_loss: 0.3659 - val_auc: 0.7645\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 0.36523\n",
      "Epoch 1554/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3363 - auc: 0.834 - ETA: 0s - loss: 0.3373 - auc: 0.822 - 0s 22us/step - loss: 0.3363 - auc: 0.8206 - val_loss: 0.3658 - val_auc: 0.7643\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 0.36523\n",
      "Epoch 1555/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3067 - auc: 0.788 - ETA: 0s - loss: 0.3311 - auc: 0.807 - 0s 22us/step - loss: 0.3391 - auc: 0.8126 - val_loss: 0.3653 - val_auc: 0.7649\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 0.36523\n",
      "Epoch 1556/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3774 - auc: 0.793 - ETA: 0s - loss: 0.3357 - auc: 0.824 - 0s 23us/step - loss: 0.3338 - auc: 0.8290 - val_loss: 0.3654 - val_auc: 0.7641\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 0.36523\n",
      "Epoch 1557/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3367 - auc: 0.829 - ETA: 0s - loss: 0.3333 - auc: 0.832 - 0s 25us/step - loss: 0.3371 - auc: 0.8172 - val_loss: 0.3652 - val_auc: 0.7652\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 0.36523\n",
      "Epoch 1558/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3242 - auc: 0.823 - ETA: 0s - loss: 0.3449 - auc: 0.813 - 0s 23us/step - loss: 0.3395 - auc: 0.8174 - val_loss: 0.3651 - val_auc: 0.7643\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01558: val_loss improved from 0.36523 to 0.36514, saving model to DeepFM.h5\n",
      "Epoch 1559/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3019 - auc: 0.848 - ETA: 0s - loss: 0.3341 - auc: 0.822 - 0s 22us/step - loss: 0.3356 - auc: 0.8229 - val_loss: 0.3653 - val_auc: 0.7653\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 0.36514\n",
      "Epoch 1560/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3088 - auc: 0.787 - ETA: 0s - loss: 0.3280 - auc: 0.839 - 0s 22us/step - loss: 0.3275 - auc: 0.8328 - val_loss: 0.3658 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01560: val_loss did not improve from 0.36514\n",
      "Epoch 1561/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3448 - auc: 0.811 - ETA: 0s - loss: 0.3266 - auc: 0.832 - 0s 22us/step - loss: 0.3318 - auc: 0.8298 - val_loss: 0.3656 - val_auc: 0.7642\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 0.36514\n",
      "Epoch 1562/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2955 - auc: 0.842 - ETA: 0s - loss: 0.3204 - auc: 0.832 - 0s 23us/step - loss: 0.3326 - auc: 0.8282 - val_loss: 0.3653 - val_auc: 0.7644\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 0.36514\n",
      "Epoch 1563/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3309 - auc: 0.855 - ETA: 0s - loss: 0.3403 - auc: 0.820 - 0s 23us/step - loss: 0.3326 - auc: 0.8227 - val_loss: 0.3656 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 0.36514\n",
      "Epoch 1564/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.775 - ETA: 0s - loss: 0.3494 - auc: 0.816 - 0s 23us/step - loss: 0.3394 - auc: 0.8181 - val_loss: 0.3657 - val_auc: 0.7651\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 0.36514\n",
      "Epoch 1565/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3943 - auc: 0.799 - ETA: 0s - loss: 0.3458 - auc: 0.821 - 0s 25us/step - loss: 0.3388 - auc: 0.8174 - val_loss: 0.3657 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 0.36514\n",
      "Epoch 1566/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3464 - auc: 0.848 - ETA: 0s - loss: 0.3379 - auc: 0.832 - 0s 23us/step - loss: 0.3308 - auc: 0.8314 - val_loss: 0.3661 - val_auc: 0.7640\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 0.36514\n",
      "Epoch 1567/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3132 - auc: 0.804 - ETA: 0s - loss: 0.3356 - auc: 0.823 - 0s 23us/step - loss: 0.3328 - auc: 0.8285 - val_loss: 0.3655 - val_auc: 0.7641\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 0.36514\n",
      "Epoch 1568/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3501 - auc: 0.868 - ETA: 0s - loss: 0.3315 - auc: 0.828 - 0s 23us/step - loss: 0.3281 - auc: 0.8325 - val_loss: 0.3655 - val_auc: 0.7646\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 0.36514\n",
      "Epoch 1569/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3066 - auc: 0.804 - ETA: 0s - loss: 0.3370 - auc: 0.821 - 0s 23us/step - loss: 0.3390 - auc: 0.8162 - val_loss: 0.3655 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 0.36514\n",
      "Epoch 1570/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3469 - auc: 0.853 - ETA: 0s - loss: 0.3368 - auc: 0.821 - 0s 22us/step - loss: 0.3333 - auc: 0.8237 - val_loss: 0.3656 - val_auc: 0.7650\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 0.36514\n",
      "Epoch 1571/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3556 - auc: 0.834 - ETA: 0s - loss: 0.3389 - auc: 0.813 - 0s 23us/step - loss: 0.3359 - auc: 0.8188 - val_loss: 0.3655 - val_auc: 0.7655\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 0.36514\n",
      "Epoch 1572/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3406 - auc: 0.788 - ETA: 0s - loss: 0.3281 - auc: 0.842 - 0s 23us/step - loss: 0.3286 - auc: 0.8341 - val_loss: 0.3656 - val_auc: 0.7647\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 0.36514\n",
      "Epoch 1573/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2916 - auc: 0.863 - ETA: 0s - loss: 0.3310 - auc: 0.823 - 0s 23us/step - loss: 0.3354 - auc: 0.8247 - val_loss: 0.3654 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 0.36514\n",
      "Epoch 1574/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3453 - auc: 0.808 - ETA: 0s - loss: 0.3404 - auc: 0.817 - 0s 23us/step - loss: 0.3350 - auc: 0.8251 - val_loss: 0.3656 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 0.36514\n",
      "Epoch 1575/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3117 - auc: 0.771 - ETA: 0s - loss: 0.3220 - auc: 0.834 - 0s 22us/step - loss: 0.3314 - auc: 0.8339 - val_loss: 0.3654 - val_auc: 0.7650\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 0.36514\n",
      "Epoch 1576/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2751 - auc: 0.807 - ETA: 0s - loss: 0.3377 - auc: 0.825 - 0s 24us/step - loss: 0.3342 - auc: 0.8265 - val_loss: 0.3657 - val_auc: 0.7639\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 0.36514\n",
      "Epoch 1577/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3002 - auc: 0.840 - ETA: 0s - loss: 0.3258 - auc: 0.830 - 0s 23us/step - loss: 0.3314 - auc: 0.8279 - val_loss: 0.3654 - val_auc: 0.7656\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 0.36514\n",
      "Epoch 1578/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3342 - auc: 0.793 - ETA: 0s - loss: 0.3415 - auc: 0.818 - 0s 23us/step - loss: 0.3315 - auc: 0.8266 - val_loss: 0.3658 - val_auc: 0.7648\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 0.36514\n"
     ]
    }
   ],
   "source": [
    "epochs = 4000\n",
    "batch_size = 256\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='DeepFM.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "#train_input = [train_category[:,i] for i in range(train_category.shape[1])]+[train_continue]\n",
    "train_input = [train_category[:,i] for i in range(train_category.shape[1])]+[train_matchTags]+[train_continue]\n",
    "hist = model.fit(train_input,train_label,epochs = epochs, batch_size=batch_size,validation_split=0.15, callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(196, 18, input_length=1, name=\"2nd_embed_userID\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(733, 18, input_length=1, name=\"2nd_embed_jobID\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(276, 18, input_length=1, name=\"2nd_embed_companyID\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(2, 18, input_length=1, name=\"2nd_embed_anyMatch\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(8, 18, input_length=1, name=\"2nd_embed_companySize\", embeddings_regularizer=<keras.reg...)`\n",
      "  \n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(196, 1, input_length=1, name=\"1st_embed_userID\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(733, 1, input_length=1, name=\"1st_embed_jobID\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(276, 1, input_length=1, name=\"1st_embed_companyID\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(2, 1, input_length=1, name=\"1st_embed_anyMatch\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(8, 1, input_length=1, name=\"1st_embed_companySize\", embeddings_regularizer=<keras.reg...)`\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\moon\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4335 samples, validate on 765 samples\n",
      "Epoch 1/4000\n",
      "4335/4335 [==============================] - ETA: 9s - loss: 0.7098 - auc: 0.460 - ETA: 0s - loss: 0.6974 - auc: 0.491 - 1s 189us/step - loss: 0.6915 - auc: 0.4858 - val_loss: 0.6733 - val_auc: 0.4668\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67328, saving model to DeepFM.h5\n",
      "Epoch 2/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.6748 - auc: 0.483 - ETA: 0s - loss: 0.6634 - auc: 0.507 - 0s 23us/step - loss: 0.6564 - auc: 0.5163 - val_loss: 0.6384 - val_auc: 0.4741\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67328 to 0.63841, saving model to DeepFM.h5\n",
      "Epoch 3/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.6303 - auc: 0.533 - ETA: 0s - loss: 0.6304 - auc: 0.506 - 0s 22us/step - loss: 0.6239 - auc: 0.5097 - val_loss: 0.6066 - val_auc: 0.4783\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63841 to 0.60664, saving model to DeepFM.h5\n",
      "Epoch 4/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.6172 - auc: 0.495 - ETA: 0s - loss: 0.6000 - auc: 0.513 - 0s 22us/step - loss: 0.5954 - auc: 0.5102 - val_loss: 0.5786 - val_auc: 0.4793\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60664 to 0.57860, saving model to DeepFM.h5\n",
      "Epoch 5/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5742 - auc: 0.483 - ETA: 0s - loss: 0.5757 - auc: 0.514 - 0s 23us/step - loss: 0.5694 - auc: 0.5156 - val_loss: 0.5543 - val_auc: 0.4752\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.57860 to 0.55426, saving model to DeepFM.h5\n",
      "Epoch 6/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5372 - auc: 0.477 - ETA: 0s - loss: 0.5523 - auc: 0.514 - 0s 21us/step - loss: 0.5472 - auc: 0.5228 - val_loss: 0.5335 - val_auc: 0.4696\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.55426 to 0.53351, saving model to DeepFM.h5\n",
      "Epoch 7/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5273 - auc: 0.521 - ETA: 0s - loss: 0.5345 - auc: 0.506 - 0s 20us/step - loss: 0.5278 - auc: 0.5277 - val_loss: 0.5160 - val_auc: 0.4749\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53351 to 0.51597, saving model to DeepFM.h5\n",
      "Epoch 8/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4995 - auc: 0.577 - ETA: 0s - loss: 0.5155 - auc: 0.493 - 0s 24us/step - loss: 0.5123 - auc: 0.4994 - val_loss: 0.5013 - val_auc: 0.4629\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51597 to 0.50129, saving model to DeepFM.h5\n",
      "Epoch 9/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5341 - auc: 0.530 - ETA: 0s - loss: 0.5000 - auc: 0.525 - 0s 22us/step - loss: 0.4978 - auc: 0.5181 - val_loss: 0.4889 - val_auc: 0.4666\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50129 to 0.48888, saving model to DeepFM.h5\n",
      "Epoch 10/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4937 - auc: 0.502 - ETA: 0s - loss: 0.4829 - auc: 0.488 - 0s 21us/step - loss: 0.4883 - auc: 0.4793 - val_loss: 0.4786 - val_auc: 0.4585\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48888 to 0.47865, saving model to DeepFM.h5\n",
      "Epoch 11/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4727 - auc: 0.456 - ETA: 0s - loss: 0.4820 - auc: 0.497 - 0s 23us/step - loss: 0.4776 - auc: 0.5093 - val_loss: 0.4700 - val_auc: 0.4550\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47865 to 0.47000, saving model to DeepFM.h5\n",
      "Epoch 12/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4839 - auc: 0.464 - ETA: 0s - loss: 0.4724 - auc: 0.497 - 0s 21us/step - loss: 0.4700 - auc: 0.5033 - val_loss: 0.4628 - val_auc: 0.4631\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47000 to 0.46283, saving model to DeepFM.h5\n",
      "Epoch 13/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4515 - auc: 0.469 - ETA: 0s - loss: 0.4674 - auc: 0.482 - 0s 28us/step - loss: 0.4639 - auc: 0.4992 - val_loss: 0.4568 - val_auc: 0.4230\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.46283 to 0.45684, saving model to DeepFM.h5\n",
      "Epoch 14/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4524 - auc: 0.465 - ETA: 0s - loss: 0.4588 - auc: 0.500 - 0s 23us/step - loss: 0.4588 - auc: 0.4844 - val_loss: 0.4519 - val_auc: 0.4476\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.45684 to 0.45193, saving model to DeepFM.h5\n",
      "Epoch 15/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4998 - auc: 0.483 - ETA: 0s - loss: 0.4507 - auc: 0.519 - 0s 21us/step - loss: 0.4528 - auc: 0.5213 - val_loss: 0.4478 - val_auc: 0.4562\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.45193 to 0.44780, saving model to DeepFM.h5\n",
      "Epoch 16/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4857 - auc: 0.496 - ETA: 0s - loss: 0.4491 - auc: 0.522 - 0s 21us/step - loss: 0.4485 - auc: 0.5246 - val_loss: 0.4444 - val_auc: 0.4768\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.44780 to 0.44437, saving model to DeepFM.h5\n",
      "Epoch 17/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4417 - auc: 0.451 - ETA: 0s - loss: 0.4453 - auc: 0.493 - 0s 19us/step - loss: 0.4469 - auc: 0.4963 - val_loss: 0.4416 - val_auc: 0.4630\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.44437 to 0.44160, saving model to DeepFM.h5\n",
      "Epoch 18/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4476 - auc: 0.502 - ETA: 0s - loss: 0.4359 - auc: 0.479 - 0s 21us/step - loss: 0.4450 - auc: 0.4898 - val_loss: 0.4393 - val_auc: 0.4682\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.44160 to 0.43933, saving model to DeepFM.h5\n",
      "Epoch 19/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5039 - auc: 0.511 - ETA: 0s - loss: 0.4479 - auc: 0.485 - 0s 21us/step - loss: 0.4427 - auc: 0.4935 - val_loss: 0.4375 - val_auc: 0.4425\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.43933 to 0.43745, saving model to DeepFM.h5\n",
      "Epoch 20/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4535 - auc: 0.506 - ETA: 0s - loss: 0.4386 - auc: 0.529 - 0s 20us/step - loss: 0.4388 - auc: 0.5262 - val_loss: 0.4359 - val_auc: 0.4503\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.43745 to 0.43585, saving model to DeepFM.h5\n",
      "Epoch 21/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4483 - auc: 0.400 - ETA: 0s - loss: 0.4359 - auc: 0.503 - 0s 20us/step - loss: 0.4396 - auc: 0.4925 - val_loss: 0.4346 - val_auc: 0.4702\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.43585 to 0.43457, saving model to DeepFM.h5\n",
      "Epoch 22/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3706 - auc: 0.441 - ETA: 0s - loss: 0.4439 - auc: 0.521 - 0s 23us/step - loss: 0.4374 - auc: 0.5049 - val_loss: 0.4335 - val_auc: 0.4805\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.43457 to 0.43349, saving model to DeepFM.h5\n",
      "Epoch 23/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3975 - auc: 0.414 - ETA: 0s - loss: 0.4423 - auc: 0.507 - 0s 24us/step - loss: 0.4369 - auc: 0.5081 - val_loss: 0.4326 - val_auc: 0.4857\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.43349 to 0.43259, saving model to DeepFM.h5\n",
      "Epoch 24/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4177 - auc: 0.466 - ETA: 0s - loss: 0.4295 - auc: 0.519 - 0s 20us/step - loss: 0.4351 - auc: 0.5183 - val_loss: 0.4319 - val_auc: 0.4544\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.43259 to 0.43186, saving model to DeepFM.h5\n",
      "Epoch 25/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4608 - auc: 0.526 - ETA: 0s - loss: 0.4369 - auc: 0.494 - 0s 20us/step - loss: 0.4363 - auc: 0.4954 - val_loss: 0.4313 - val_auc: 0.4744\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.43186 to 0.43128, saving model to DeepFM.h5\n",
      "Epoch 26/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4374 - auc: 0.589 - ETA: 0s - loss: 0.4328 - auc: 0.532 - 0s 20us/step - loss: 0.4336 - auc: 0.5221 - val_loss: 0.4308 - val_auc: 0.4813\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.43128 to 0.43078, saving model to DeepFM.h5\n",
      "Epoch 27/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4210 - auc: 0.619 - ETA: 0s - loss: 0.4432 - auc: 0.496 - 0s 20us/step - loss: 0.4353 - auc: 0.4930 - val_loss: 0.4304 - val_auc: 0.4887\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.43078 to 0.43038, saving model to DeepFM.h5\n",
      "Epoch 28/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3929 - auc: 0.471 - ETA: 0s - loss: 0.4267 - auc: 0.517 - 0s 21us/step - loss: 0.4323 - auc: 0.5274 - val_loss: 0.4300 - val_auc: 0.4632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: val_loss improved from 0.43038 to 0.43003, saving model to DeepFM.h5\n",
      "Epoch 29/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3841 - auc: 0.512 - ETA: 0s - loss: 0.4327 - auc: 0.499 - 0s 19us/step - loss: 0.4338 - auc: 0.5059 - val_loss: 0.4298 - val_auc: 0.4849\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.43003 to 0.42976, saving model to DeepFM.h5\n",
      "Epoch 30/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4594 - auc: 0.522 - ETA: 0s - loss: 0.4409 - auc: 0.518 - 0s 20us/step - loss: 0.4325 - auc: 0.5162 - val_loss: 0.4295 - val_auc: 0.4931\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.42976 to 0.42952, saving model to DeepFM.h5\n",
      "Epoch 31/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4697 - auc: 0.551 - ETA: 0s - loss: 0.4410 - auc: 0.518 - 0s 20us/step - loss: 0.4319 - auc: 0.5219 - val_loss: 0.4293 - val_auc: 0.4834\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.42952 to 0.42931, saving model to DeepFM.h5\n",
      "Epoch 32/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4445 - auc: 0.452 - ETA: 0s - loss: 0.4366 - auc: 0.479 - 0s 20us/step - loss: 0.4350 - auc: 0.4826 - val_loss: 0.4292 - val_auc: 0.4905\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.42931 to 0.42917, saving model to DeepFM.h5\n",
      "Epoch 33/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3732 - auc: 0.513 - ETA: 0s - loss: 0.4313 - auc: 0.516 - 0s 21us/step - loss: 0.4333 - auc: 0.5000 - val_loss: 0.4290 - val_auc: 0.4908\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.42917 to 0.42903, saving model to DeepFM.h5\n",
      "Epoch 34/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3512 - auc: 0.343 - ETA: 0s - loss: 0.4287 - auc: 0.504 - 0s 21us/step - loss: 0.4328 - auc: 0.5073 - val_loss: 0.4289 - val_auc: 0.5034\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.42903 to 0.42892, saving model to DeepFM.h5\n",
      "Epoch 35/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4075 - auc: 0.510 - ETA: 0s - loss: 0.4406 - auc: 0.513 - 0s 21us/step - loss: 0.4325 - auc: 0.5146 - val_loss: 0.4288 - val_auc: 0.4837\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.42892 to 0.42881, saving model to DeepFM.h5\n",
      "Epoch 36/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3744 - auc: 0.509 - ETA: 0s - loss: 0.4358 - auc: 0.500 - 0s 20us/step - loss: 0.4321 - auc: 0.5117 - val_loss: 0.4287 - val_auc: 0.4954\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.42881 to 0.42872, saving model to DeepFM.h5\n",
      "Epoch 37/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3797 - auc: 0.523 - ETA: 0s - loss: 0.4420 - auc: 0.509 - 0s 20us/step - loss: 0.4322 - auc: 0.5103 - val_loss: 0.4286 - val_auc: 0.4937\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.42872 to 0.42864, saving model to DeepFM.h5\n",
      "Epoch 38/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4433 - auc: 0.624 - ETA: 0s - loss: 0.4307 - auc: 0.512 - 0s 25us/step - loss: 0.4316 - auc: 0.5183 - val_loss: 0.4286 - val_auc: 0.4913\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.42864 to 0.42857, saving model to DeepFM.h5\n",
      "Epoch 39/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4131 - auc: 0.525 - ETA: 0s - loss: 0.4320 - auc: 0.485 - 0s 21us/step - loss: 0.4335 - auc: 0.4883 - val_loss: 0.4285 - val_auc: 0.4971\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.42857 to 0.42851, saving model to DeepFM.h5\n",
      "Epoch 40/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4059 - auc: 0.612 - ETA: 0s - loss: 0.4352 - auc: 0.525 - 0s 22us/step - loss: 0.4310 - auc: 0.5214 - val_loss: 0.4285 - val_auc: 0.5225\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.42851 to 0.42845, saving model to DeepFM.h5\n",
      "Epoch 41/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4170 - auc: 0.540 - ETA: 0s - loss: 0.4330 - auc: 0.485 - 0s 20us/step - loss: 0.4336 - auc: 0.4875 - val_loss: 0.4284 - val_auc: 0.5036\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.42845 to 0.42841, saving model to DeepFM.h5\n",
      "Epoch 42/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3743 - auc: 0.488 - ETA: 0s - loss: 0.4362 - auc: 0.501 - 0s 21us/step - loss: 0.4333 - auc: 0.4936 - val_loss: 0.4284 - val_auc: 0.4943\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.42841 to 0.42836, saving model to DeepFM.h5\n",
      "Epoch 43/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4311 - auc: 0.465 - ETA: 0s - loss: 0.4260 - auc: 0.531 - 0s 28us/step - loss: 0.4307 - auc: 0.5241 - val_loss: 0.4283 - val_auc: 0.4867\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.42836 to 0.42831, saving model to DeepFM.h5\n",
      "Epoch 44/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4567 - auc: 0.458 - ETA: 0s - loss: 0.4269 - auc: 0.498 - 0s 22us/step - loss: 0.4330 - auc: 0.4964 - val_loss: 0.4283 - val_auc: 0.4921\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.42831 to 0.42827, saving model to DeepFM.h5\n",
      "Epoch 45/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4137 - auc: 0.503 - ETA: 0s - loss: 0.4277 - auc: 0.509 - 0s 23us/step - loss: 0.4313 - auc: 0.5172 - val_loss: 0.4282 - val_auc: 0.4849\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.42827 to 0.42823, saving model to DeepFM.h5\n",
      "Epoch 46/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3444 - auc: 0.525 - ETA: 0s - loss: 0.4266 - auc: 0.509 - 0s 20us/step - loss: 0.4322 - auc: 0.5056 - val_loss: 0.4282 - val_auc: 0.4799\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.42823 to 0.42820, saving model to DeepFM.h5\n",
      "Epoch 47/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4019 - auc: 0.542 - ETA: 0s - loss: 0.4254 - auc: 0.519 - 0s 21us/step - loss: 0.4314 - auc: 0.5119 - val_loss: 0.4282 - val_auc: 0.4941\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.42820 to 0.42816, saving model to DeepFM.h5\n",
      "Epoch 48/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4436 - auc: 0.540 - ETA: 0s - loss: 0.4277 - auc: 0.517 - 0s 22us/step - loss: 0.4311 - auc: 0.5136 - val_loss: 0.4281 - val_auc: 0.4974\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.42816 to 0.42813, saving model to DeepFM.h5\n",
      "Epoch 49/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5224 - auc: 0.492 - ETA: 0s - loss: 0.4281 - auc: 0.514 - 0s 21us/step - loss: 0.4323 - auc: 0.5066 - val_loss: 0.4281 - val_auc: 0.5089\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.42813 to 0.42809, saving model to DeepFM.h5\n",
      "Epoch 50/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4222 - auc: 0.558 - ETA: 0s - loss: 0.4314 - auc: 0.534 - 0s 20us/step - loss: 0.4299 - auc: 0.5316 - val_loss: 0.4281 - val_auc: 0.5095\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.42809 to 0.42806, saving model to DeepFM.h5\n",
      "Epoch 51/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4204 - auc: 0.416 - ETA: 0s - loss: 0.4241 - auc: 0.480 - 0s 21us/step - loss: 0.4340 - auc: 0.4790 - val_loss: 0.4280 - val_auc: 0.5107\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.42806 to 0.42803, saving model to DeepFM.h5\n",
      "Epoch 52/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4203 - auc: 0.504 - ETA: 0s - loss: 0.4323 - auc: 0.507 - 0s 21us/step - loss: 0.4313 - auc: 0.5171 - val_loss: 0.4280 - val_auc: 0.5234\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.42803 to 0.42800, saving model to DeepFM.h5\n",
      "Epoch 53/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4074 - auc: 0.507 - ETA: 0s - loss: 0.4273 - auc: 0.487 - 0s 21us/step - loss: 0.4325 - auc: 0.4968 - val_loss: 0.4280 - val_auc: 0.5249\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.42800 to 0.42797, saving model to DeepFM.h5\n",
      "Epoch 54/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4292 - auc: 0.546 - ETA: 0s - loss: 0.4325 - auc: 0.496 - 0s 22us/step - loss: 0.4314 - auc: 0.5101 - val_loss: 0.4279 - val_auc: 0.5312\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.42797 to 0.42794, saving model to DeepFM.h5\n",
      "Epoch 55/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3342 - auc: 0.460 - ETA: 0s - loss: 0.4349 - auc: 0.491 - 0s 21us/step - loss: 0.4331 - auc: 0.4899 - val_loss: 0.4279 - val_auc: 0.5385\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.42794 to 0.42791, saving model to DeepFM.h5\n",
      "Epoch 56/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4112 - auc: 0.428 - ETA: 0s - loss: 0.4336 - auc: 0.523 - 0s 20us/step - loss: 0.4317 - auc: 0.5141 - val_loss: 0.4279 - val_auc: 0.5406\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.42791 to 0.42788, saving model to DeepFM.h5\n",
      "Epoch 57/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3708 - auc: 0.498 - ETA: 0s - loss: 0.4333 - auc: 0.498 - 0s 20us/step - loss: 0.4316 - auc: 0.5128 - val_loss: 0.4279 - val_auc: 0.5481\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.42788 to 0.42785, saving model to DeepFM.h5\n",
      "Epoch 58/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4739 - auc: 0.520 - ETA: 0s - loss: 0.4386 - auc: 0.517 - 0s 21us/step - loss: 0.4318 - auc: 0.5136 - val_loss: 0.4278 - val_auc: 0.5414\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.42785 to 0.42782, saving model to DeepFM.h5\n",
      "Epoch 59/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4551 - auc: 0.488 - ETA: 0s - loss: 0.4325 - auc: 0.516 - 0s 22us/step - loss: 0.4310 - auc: 0.5180 - val_loss: 0.4278 - val_auc: 0.5506\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.42782 to 0.42779, saving model to DeepFM.h5\n",
      "Epoch 60/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4279 - auc: 0.478 - ETA: 0s - loss: 0.4247 - auc: 0.494 - 0s 20us/step - loss: 0.4319 - auc: 0.5086 - val_loss: 0.4278 - val_auc: 0.5542\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.42779 to 0.42776, saving model to DeepFM.h5\n",
      "Epoch 61/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3992 - auc: 0.504 - ETA: 0s - loss: 0.4268 - auc: 0.490 - 0s 20us/step - loss: 0.4327 - auc: 0.4949 - val_loss: 0.4277 - val_auc: 0.5524\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.42776 to 0.42773, saving model to DeepFM.h5\n",
      "Epoch 62/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4114 - auc: 0.610 - ETA: 0s - loss: 0.4332 - auc: 0.519 - 0s 20us/step - loss: 0.4311 - auc: 0.5161 - val_loss: 0.4277 - val_auc: 0.5585\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.42773 to 0.42771, saving model to DeepFM.h5\n",
      "Epoch 63/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4037 - auc: 0.558 - ETA: 0s - loss: 0.4285 - auc: 0.526 - 0s 21us/step - loss: 0.4308 - auc: 0.5210 - val_loss: 0.4277 - val_auc: 0.5633\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.42771 to 0.42768, saving model to DeepFM.h5\n",
      "Epoch 64/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3697 - auc: 0.522 - ETA: 0s - loss: 0.4343 - auc: 0.500 - 0s 21us/step - loss: 0.4314 - auc: 0.5106 - val_loss: 0.4277 - val_auc: 0.5626\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.42768 to 0.42765, saving model to DeepFM.h5\n",
      "Epoch 65/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4344 - auc: 0.464 - ETA: 0s - loss: 0.4293 - auc: 0.517 - 0s 23us/step - loss: 0.4312 - auc: 0.5098 - val_loss: 0.4276 - val_auc: 0.5562\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.42765 to 0.42762, saving model to DeepFM.h5\n",
      "Epoch 66/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3965 - auc: 0.525 - ETA: 0s - loss: 0.4356 - auc: 0.528 - 0s 23us/step - loss: 0.4301 - auc: 0.5244 - val_loss: 0.4276 - val_auc: 0.5563\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.42762 to 0.42759, saving model to DeepFM.h5\n",
      "Epoch 67/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4274 - auc: 0.409 - ETA: 0s - loss: 0.4291 - auc: 0.502 - 0s 20us/step - loss: 0.4327 - auc: 0.4973 - val_loss: 0.4276 - val_auc: 0.5594\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.42759 to 0.42756, saving model to DeepFM.h5\n",
      "Epoch 68/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4123 - auc: 0.491 - ETA: 0s - loss: 0.4289 - auc: 0.525 - 0s 21us/step - loss: 0.4311 - auc: 0.5146 - val_loss: 0.4275 - val_auc: 0.5582\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.42756 to 0.42754, saving model to DeepFM.h5\n",
      "Epoch 69/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4151 - auc: 0.472 - ETA: 0s - loss: 0.4257 - auc: 0.507 - 0s 21us/step - loss: 0.4306 - auc: 0.5195 - val_loss: 0.4275 - val_auc: 0.5674\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.42754 to 0.42750, saving model to DeepFM.h5\n",
      "Epoch 70/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4705 - auc: 0.553 - ETA: 0s - loss: 0.4302 - auc: 0.496 - 0s 20us/step - loss: 0.4324 - auc: 0.4959 - val_loss: 0.4275 - val_auc: 0.5688\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.42750 to 0.42747, saving model to DeepFM.h5\n",
      "Epoch 71/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4430 - auc: 0.561 - ETA: 0s - loss: 0.4378 - auc: 0.522 - 0s 21us/step - loss: 0.4304 - auc: 0.5265 - val_loss: 0.4274 - val_auc: 0.5715\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.42747 to 0.42745, saving model to DeepFM.h5\n",
      "Epoch 72/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4517 - auc: 0.532 - ETA: 0s - loss: 0.4302 - auc: 0.492 - 0s 23us/step - loss: 0.4331 - auc: 0.4880 - val_loss: 0.4274 - val_auc: 0.5686\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.42745 to 0.42742, saving model to DeepFM.h5\n",
      "Epoch 73/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4807 - auc: 0.521 - ETA: 0s - loss: 0.4324 - auc: 0.521 - 0s 20us/step - loss: 0.4304 - auc: 0.5266 - val_loss: 0.4274 - val_auc: 0.5697\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.42742 to 0.42739, saving model to DeepFM.h5\n",
      "Epoch 74/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3973 - auc: 0.499 - ETA: 0s - loss: 0.4271 - auc: 0.548 - 0s 20us/step - loss: 0.4292 - auc: 0.5347 - val_loss: 0.4274 - val_auc: 0.5526\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.42739 to 0.42736, saving model to DeepFM.h5\n",
      "Epoch 75/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4277 - auc: 0.487 - ETA: 0s - loss: 0.4282 - auc: 0.518 - 0s 21us/step - loss: 0.4309 - auc: 0.5170 - val_loss: 0.4273 - val_auc: 0.5590\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.42736 to 0.42733, saving model to DeepFM.h5\n",
      "Epoch 76/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4895 - auc: 0.484 - ETA: 0s - loss: 0.4246 - auc: 0.507 - 0s 21us/step - loss: 0.4311 - auc: 0.5094 - val_loss: 0.4273 - val_auc: 0.5545\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.42733 to 0.42730, saving model to DeepFM.h5\n",
      "Epoch 77/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4006 - auc: 0.583 - ETA: 0s - loss: 0.4224 - auc: 0.513 - 0s 22us/step - loss: 0.4318 - auc: 0.5070 - val_loss: 0.4273 - val_auc: 0.5539\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.42730 to 0.42727, saving model to DeepFM.h5\n",
      "Epoch 78/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4509 - auc: 0.545 - ETA: 0s - loss: 0.4411 - auc: 0.513 - 0s 21us/step - loss: 0.4312 - auc: 0.5127 - val_loss: 0.4272 - val_auc: 0.5586\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.42727 to 0.42725, saving model to DeepFM.h5\n",
      "Epoch 79/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4485 - auc: 0.486 - ETA: 0s - loss: 0.4323 - auc: 0.528 - 0s 22us/step - loss: 0.4304 - auc: 0.5262 - val_loss: 0.4272 - val_auc: 0.5570\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.42725 to 0.42722, saving model to DeepFM.h5\n",
      "Epoch 80/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.519 - ETA: 0s - loss: 0.4383 - auc: 0.526 - 0s 24us/step - loss: 0.4302 - auc: 0.5227 - val_loss: 0.4272 - val_auc: 0.5526\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.42722 to 0.42719, saving model to DeepFM.h5\n",
      "Epoch 81/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5073 - auc: 0.512 - ETA: 0s - loss: 0.4325 - auc: 0.522 - 0s 19us/step - loss: 0.4302 - auc: 0.5222 - val_loss: 0.4272 - val_auc: 0.5577\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.42719 to 0.42717, saving model to DeepFM.h5\n",
      "Epoch 82/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3828 - auc: 0.532 - ETA: 0s - loss: 0.4256 - auc: 0.495 - 0s 20us/step - loss: 0.4317 - auc: 0.5021 - val_loss: 0.4271 - val_auc: 0.5563\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.42717 to 0.42714, saving model to DeepFM.h5\n",
      "Epoch 83/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3748 - auc: 0.555 - ETA: 0s - loss: 0.4319 - auc: 0.512 - 0s 19us/step - loss: 0.4313 - auc: 0.5092 - val_loss: 0.4271 - val_auc: 0.5604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss improved from 0.42714 to 0.42711, saving model to DeepFM.h5\n",
      "Epoch 84/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3972 - auc: 0.529 - ETA: 0s - loss: 0.4356 - auc: 0.533 - 0s 20us/step - loss: 0.4298 - auc: 0.5301 - val_loss: 0.4271 - val_auc: 0.5606\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.42711 to 0.42709, saving model to DeepFM.h5\n",
      "Epoch 85/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4424 - auc: 0.473 - ETA: 0s - loss: 0.4270 - auc: 0.520 - 0s 19us/step - loss: 0.4301 - auc: 0.5254 - val_loss: 0.4271 - val_auc: 0.5564\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.42709 to 0.42706, saving model to DeepFM.h5\n",
      "Epoch 86/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4838 - auc: 0.471 - ETA: 0s - loss: 0.4254 - auc: 0.517 - 0s 20us/step - loss: 0.4313 - auc: 0.5044 - val_loss: 0.4270 - val_auc: 0.5627\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.42706 to 0.42703, saving model to DeepFM.h5\n",
      "Epoch 87/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4071 - auc: 0.455 - ETA: 0s - loss: 0.4346 - auc: 0.532 - 0s 20us/step - loss: 0.4293 - auc: 0.5302 - val_loss: 0.4270 - val_auc: 0.5615\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.42703 to 0.42700, saving model to DeepFM.h5\n",
      "Epoch 88/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4024 - auc: 0.477 - ETA: 0s - loss: 0.4363 - auc: 0.503 - 0s 20us/step - loss: 0.4309 - auc: 0.5099 - val_loss: 0.4270 - val_auc: 0.5564\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.42700 to 0.42698, saving model to DeepFM.h5\n",
      "Epoch 89/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4254 - auc: 0.511 - ETA: 0s - loss: 0.4371 - auc: 0.499 - 0s 21us/step - loss: 0.4318 - auc: 0.4995 - val_loss: 0.4269 - val_auc: 0.5598\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.42698 to 0.42695, saving model to DeepFM.h5\n",
      "Epoch 90/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4688 - auc: 0.493 - ETA: 0s - loss: 0.4317 - auc: 0.515 - 0s 20us/step - loss: 0.4301 - auc: 0.5229 - val_loss: 0.4269 - val_auc: 0.5603\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.42695 to 0.42692, saving model to DeepFM.h5\n",
      "Epoch 91/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3947 - auc: 0.561 - ETA: 0s - loss: 0.4285 - auc: 0.514 - 0s 20us/step - loss: 0.4308 - auc: 0.5143 - val_loss: 0.4269 - val_auc: 0.5643\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.42692 to 0.42689, saving model to DeepFM.h5\n",
      "Epoch 92/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5236 - auc: 0.429 - ETA: 0s - loss: 0.4285 - auc: 0.489 - 0s 20us/step - loss: 0.4325 - auc: 0.4885 - val_loss: 0.4269 - val_auc: 0.5685\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.42689 to 0.42687, saving model to DeepFM.h5\n",
      "Epoch 93/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4248 - auc: 0.511 - ETA: 0s - loss: 0.4359 - auc: 0.514 - 0s 22us/step - loss: 0.4306 - auc: 0.5146 - val_loss: 0.4268 - val_auc: 0.5676\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.42687 to 0.42684, saving model to DeepFM.h5\n",
      "Epoch 94/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4339 - auc: 0.497 - ETA: 0s - loss: 0.4291 - auc: 0.521 - 0s 21us/step - loss: 0.4311 - auc: 0.5147 - val_loss: 0.4268 - val_auc: 0.5735\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.42684 to 0.42682, saving model to DeepFM.h5\n",
      "Epoch 95/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4224 - auc: 0.465 - ETA: 0s - loss: 0.4333 - auc: 0.483 - 0s 22us/step - loss: 0.4326 - auc: 0.4913 - val_loss: 0.4268 - val_auc: 0.5663\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.42682 to 0.42679, saving model to DeepFM.h5\n",
      "Epoch 96/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4175 - auc: 0.510 - ETA: 0s - loss: 0.4224 - auc: 0.521 - 0s 21us/step - loss: 0.4297 - auc: 0.5187 - val_loss: 0.4268 - val_auc: 0.5715\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.42679 to 0.42677, saving model to DeepFM.h5\n",
      "Epoch 97/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4286 - auc: 0.560 - ETA: 0s - loss: 0.4354 - auc: 0.536 - 0s 20us/step - loss: 0.4298 - auc: 0.5255 - val_loss: 0.4267 - val_auc: 0.5751\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.42677 to 0.42674, saving model to DeepFM.h5\n",
      "Epoch 98/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4436 - auc: 0.551 - ETA: 0s - loss: 0.4272 - auc: 0.529 - 0s 22us/step - loss: 0.4294 - auc: 0.5304 - val_loss: 0.4267 - val_auc: 0.5678\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.42674 to 0.42672, saving model to DeepFM.h5\n",
      "Epoch 99/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4678 - auc: 0.426 - ETA: 0s - loss: 0.4230 - auc: 0.499 - 0s 21us/step - loss: 0.4304 - auc: 0.5123 - val_loss: 0.4267 - val_auc: 0.5686\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.42672 to 0.42669, saving model to DeepFM.h5\n",
      "Epoch 100/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4634 - auc: 0.488 - ETA: 0s - loss: 0.4339 - auc: 0.523 - 0s 22us/step - loss: 0.4301 - auc: 0.5211 - val_loss: 0.4267 - val_auc: 0.5666\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.42669 to 0.42667, saving model to DeepFM.h5\n",
      "Epoch 101/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4217 - auc: 0.576 - ETA: 0s - loss: 0.4275 - auc: 0.525 - 0s 22us/step - loss: 0.4302 - auc: 0.5211 - val_loss: 0.4266 - val_auc: 0.5674\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.42667 to 0.42664, saving model to DeepFM.h5\n",
      "Epoch 102/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3842 - auc: 0.504 - ETA: 0s - loss: 0.4254 - auc: 0.516 - 0s 21us/step - loss: 0.4300 - auc: 0.5202 - val_loss: 0.4266 - val_auc: 0.5636\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.42664 to 0.42661, saving model to DeepFM.h5\n",
      "Epoch 103/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.412 - ETA: 0s - loss: 0.4316 - auc: 0.524 - 0s 20us/step - loss: 0.4291 - auc: 0.5333 - val_loss: 0.4266 - val_auc: 0.5651\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.42661 to 0.42659, saving model to DeepFM.h5\n",
      "Epoch 104/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4217 - auc: 0.569 - ETA: 0s - loss: 0.4393 - auc: 0.526 - 0s 20us/step - loss: 0.4292 - auc: 0.5282 - val_loss: 0.4266 - val_auc: 0.5634\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.42659 to 0.42657, saving model to DeepFM.h5\n",
      "Epoch 105/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3679 - auc: 0.565 - ETA: 0s - loss: 0.4363 - auc: 0.518 - 0s 20us/step - loss: 0.4304 - auc: 0.5203 - val_loss: 0.4265 - val_auc: 0.5637\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.42657 to 0.42654, saving model to DeepFM.h5\n",
      "Epoch 106/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3808 - auc: 0.533 - ETA: 0s - loss: 0.4432 - auc: 0.509 - 0s 21us/step - loss: 0.4310 - auc: 0.5078 - val_loss: 0.4265 - val_auc: 0.5661\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.42654 to 0.42651, saving model to DeepFM.h5\n",
      "Epoch 107/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3962 - auc: 0.526 - ETA: 0s - loss: 0.4309 - auc: 0.507 - 0s 21us/step - loss: 0.4310 - auc: 0.5073 - val_loss: 0.4265 - val_auc: 0.5724\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.42651 to 0.42649, saving model to DeepFM.h5\n",
      "Epoch 108/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4441 - auc: 0.555 - ETA: 0s - loss: 0.4226 - auc: 0.519 - 0s 22us/step - loss: 0.4288 - auc: 0.5386 - val_loss: 0.4265 - val_auc: 0.5678\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.42649 to 0.42647, saving model to DeepFM.h5\n",
      "Epoch 109/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5004 - auc: 0.500 - ETA: 0s - loss: 0.4324 - auc: 0.496 - 0s 21us/step - loss: 0.4321 - auc: 0.4900 - val_loss: 0.4264 - val_auc: 0.5712\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.42647 to 0.42645, saving model to DeepFM.h5\n",
      "Epoch 110/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4107 - auc: 0.426 - ETA: 0s - loss: 0.4336 - auc: 0.491 - 0s 20us/step - loss: 0.4311 - auc: 0.4995 - val_loss: 0.4264 - val_auc: 0.5742\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.42645 to 0.42642, saving model to DeepFM.h5\n",
      "Epoch 111/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4923 - auc: 0.464 - ETA: 0s - loss: 0.4268 - auc: 0.517 - 0s 21us/step - loss: 0.4311 - auc: 0.5070 - val_loss: 0.4264 - val_auc: 0.5704\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.42642 to 0.42640, saving model to DeepFM.h5\n",
      "Epoch 112/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4761 - auc: 0.547 - ETA: 0s - loss: 0.4274 - auc: 0.542 - 0s 20us/step - loss: 0.4287 - auc: 0.5404 - val_loss: 0.4264 - val_auc: 0.5767\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.42640 to 0.42637, saving model to DeepFM.h5\n",
      "Epoch 113/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5135 - auc: 0.502 - ETA: 0s - loss: 0.4348 - auc: 0.537 - 0s 20us/step - loss: 0.4289 - auc: 0.5306 - val_loss: 0.4263 - val_auc: 0.5733\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.42637 to 0.42635, saving model to DeepFM.h5\n",
      "Epoch 114/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4364 - auc: 0.439 - ETA: 0s - loss: 0.4377 - auc: 0.539 - 0s 20us/step - loss: 0.4283 - auc: 0.5368 - val_loss: 0.4263 - val_auc: 0.5676\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.42635 to 0.42632, saving model to DeepFM.h5\n",
      "Epoch 115/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4118 - auc: 0.484 - ETA: 0s - loss: 0.4343 - auc: 0.509 - 0s 22us/step - loss: 0.4300 - auc: 0.5181 - val_loss: 0.4263 - val_auc: 0.5780\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.42632 to 0.42630, saving model to DeepFM.h5\n",
      "Epoch 116/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3742 - auc: 0.551 - ETA: 0s - loss: 0.4376 - auc: 0.516 - 0s 21us/step - loss: 0.4297 - auc: 0.5219 - val_loss: 0.4263 - val_auc: 0.5794\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.42630 to 0.42627, saving model to DeepFM.h5\n",
      "Epoch 117/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4845 - auc: 0.535 - ETA: 0s - loss: 0.4364 - auc: 0.556 - 0s 24us/step - loss: 0.4273 - auc: 0.5545 - val_loss: 0.4262 - val_auc: 0.5693\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.42627 to 0.42625, saving model to DeepFM.h5\n",
      "Epoch 118/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4105 - auc: 0.628 - ETA: 0s - loss: 0.4421 - auc: 0.539 - 0s 23us/step - loss: 0.4294 - auc: 0.5243 - val_loss: 0.4262 - val_auc: 0.5692\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.42625 to 0.42623, saving model to DeepFM.h5\n",
      "Epoch 119/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5365 - auc: 0.510 - ETA: 0s - loss: 0.4241 - auc: 0.524 - 0s 24us/step - loss: 0.4298 - auc: 0.5220 - val_loss: 0.4262 - val_auc: 0.5713\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.42623 to 0.42620, saving model to DeepFM.h5\n",
      "Epoch 120/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4286 - auc: 0.540 - ETA: 0s - loss: 0.4342 - auc: 0.509 - 0s 21us/step - loss: 0.4294 - auc: 0.5234 - val_loss: 0.4262 - val_auc: 0.5742\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.42620 to 0.42617, saving model to DeepFM.h5\n",
      "Epoch 121/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4041 - auc: 0.600 - ETA: 0s - loss: 0.4302 - auc: 0.534 - 0s 20us/step - loss: 0.4293 - auc: 0.5281 - val_loss: 0.4261 - val_auc: 0.5694\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.42617 to 0.42615, saving model to DeepFM.h5\n",
      "Epoch 122/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4164 - auc: 0.525 - ETA: 0s - loss: 0.4311 - auc: 0.524 - 0s 22us/step - loss: 0.4293 - auc: 0.5224 - val_loss: 0.4261 - val_auc: 0.5717\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.42615 to 0.42613, saving model to DeepFM.h5\n",
      "Epoch 123/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4513 - auc: 0.537 - ETA: 0s - loss: 0.4341 - auc: 0.546 - 0s 20us/step - loss: 0.4285 - auc: 0.5385 - val_loss: 0.4261 - val_auc: 0.5740\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.42613 to 0.42610, saving model to DeepFM.h5\n",
      "Epoch 124/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4477 - auc: 0.485 - ETA: 0s - loss: 0.4332 - auc: 0.504 - 0s 20us/step - loss: 0.4311 - auc: 0.5062 - val_loss: 0.4261 - val_auc: 0.5808\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.42610 to 0.42608, saving model to DeepFM.h5\n",
      "Epoch 125/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4596 - auc: 0.498 - ETA: 0s - loss: 0.4120 - auc: 0.530 - 0s 20us/step - loss: 0.4287 - auc: 0.5346 - val_loss: 0.4261 - val_auc: 0.5783\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.42608 to 0.42605, saving model to DeepFM.h5\n",
      "Epoch 126/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4359 - auc: 0.445 - ETA: 0s - loss: 0.4422 - auc: 0.522 - 0s 19us/step - loss: 0.4285 - auc: 0.5307 - val_loss: 0.4260 - val_auc: 0.5796\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.42605 to 0.42602, saving model to DeepFM.h5\n",
      "Epoch 127/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4408 - auc: 0.499 - ETA: 0s - loss: 0.4296 - auc: 0.512 - 0s 21us/step - loss: 0.4299 - auc: 0.5209 - val_loss: 0.4260 - val_auc: 0.5793\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.42602 to 0.42600, saving model to DeepFM.h5\n",
      "Epoch 128/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4336 - auc: 0.578 - ETA: 0s - loss: 0.4367 - auc: 0.537 - 0s 23us/step - loss: 0.4280 - auc: 0.5426 - val_loss: 0.4260 - val_auc: 0.5811\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.42600 to 0.42597, saving model to DeepFM.h5\n",
      "Epoch 129/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4644 - auc: 0.533 - ETA: 0s - loss: 0.4202 - auc: 0.526 - 0s 21us/step - loss: 0.4283 - auc: 0.5369 - val_loss: 0.4259 - val_auc: 0.5736\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.42597 to 0.42595, saving model to DeepFM.h5\n",
      "Epoch 130/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3850 - auc: 0.484 - ETA: 0s - loss: 0.4356 - auc: 0.533 - 0s 20us/step - loss: 0.4289 - auc: 0.5306 - val_loss: 0.4259 - val_auc: 0.5786\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.42595 to 0.42592, saving model to DeepFM.h5\n",
      "Epoch 131/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3981 - auc: 0.609 - ETA: 0s - loss: 0.4242 - auc: 0.529 - 0s 20us/step - loss: 0.4284 - auc: 0.5384 - val_loss: 0.4259 - val_auc: 0.5822\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.42592 to 0.42590, saving model to DeepFM.h5\n",
      "Epoch 132/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4003 - auc: 0.572 - ETA: 0s - loss: 0.4396 - auc: 0.532 - 0s 21us/step - loss: 0.4287 - auc: 0.5356 - val_loss: 0.4259 - val_auc: 0.5824\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.42590 to 0.42587, saving model to DeepFM.h5\n",
      "Epoch 133/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4366 - auc: 0.538 - ETA: 0s - loss: 0.4361 - auc: 0.516 - 0s 21us/step - loss: 0.4293 - auc: 0.5256 - val_loss: 0.4258 - val_auc: 0.5811\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.42587 to 0.42585, saving model to DeepFM.h5\n",
      "Epoch 134/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4431 - auc: 0.540 - ETA: 0s - loss: 0.4278 - auc: 0.534 - 0s 20us/step - loss: 0.4287 - auc: 0.5348 - val_loss: 0.4258 - val_auc: 0.5803\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.42585 to 0.42582, saving model to DeepFM.h5\n",
      "Epoch 135/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4661 - auc: 0.500 - ETA: 0s - loss: 0.4321 - auc: 0.515 - 0s 22us/step - loss: 0.4296 - auc: 0.5153 - val_loss: 0.4258 - val_auc: 0.5846\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.42582 to 0.42580, saving model to DeepFM.h5\n",
      "Epoch 136/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4577 - auc: 0.451 - ETA: 0s - loss: 0.4315 - auc: 0.551 - 0s 20us/step - loss: 0.4278 - auc: 0.5421 - val_loss: 0.4258 - val_auc: 0.5816\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.42580 to 0.42577, saving model to DeepFM.h5\n",
      "Epoch 137/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4631 - auc: 0.469 - ETA: 0s - loss: 0.4179 - auc: 0.541 - 0s 20us/step - loss: 0.4281 - auc: 0.5389 - val_loss: 0.4257 - val_auc: 0.5798\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.42577 to 0.42575, saving model to DeepFM.h5\n",
      "Epoch 138/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4873 - auc: 0.612 - ETA: 0s - loss: 0.4287 - auc: 0.542 - 0s 21us/step - loss: 0.4288 - auc: 0.5343 - val_loss: 0.4257 - val_auc: 0.5781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00138: val_loss improved from 0.42575 to 0.42573, saving model to DeepFM.h5\n",
      "Epoch 139/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.632 - ETA: 0s - loss: 0.4288 - auc: 0.544 - 0s 20us/step - loss: 0.4285 - auc: 0.5386 - val_loss: 0.4257 - val_auc: 0.5775\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.42573 to 0.42570, saving model to DeepFM.h5\n",
      "Epoch 140/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4740 - auc: 0.502 - ETA: 0s - loss: 0.4433 - auc: 0.527 - 0s 20us/step - loss: 0.4290 - auc: 0.5293 - val_loss: 0.4257 - val_auc: 0.5768\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.42570 to 0.42568, saving model to DeepFM.h5\n",
      "Epoch 141/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4098 - auc: 0.523 - ETA: 0s - loss: 0.4431 - auc: 0.513 - 0s 20us/step - loss: 0.4302 - auc: 0.5098 - val_loss: 0.4257 - val_auc: 0.5788\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.42568 to 0.42565, saving model to DeepFM.h5\n",
      "Epoch 142/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4190 - auc: 0.500 - ETA: 0s - loss: 0.4229 - auc: 0.530 - 0s 21us/step - loss: 0.4285 - auc: 0.5379 - val_loss: 0.4256 - val_auc: 0.5790\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.42565 to 0.42563, saving model to DeepFM.h5\n",
      "Epoch 143/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3960 - auc: 0.506 - ETA: 0s - loss: 0.4325 - auc: 0.547 - 0s 22us/step - loss: 0.4269 - auc: 0.5498 - val_loss: 0.4256 - val_auc: 0.5798\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.42563 to 0.42560, saving model to DeepFM.h5\n",
      "Epoch 144/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4730 - auc: 0.512 - ETA: 0s - loss: 0.4264 - auc: 0.529 - 0s 19us/step - loss: 0.4285 - auc: 0.5287 - val_loss: 0.4256 - val_auc: 0.5806\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.42560 to 0.42558, saving model to DeepFM.h5\n",
      "Epoch 145/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4819 - auc: 0.485 - ETA: 0s - loss: 0.4438 - auc: 0.533 - 0s 24us/step - loss: 0.4294 - auc: 0.5249 - val_loss: 0.4256 - val_auc: 0.5799\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.42558 to 0.42555, saving model to DeepFM.h5\n",
      "Epoch 146/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4560 - auc: 0.637 - ETA: 0s - loss: 0.4225 - auc: 0.541 - 0s 27us/step - loss: 0.4289 - auc: 0.5327 - val_loss: 0.4255 - val_auc: 0.5839\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.42555 to 0.42553, saving model to DeepFM.h5\n",
      "Epoch 147/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4243 - auc: 0.519 - ETA: 0s - loss: 0.4292 - auc: 0.519 - 0s 20us/step - loss: 0.4283 - auc: 0.5334 - val_loss: 0.4255 - val_auc: 0.5865\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.42553 to 0.42550, saving model to DeepFM.h5\n",
      "Epoch 148/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3931 - auc: 0.571 - ETA: 0s - loss: 0.4388 - auc: 0.527 - 0s 20us/step - loss: 0.4293 - auc: 0.5267 - val_loss: 0.4255 - val_auc: 0.5856\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.42550 to 0.42548, saving model to DeepFM.h5\n",
      "Epoch 149/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4591 - auc: 0.523 - ETA: 0s - loss: 0.4193 - auc: 0.533 - 0s 23us/step - loss: 0.4281 - auc: 0.5408 - val_loss: 0.4255 - val_auc: 0.5879\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.42548 to 0.42545, saving model to DeepFM.h5\n",
      "Epoch 150/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4295 - auc: 0.531 - ETA: 0s - loss: 0.4224 - auc: 0.539 - 0s 21us/step - loss: 0.4271 - auc: 0.5501 - val_loss: 0.4254 - val_auc: 0.5900\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.42545 to 0.42543, saving model to DeepFM.h5\n",
      "Epoch 151/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4274 - auc: 0.595 - ETA: 0s - loss: 0.4360 - auc: 0.541 - 0s 23us/step - loss: 0.4282 - auc: 0.5364 - val_loss: 0.4254 - val_auc: 0.5890\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.42543 to 0.42540, saving model to DeepFM.h5\n",
      "Epoch 152/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3734 - auc: 0.526 - ETA: 0s - loss: 0.4287 - auc: 0.521 - 0s 20us/step - loss: 0.4284 - auc: 0.5324 - val_loss: 0.4254 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.42540 to 0.42538, saving model to DeepFM.h5\n",
      "Epoch 153/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4731 - auc: 0.584 - ETA: 0s - loss: 0.4261 - auc: 0.527 - ETA: 0s - loss: 0.4243 - auc: 0.523 - 0s 32us/step - loss: 0.4293 - auc: 0.5224 - val_loss: 0.4253 - val_auc: 0.5877\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.42538 to 0.42535, saving model to DeepFM.h5\n",
      "Epoch 154/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3854 - auc: 0.584 - ETA: 0s - loss: 0.4237 - auc: 0.538 - 0s 26us/step - loss: 0.4278 - auc: 0.5431 - val_loss: 0.4253 - val_auc: 0.5878\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.42535 to 0.42533, saving model to DeepFM.h5\n",
      "Epoch 155/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4070 - auc: 0.467 - ETA: 0s - loss: 0.4279 - auc: 0.520 - 0s 20us/step - loss: 0.4284 - auc: 0.5299 - val_loss: 0.4253 - val_auc: 0.5878\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.42533 to 0.42530, saving model to DeepFM.h5\n",
      "Epoch 156/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4710 - auc: 0.554 - ETA: 0s - loss: 0.4324 - auc: 0.539 - 0s 26us/step - loss: 0.4280 - auc: 0.5428 - val_loss: 0.4253 - val_auc: 0.5873\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.42530 to 0.42528, saving model to DeepFM.h5\n",
      "Epoch 157/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4723 - auc: 0.607 - ETA: 0s - loss: 0.4307 - auc: 0.557 - 0s 21us/step - loss: 0.4278 - auc: 0.5442 - val_loss: 0.4253 - val_auc: 0.5886\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.42528 to 0.42525, saving model to DeepFM.h5\n",
      "Epoch 158/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4702 - auc: 0.619 - ETA: 0s - loss: 0.4182 - auc: 0.539 - 0s 20us/step - loss: 0.4282 - auc: 0.5344 - val_loss: 0.4252 - val_auc: 0.5884\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.42525 to 0.42523, saving model to DeepFM.h5\n",
      "Epoch 159/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3908 - auc: 0.559 - ETA: 0s - loss: 0.4360 - auc: 0.531 - 0s 21us/step - loss: 0.4285 - auc: 0.5271 - val_loss: 0.4252 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.42523 to 0.42521, saving model to DeepFM.h5\n",
      "Epoch 160/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4346 - auc: 0.479 - ETA: 0s - loss: 0.4295 - auc: 0.513 - 0s 21us/step - loss: 0.4291 - auc: 0.5241 - val_loss: 0.4252 - val_auc: 0.5896\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.42521 to 0.42519, saving model to DeepFM.h5\n",
      "Epoch 161/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5303 - auc: 0.492 - ETA: 0s - loss: 0.4397 - auc: 0.543 - 0s 23us/step - loss: 0.4272 - auc: 0.5491 - val_loss: 0.4252 - val_auc: 0.5898\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.42519 to 0.42516, saving model to DeepFM.h5\n",
      "Epoch 162/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4073 - auc: 0.550 - ETA: 0s - loss: 0.4204 - auc: 0.557 - ETA: 0s - loss: 0.4316 - auc: 0.539 - 0s 39us/step - loss: 0.4282 - auc: 0.5291 - val_loss: 0.4251 - val_auc: 0.5877\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.42516 to 0.42514, saving model to DeepFM.h5\n",
      "Epoch 163/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4450 - auc: 0.590 - ETA: 0s - loss: 0.4232 - auc: 0.531 - 0s 20us/step - loss: 0.4278 - auc: 0.5366 - val_loss: 0.4251 - val_auc: 0.5860\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.42514 to 0.42511, saving model to DeepFM.h5\n",
      "Epoch 164/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.616 - ETA: 0s - loss: 0.4176 - auc: 0.562 - 0s 23us/step - loss: 0.4259 - auc: 0.5674 - val_loss: 0.4251 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.42511 to 0.42509, saving model to DeepFM.h5\n",
      "Epoch 165/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4034 - auc: 0.525 - ETA: 0s - loss: 0.4423 - auc: 0.532 - 0s 21us/step - loss: 0.4281 - auc: 0.5357 - val_loss: 0.4251 - val_auc: 0.5854\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00165: val_loss improved from 0.42509 to 0.42506, saving model to DeepFM.h5\n",
      "Epoch 166/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4933 - auc: 0.504 - ETA: 0s - loss: 0.4432 - auc: 0.534 - 0s 27us/step - loss: 0.4274 - auc: 0.5434 - val_loss: 0.4250 - val_auc: 0.5865\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.42506 to 0.42504, saving model to DeepFM.h5\n",
      "Epoch 167/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3904 - auc: 0.492 - ETA: 0s - loss: 0.4210 - auc: 0.543 - 0s 20us/step - loss: 0.4284 - auc: 0.5275 - val_loss: 0.4250 - val_auc: 0.5890\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.42504 to 0.42501, saving model to DeepFM.h5\n",
      "Epoch 168/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4821 - auc: 0.499 - ETA: 0s - loss: 0.4279 - auc: 0.524 - 0s 20us/step - loss: 0.4290 - auc: 0.5303 - val_loss: 0.4250 - val_auc: 0.5878\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.42501 to 0.42499, saving model to DeepFM.h5\n",
      "Epoch 169/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3713 - auc: 0.472 - ETA: 0s - loss: 0.4239 - auc: 0.531 - 0s 20us/step - loss: 0.4279 - auc: 0.5330 - val_loss: 0.4250 - val_auc: 0.5868\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.42499 to 0.42497, saving model to DeepFM.h5\n",
      "Epoch 170/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4293 - auc: 0.534 - ETA: 0s - loss: 0.4354 - auc: 0.518 - 0s 20us/step - loss: 0.4293 - auc: 0.5235 - val_loss: 0.4249 - val_auc: 0.5860\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.42497 to 0.42494, saving model to DeepFM.h5\n",
      "Epoch 171/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4248 - auc: 0.498 - ETA: 0s - loss: 0.4339 - auc: 0.557 - 0s 21us/step - loss: 0.4273 - auc: 0.5503 - val_loss: 0.4249 - val_auc: 0.5871\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.42494 to 0.42492, saving model to DeepFM.h5\n",
      "Epoch 172/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4665 - auc: 0.483 - ETA: 0s - loss: 0.4226 - auc: 0.535 - 0s 21us/step - loss: 0.4282 - auc: 0.5393 - val_loss: 0.4249 - val_auc: 0.5837\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.42492 to 0.42489, saving model to DeepFM.h5\n",
      "Epoch 173/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4250 - auc: 0.585 - ETA: 0s - loss: 0.4342 - auc: 0.528 - 0s 23us/step - loss: 0.4281 - auc: 0.5304 - val_loss: 0.4249 - val_auc: 0.5829\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.42489 to 0.42487, saving model to DeepFM.h5\n",
      "Epoch 174/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4578 - auc: 0.592 - ETA: 0s - loss: 0.4328 - auc: 0.540 - 0s 21us/step - loss: 0.4273 - auc: 0.5420 - val_loss: 0.4248 - val_auc: 0.5824\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.42487 to 0.42484, saving model to DeepFM.h5\n",
      "Epoch 175/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4327 - auc: 0.477 - ETA: 0s - loss: 0.4318 - auc: 0.541 - 0s 20us/step - loss: 0.4281 - auc: 0.5375 - val_loss: 0.4248 - val_auc: 0.5814\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.42484 to 0.42482, saving model to DeepFM.h5\n",
      "Epoch 176/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4338 - auc: 0.542 - ETA: 0s - loss: 0.4303 - auc: 0.534 - 0s 21us/step - loss: 0.4273 - auc: 0.5412 - val_loss: 0.4248 - val_auc: 0.5843\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.42482 to 0.42479, saving model to DeepFM.h5\n",
      "Epoch 177/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4530 - auc: 0.592 - ETA: 0s - loss: 0.4343 - auc: 0.518 - 0s 21us/step - loss: 0.4290 - auc: 0.5211 - val_loss: 0.4248 - val_auc: 0.5843\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.42479 to 0.42477, saving model to DeepFM.h5\n",
      "Epoch 178/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4105 - auc: 0.521 - ETA: 0s - loss: 0.4282 - auc: 0.528 - 0s 21us/step - loss: 0.4285 - auc: 0.5267 - val_loss: 0.4247 - val_auc: 0.5844\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.42477 to 0.42474, saving model to DeepFM.h5\n",
      "Epoch 179/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4339 - auc: 0.641 - ETA: 0s - loss: 0.4270 - auc: 0.553 - 0s 24us/step - loss: 0.4267 - auc: 0.5491 - val_loss: 0.4247 - val_auc: 0.5838\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.42474 to 0.42472, saving model to DeepFM.h5\n",
      "Epoch 180/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4169 - auc: 0.520 - ETA: 0s - loss: 0.4239 - auc: 0.547 - 0s 19us/step - loss: 0.4268 - auc: 0.5531 - val_loss: 0.4247 - val_auc: 0.5819\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.42472 to 0.42470, saving model to DeepFM.h5\n",
      "Epoch 181/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4549 - auc: 0.554 - ETA: 0s - loss: 0.4277 - auc: 0.551 - 0s 20us/step - loss: 0.4269 - auc: 0.5510 - val_loss: 0.4247 - val_auc: 0.5769\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.42470 to 0.42467, saving model to DeepFM.h5\n",
      "Epoch 182/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4590 - auc: 0.593 - ETA: 0s - loss: 0.4222 - auc: 0.556 - 0s 20us/step - loss: 0.4279 - auc: 0.5364 - val_loss: 0.4246 - val_auc: 0.5772\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.42467 to 0.42465, saving model to DeepFM.h5\n",
      "Epoch 183/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4123 - auc: 0.488 - ETA: 0s - loss: 0.4164 - auc: 0.542 - 0s 26us/step - loss: 0.4275 - auc: 0.5386 - val_loss: 0.4246 - val_auc: 0.5756\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.42465 to 0.42462, saving model to DeepFM.h5\n",
      "Epoch 184/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4304 - auc: 0.518 - ETA: 0s - loss: 0.4348 - auc: 0.550 - 0s 22us/step - loss: 0.4268 - auc: 0.5501 - val_loss: 0.4246 - val_auc: 0.5764\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.42462 to 0.42460, saving model to DeepFM.h5\n",
      "Epoch 185/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4035 - auc: 0.505 - ETA: 0s - loss: 0.4186 - auc: 0.529 - 0s 22us/step - loss: 0.4283 - auc: 0.5333 - val_loss: 0.4246 - val_auc: 0.5739\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.42460 to 0.42457, saving model to DeepFM.h5\n",
      "Epoch 186/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4481 - auc: 0.550 - ETA: 0s - loss: 0.4300 - auc: 0.544 - 0s 21us/step - loss: 0.4275 - auc: 0.5367 - val_loss: 0.4246 - val_auc: 0.5755\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.42457 to 0.42455, saving model to DeepFM.h5\n",
      "Epoch 187/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4674 - auc: 0.568 - ETA: 0s - loss: 0.4170 - auc: 0.564 - 0s 21us/step - loss: 0.4256 - auc: 0.5703 - val_loss: 0.4245 - val_auc: 0.5805\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.42455 to 0.42453, saving model to DeepFM.h5\n",
      "Epoch 188/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4857 - auc: 0.526 - ETA: 0s - loss: 0.4310 - auc: 0.530 - 0s 21us/step - loss: 0.4278 - auc: 0.5409 - val_loss: 0.4245 - val_auc: 0.5790\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.42453 to 0.42450, saving model to DeepFM.h5\n",
      "Epoch 189/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4267 - auc: 0.479 - ETA: 0s - loss: 0.4292 - auc: 0.535 - 0s 21us/step - loss: 0.4276 - auc: 0.5399 - val_loss: 0.4245 - val_auc: 0.5753\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.42450 to 0.42448, saving model to DeepFM.h5\n",
      "Epoch 190/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3740 - auc: 0.555 - ETA: 0s - loss: 0.4223 - auc: 0.536 - 0s 22us/step - loss: 0.4267 - auc: 0.5487 - val_loss: 0.4245 - val_auc: 0.5712\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.42448 to 0.42445, saving model to DeepFM.h5\n",
      "Epoch 191/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3832 - auc: 0.606 - ETA: 0s - loss: 0.4269 - auc: 0.533 - 0s 20us/step - loss: 0.4270 - auc: 0.5409 - val_loss: 0.4244 - val_auc: 0.5721\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.42445 to 0.42442, saving model to DeepFM.h5\n",
      "Epoch 192/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4513 - auc: 0.614 - ETA: 0s - loss: 0.4229 - auc: 0.530 - 0s 19us/step - loss: 0.4278 - auc: 0.5398 - val_loss: 0.4244 - val_auc: 0.5726\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.42442 to 0.42440, saving model to DeepFM.h5\n",
      "Epoch 193/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4237 - auc: 0.533 - ETA: 0s - loss: 0.4140 - auc: 0.590 - 0s 23us/step - loss: 0.4259 - auc: 0.5647 - val_loss: 0.4244 - val_auc: 0.5726\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.42440 to 0.42437, saving model to DeepFM.h5\n",
      "Epoch 194/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3766 - auc: 0.608 - ETA: 0s - loss: 0.4204 - auc: 0.563 - 0s 21us/step - loss: 0.4269 - auc: 0.5481 - val_loss: 0.4243 - val_auc: 0.5733\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.42437 to 0.42435, saving model to DeepFM.h5\n",
      "Epoch 195/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3930 - auc: 0.568 - ETA: 0s - loss: 0.4139 - auc: 0.559 - 0s 21us/step - loss: 0.4270 - auc: 0.5465 - val_loss: 0.4243 - val_auc: 0.5730\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.42435 to 0.42432, saving model to DeepFM.h5\n",
      "Epoch 196/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4017 - auc: 0.542 - ETA: 0s - loss: 0.4375 - auc: 0.543 - 0s 21us/step - loss: 0.4281 - auc: 0.5334 - val_loss: 0.4243 - val_auc: 0.5764\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.42432 to 0.42430, saving model to DeepFM.h5\n",
      "Epoch 197/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4154 - auc: 0.518 - ETA: 0s - loss: 0.4309 - auc: 0.538 - 0s 23us/step - loss: 0.4272 - auc: 0.5417 - val_loss: 0.4243 - val_auc: 0.5776\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.42430 to 0.42427, saving model to DeepFM.h5\n",
      "Epoch 198/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4168 - auc: 0.529 - ETA: 0s - loss: 0.4358 - auc: 0.544 - 0s 22us/step - loss: 0.4273 - auc: 0.5404 - val_loss: 0.4242 - val_auc: 0.5793\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.42427 to 0.42425, saving model to DeepFM.h5\n",
      "Epoch 199/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4193 - auc: 0.470 - ETA: 0s - loss: 0.4174 - auc: 0.538 - 0s 23us/step - loss: 0.4266 - auc: 0.5468 - val_loss: 0.4242 - val_auc: 0.5781\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.42425 to 0.42422, saving model to DeepFM.h5\n",
      "Epoch 200/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4123 - auc: 0.489 - ETA: 0s - loss: 0.4303 - auc: 0.542 - 0s 22us/step - loss: 0.4268 - auc: 0.5466 - val_loss: 0.4242 - val_auc: 0.5755\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.42422 to 0.42420, saving model to DeepFM.h5\n",
      "Epoch 201/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4573 - auc: 0.441 - ETA: 0s - loss: 0.4153 - auc: 0.523 - 0s 21us/step - loss: 0.4285 - auc: 0.5254 - val_loss: 0.4242 - val_auc: 0.5737\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.42420 to 0.42417, saving model to DeepFM.h5\n",
      "Epoch 202/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4895 - auc: 0.534 - ETA: 0s - loss: 0.4336 - auc: 0.533 - 0s 22us/step - loss: 0.4279 - auc: 0.5294 - val_loss: 0.4241 - val_auc: 0.5741\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.42417 to 0.42415, saving model to DeepFM.h5\n",
      "Epoch 203/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3954 - auc: 0.526 - ETA: 0s - loss: 0.4444 - auc: 0.533 - 0s 25us/step - loss: 0.4271 - auc: 0.5452 - val_loss: 0.4241 - val_auc: 0.5716\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.42415 to 0.42412, saving model to DeepFM.h5\n",
      "Epoch 204/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4536 - auc: 0.561 - ETA: 0s - loss: 0.4367 - auc: 0.550 - 0s 21us/step - loss: 0.4260 - auc: 0.5553 - val_loss: 0.4241 - val_auc: 0.5754\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.42412 to 0.42410, saving model to DeepFM.h5\n",
      "Epoch 205/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3925 - auc: 0.589 - ETA: 0s - loss: 0.4372 - auc: 0.541 - 0s 23us/step - loss: 0.4279 - auc: 0.5322 - val_loss: 0.4241 - val_auc: 0.5755\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.42410 to 0.42407, saving model to DeepFM.h5\n",
      "Epoch 206/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4202 - auc: 0.571 - ETA: 0s - loss: 0.4259 - auc: 0.526 - 0s 21us/step - loss: 0.4274 - auc: 0.5328 - val_loss: 0.4240 - val_auc: 0.5759\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.42407 to 0.42404, saving model to DeepFM.h5\n",
      "Epoch 207/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4846 - auc: 0.539 - ETA: 0s - loss: 0.4230 - auc: 0.549 - 0s 20us/step - loss: 0.4260 - auc: 0.5521 - val_loss: 0.4240 - val_auc: 0.5741\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.42404 to 0.42402, saving model to DeepFM.h5\n",
      "Epoch 208/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5197 - auc: 0.436 - ETA: 0s - loss: 0.4360 - auc: 0.541 - 0s 22us/step - loss: 0.4269 - auc: 0.5435 - val_loss: 0.4240 - val_auc: 0.5731\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.42402 to 0.42399, saving model to DeepFM.h5\n",
      "Epoch 209/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4059 - auc: 0.577 - ETA: 0s - loss: 0.4305 - auc: 0.578 - 0s 20us/step - loss: 0.4250 - auc: 0.5735 - val_loss: 0.4240 - val_auc: 0.5757\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.42399 to 0.42396, saving model to DeepFM.h5\n",
      "Epoch 210/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4403 - auc: 0.569 - ETA: 0s - loss: 0.4305 - auc: 0.562 - 0s 22us/step - loss: 0.4258 - auc: 0.5543 - val_loss: 0.4239 - val_auc: 0.5761\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.42396 to 0.42394, saving model to DeepFM.h5\n",
      "Epoch 211/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4559 - auc: 0.541 - ETA: 0s - loss: 0.4284 - auc: 0.537 - 0s 23us/step - loss: 0.4263 - auc: 0.5523 - val_loss: 0.4239 - val_auc: 0.5784\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.42394 to 0.42391, saving model to DeepFM.h5\n",
      "Epoch 212/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4084 - auc: 0.514 - ETA: 0s - loss: 0.4310 - auc: 0.531 - 0s 23us/step - loss: 0.4269 - auc: 0.5485 - val_loss: 0.4239 - val_auc: 0.5793\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.42391 to 0.42388, saving model to DeepFM.h5\n",
      "Epoch 213/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4303 - auc: 0.543 - ETA: 0s - loss: 0.4271 - auc: 0.538 - 0s 21us/step - loss: 0.4272 - auc: 0.5442 - val_loss: 0.4239 - val_auc: 0.5761\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.42388 to 0.42385, saving model to DeepFM.h5\n",
      "Epoch 214/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4519 - auc: 0.510 - ETA: 0s - loss: 0.4269 - auc: 0.562 - 0s 19us/step - loss: 0.4254 - auc: 0.5594 - val_loss: 0.4238 - val_auc: 0.5816\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.42385 to 0.42383, saving model to DeepFM.h5\n",
      "Epoch 215/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4411 - auc: 0.558 - ETA: 0s - loss: 0.4231 - auc: 0.557 - 0s 20us/step - loss: 0.4254 - auc: 0.5645 - val_loss: 0.4238 - val_auc: 0.5853\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.42383 to 0.42381, saving model to DeepFM.h5\n",
      "Epoch 216/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3819 - auc: 0.509 - ETA: 0s - loss: 0.4185 - auc: 0.571 - 0s 20us/step - loss: 0.4251 - auc: 0.5642 - val_loss: 0.4238 - val_auc: 0.5861\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.42381 to 0.42378, saving model to DeepFM.h5\n",
      "Epoch 217/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4241 - auc: 0.522 - ETA: 0s - loss: 0.4238 - auc: 0.521 - 0s 20us/step - loss: 0.4275 - auc: 0.5372 - val_loss: 0.4238 - val_auc: 0.5847\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.42378 to 0.42376, saving model to DeepFM.h5\n",
      "Epoch 218/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4296 - auc: 0.603 - ETA: 0s - loss: 0.4337 - auc: 0.574 - 0s 23us/step - loss: 0.4247 - auc: 0.5754 - val_loss: 0.4237 - val_auc: 0.5883\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.42376 to 0.42373, saving model to DeepFM.h5\n",
      "Epoch 219/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3959 - auc: 0.610 - ETA: 0s - loss: 0.4334 - auc: 0.535 - 0s 24us/step - loss: 0.4268 - auc: 0.5416 - val_loss: 0.4237 - val_auc: 0.5880\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.42373 to 0.42371, saving model to DeepFM.h5\n",
      "Epoch 220/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3679 - auc: 0.640 - ETA: 0s - loss: 0.4141 - auc: 0.556 - 0s 22us/step - loss: 0.4271 - auc: 0.5439 - val_loss: 0.4237 - val_auc: 0.5821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00220: val_loss improved from 0.42371 to 0.42368, saving model to DeepFM.h5\n",
      "Epoch 221/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4600 - auc: 0.565 - ETA: 0s - loss: 0.4279 - auc: 0.548 - 0s 21us/step - loss: 0.4263 - auc: 0.5512 - val_loss: 0.4236 - val_auc: 0.5802\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.42368 to 0.42365, saving model to DeepFM.h5\n",
      "Epoch 222/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3877 - auc: 0.598 - ETA: 0s - loss: 0.4309 - auc: 0.555 - 0s 20us/step - loss: 0.4252 - auc: 0.5632 - val_loss: 0.4236 - val_auc: 0.5808\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.42365 to 0.42362, saving model to DeepFM.h5\n",
      "Epoch 223/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3889 - auc: 0.508 - ETA: 0s - loss: 0.4283 - auc: 0.543 - 0s 22us/step - loss: 0.4267 - auc: 0.5474 - val_loss: 0.4236 - val_auc: 0.5813\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.42362 to 0.42359, saving model to DeepFM.h5\n",
      "Epoch 224/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5287 - auc: 0.572 - ETA: 0s - loss: 0.4259 - auc: 0.556 - 0s 25us/step - loss: 0.4256 - auc: 0.5566 - val_loss: 0.4236 - val_auc: 0.5821\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.42359 to 0.42357, saving model to DeepFM.h5\n",
      "Epoch 225/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4231 - auc: 0.621 - ETA: 0s - loss: 0.4311 - auc: 0.539 - 0s 23us/step - loss: 0.4269 - auc: 0.5436 - val_loss: 0.4235 - val_auc: 0.5799\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.42357 to 0.42353, saving model to DeepFM.h5\n",
      "Epoch 226/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4875 - auc: 0.491 - ETA: 0s - loss: 0.4281 - auc: 0.535 - 0s 23us/step - loss: 0.4266 - auc: 0.5443 - val_loss: 0.4235 - val_auc: 0.5775\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.42353 to 0.42350, saving model to DeepFM.h5\n",
      "Epoch 227/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4438 - auc: 0.606 - ETA: 0s - loss: 0.4211 - auc: 0.545 - 0s 20us/step - loss: 0.4270 - auc: 0.5422 - val_loss: 0.4235 - val_auc: 0.5763\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.42350 to 0.42347, saving model to DeepFM.h5\n",
      "Epoch 228/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4467 - auc: 0.583 - ETA: 0s - loss: 0.4259 - auc: 0.533 - 0s 22us/step - loss: 0.4263 - auc: 0.5485 - val_loss: 0.4234 - val_auc: 0.5722\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.42347 to 0.42344, saving model to DeepFM.h5\n",
      "Epoch 229/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4048 - auc: 0.591 - ETA: 0s - loss: 0.4132 - auc: 0.557 - 0s 18us/step - loss: 0.4257 - auc: 0.5549 - val_loss: 0.4234 - val_auc: 0.5745\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.42344 to 0.42341, saving model to DeepFM.h5\n",
      "Epoch 230/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4379 - auc: 0.500 - ETA: 0s - loss: 0.4272 - auc: 0.541 - 0s 19us/step - loss: 0.4267 - auc: 0.5414 - val_loss: 0.4234 - val_auc: 0.5745\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.42341 to 0.42338, saving model to DeepFM.h5\n",
      "Epoch 231/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4087 - auc: 0.539 - ETA: 0s - loss: 0.4378 - auc: 0.551 - 0s 23us/step - loss: 0.4255 - auc: 0.5602 - val_loss: 0.4234 - val_auc: 0.5785\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.42338 to 0.42336, saving model to DeepFM.h5\n",
      "Epoch 232/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4443 - auc: 0.587 - ETA: 0s - loss: 0.4177 - auc: 0.565 - 0s 21us/step - loss: 0.4259 - auc: 0.5589 - val_loss: 0.4233 - val_auc: 0.5792\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.42336 to 0.42333, saving model to DeepFM.h5\n",
      "Epoch 233/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4068 - auc: 0.547 - ETA: 0s - loss: 0.4240 - auc: 0.550 - 0s 20us/step - loss: 0.4266 - auc: 0.5428 - val_loss: 0.4233 - val_auc: 0.5785\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.42333 to 0.42330, saving model to DeepFM.h5\n",
      "Epoch 234/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4822 - auc: 0.547 - ETA: 0s - loss: 0.4366 - auc: 0.568 - 0s 19us/step - loss: 0.4251 - auc: 0.5633 - val_loss: 0.4233 - val_auc: 0.5811\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.42330 to 0.42327, saving model to DeepFM.h5\n",
      "Epoch 235/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4080 - auc: 0.534 - ETA: 0s - loss: 0.4259 - auc: 0.542 - 0s 20us/step - loss: 0.4253 - auc: 0.5590 - val_loss: 0.4232 - val_auc: 0.5833\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.42327 to 0.42323, saving model to DeepFM.h5\n",
      "Epoch 236/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3112 - auc: 0.678 - ETA: 0s - loss: 0.4240 - auc: 0.585 - 0s 21us/step - loss: 0.4241 - auc: 0.5748 - val_loss: 0.4232 - val_auc: 0.5867\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.42323 to 0.42321, saving model to DeepFM.h5\n",
      "Epoch 237/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4305 - auc: 0.497 - ETA: 0s - loss: 0.4321 - auc: 0.560 - 0s 20us/step - loss: 0.4257 - auc: 0.5558 - val_loss: 0.4232 - val_auc: 0.5885\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.42321 to 0.42318, saving model to DeepFM.h5\n",
      "Epoch 238/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3561 - auc: 0.590 - ETA: 0s - loss: 0.4244 - auc: 0.567 - 0s 20us/step - loss: 0.4253 - auc: 0.5638 - val_loss: 0.4231 - val_auc: 0.5873\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.42318 to 0.42315, saving model to DeepFM.h5\n",
      "Epoch 239/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.517 - ETA: 0s - loss: 0.4268 - auc: 0.551 - 0s 21us/step - loss: 0.4264 - auc: 0.5460 - val_loss: 0.4231 - val_auc: 0.5865\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.42315 to 0.42312, saving model to DeepFM.h5\n",
      "Epoch 240/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4560 - auc: 0.554 - ETA: 0s - loss: 0.4188 - auc: 0.547 - 0s 21us/step - loss: 0.4263 - auc: 0.5512 - val_loss: 0.4231 - val_auc: 0.5864\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.42312 to 0.42308, saving model to DeepFM.h5\n",
      "Epoch 241/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4168 - auc: 0.522 - ETA: 0s - loss: 0.4242 - auc: 0.555 - 0s 20us/step - loss: 0.4266 - auc: 0.5448 - val_loss: 0.4231 - val_auc: 0.5836\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.42308 to 0.42306, saving model to DeepFM.h5\n",
      "Epoch 242/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5085 - auc: 0.631 - ETA: 0s - loss: 0.4374 - auc: 0.569 - 0s 19us/step - loss: 0.4248 - auc: 0.5723 - val_loss: 0.4230 - val_auc: 0.5891\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.42306 to 0.42304, saving model to DeepFM.h5\n",
      "Epoch 243/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3949 - auc: 0.613 - ETA: 0s - loss: 0.4259 - auc: 0.544 - 0s 21us/step - loss: 0.4263 - auc: 0.5456 - val_loss: 0.4230 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.42304 to 0.42301, saving model to DeepFM.h5\n",
      "Epoch 244/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3629 - auc: 0.561 - ETA: 0s - loss: 0.4208 - auc: 0.543 - 0s 21us/step - loss: 0.4270 - auc: 0.5320 - val_loss: 0.4230 - val_auc: 0.5873\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.42301 to 0.42298, saving model to DeepFM.h5\n",
      "Epoch 245/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3397 - auc: 0.640 - ETA: 0s - loss: 0.4283 - auc: 0.547 - 0s 20us/step - loss: 0.4262 - auc: 0.5475 - val_loss: 0.4230 - val_auc: 0.5851\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.42298 to 0.42295, saving model to DeepFM.h5\n",
      "Epoch 246/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4417 - auc: 0.636 - ETA: 0s - loss: 0.4198 - auc: 0.576 - 0s 20us/step - loss: 0.4241 - auc: 0.5784 - val_loss: 0.4229 - val_auc: 0.5870\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.42295 to 0.42293, saving model to DeepFM.h5\n",
      "Epoch 247/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4043 - auc: 0.582 - ETA: 0s - loss: 0.4197 - auc: 0.583 - 0s 21us/step - loss: 0.4254 - auc: 0.5649 - val_loss: 0.4229 - val_auc: 0.5896\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.42293 to 0.42291, saving model to DeepFM.h5\n",
      "Epoch 248/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4414 - auc: 0.557 - ETA: 0s - loss: 0.4280 - auc: 0.556 - 0s 21us/step - loss: 0.4249 - auc: 0.5664 - val_loss: 0.4229 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.42291 to 0.42288, saving model to DeepFM.h5\n",
      "Epoch 249/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3966 - auc: 0.505 - ETA: 0s - loss: 0.4249 - auc: 0.550 - 0s 20us/step - loss: 0.4258 - auc: 0.5534 - val_loss: 0.4229 - val_auc: 0.5880\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.42288 to 0.42285, saving model to DeepFM.h5\n",
      "Epoch 250/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.581 - ETA: 0s - loss: 0.4272 - auc: 0.550 - 0s 20us/step - loss: 0.4263 - auc: 0.5492 - val_loss: 0.4228 - val_auc: 0.5882\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.42285 to 0.42282, saving model to DeepFM.h5\n",
      "Epoch 251/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4478 - auc: 0.556 - ETA: 0s - loss: 0.4264 - auc: 0.551 - 0s 21us/step - loss: 0.4256 - auc: 0.5516 - val_loss: 0.4228 - val_auc: 0.5907\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.42282 to 0.42279, saving model to DeepFM.h5\n",
      "Epoch 252/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3938 - auc: 0.536 - ETA: 0s - loss: 0.4141 - auc: 0.557 - 0s 21us/step - loss: 0.4255 - auc: 0.5550 - val_loss: 0.4228 - val_auc: 0.5951\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.42279 to 0.42276, saving model to DeepFM.h5\n",
      "Epoch 253/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4525 - auc: 0.577 - ETA: 0s - loss: 0.4278 - auc: 0.557 - 0s 20us/step - loss: 0.4252 - auc: 0.5621 - val_loss: 0.4227 - val_auc: 0.5938\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.42276 to 0.42273, saving model to DeepFM.h5\n",
      "Epoch 254/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3562 - auc: 0.602 - ETA: 0s - loss: 0.4314 - auc: 0.564 - 0s 21us/step - loss: 0.4264 - auc: 0.5495 - val_loss: 0.4227 - val_auc: 0.5938\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.42273 to 0.42270, saving model to DeepFM.h5\n",
      "Epoch 255/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3292 - auc: 0.668 - ETA: 0s - loss: 0.4147 - auc: 0.566 - 0s 21us/step - loss: 0.4245 - auc: 0.5620 - val_loss: 0.4227 - val_auc: 0.5939\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.42270 to 0.42268, saving model to DeepFM.h5\n",
      "Epoch 256/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4367 - auc: 0.594 - ETA: 0s - loss: 0.4237 - auc: 0.582 - 0s 24us/step - loss: 0.4251 - auc: 0.5636 - val_loss: 0.4227 - val_auc: 0.5938\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.42268 to 0.42266, saving model to DeepFM.h5\n",
      "Epoch 257/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4706 - auc: 0.607 - ETA: 0s - loss: 0.4165 - auc: 0.540 - 0s 23us/step - loss: 0.4262 - auc: 0.5495 - val_loss: 0.4226 - val_auc: 0.5952\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.42266 to 0.42262, saving model to DeepFM.h5\n",
      "Epoch 258/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4011 - auc: 0.510 - ETA: 0s - loss: 0.4248 - auc: 0.558 - 0s 22us/step - loss: 0.4258 - auc: 0.5518 - val_loss: 0.4226 - val_auc: 0.5951\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.42262 to 0.42259, saving model to DeepFM.h5\n",
      "Epoch 259/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3847 - auc: 0.583 - ETA: 0s - loss: 0.4217 - auc: 0.596 - 0s 22us/step - loss: 0.4235 - auc: 0.5845 - val_loss: 0.4226 - val_auc: 0.5972\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.42259 to 0.42257, saving model to DeepFM.h5\n",
      "Epoch 260/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4058 - auc: 0.572 - ETA: 0s - loss: 0.4242 - auc: 0.549 - 0s 23us/step - loss: 0.4265 - auc: 0.5378 - val_loss: 0.4225 - val_auc: 0.5952\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.42257 to 0.42253, saving model to DeepFM.h5\n",
      "Epoch 261/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3795 - auc: 0.644 - ETA: 0s - loss: 0.4235 - auc: 0.586 - 0s 22us/step - loss: 0.4229 - auc: 0.5895 - val_loss: 0.4225 - val_auc: 0.5998\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.42253 to 0.42251, saving model to DeepFM.h5\n",
      "Epoch 262/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3883 - auc: 0.629 - ETA: 0s - loss: 0.4110 - auc: 0.594 - 0s 23us/step - loss: 0.4234 - auc: 0.5839 - val_loss: 0.4225 - val_auc: 0.5968\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.42251 to 0.42248, saving model to DeepFM.h5\n",
      "Epoch 263/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4280 - auc: 0.547 - ETA: 0s - loss: 0.4183 - auc: 0.566 - 0s 20us/step - loss: 0.4251 - auc: 0.5611 - val_loss: 0.4225 - val_auc: 0.5984\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.42248 to 0.42245, saving model to DeepFM.h5\n",
      "Epoch 264/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4326 - auc: 0.560 - ETA: 0s - loss: 0.4194 - auc: 0.561 - 0s 20us/step - loss: 0.4245 - auc: 0.5672 - val_loss: 0.4224 - val_auc: 0.5973\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.42245 to 0.42242, saving model to DeepFM.h5\n",
      "Epoch 265/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4809 - auc: 0.489 - ETA: 0s - loss: 0.4258 - auc: 0.560 - 0s 24us/step - loss: 0.4247 - auc: 0.5690 - val_loss: 0.4224 - val_auc: 0.5972\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.42242 to 0.42239, saving model to DeepFM.h5\n",
      "Epoch 266/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3901 - auc: 0.488 - ETA: 0s - loss: 0.4327 - auc: 0.537 - 0s 20us/step - loss: 0.4263 - auc: 0.5460 - val_loss: 0.4224 - val_auc: 0.5993\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.42239 to 0.42236, saving model to DeepFM.h5\n",
      "Epoch 267/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4913 - auc: 0.539 - ETA: 0s - loss: 0.4246 - auc: 0.556 - 0s 22us/step - loss: 0.4257 - auc: 0.5532 - val_loss: 0.4223 - val_auc: 0.5996\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.42236 to 0.42234, saving model to DeepFM.h5\n",
      "Epoch 268/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4772 - auc: 0.528 - ETA: 0s - loss: 0.4247 - auc: 0.536 - 0s 24us/step - loss: 0.4261 - auc: 0.5447 - val_loss: 0.4223 - val_auc: 0.5985\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.42234 to 0.42231, saving model to DeepFM.h5\n",
      "Epoch 269/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3805 - auc: 0.551 - ETA: 0s - loss: 0.4178 - auc: 0.567 - 0s 19us/step - loss: 0.4245 - auc: 0.5626 - val_loss: 0.4223 - val_auc: 0.5977\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.42231 to 0.42227, saving model to DeepFM.h5\n",
      "Epoch 270/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3957 - auc: 0.584 - ETA: 0s - loss: 0.4220 - auc: 0.579 - 0s 20us/step - loss: 0.4238 - auc: 0.5769 - val_loss: 0.4222 - val_auc: 0.5965\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.42227 to 0.42224, saving model to DeepFM.h5\n",
      "Epoch 271/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3660 - auc: 0.550 - ETA: 0s - loss: 0.4256 - auc: 0.562 - 0s 22us/step - loss: 0.4241 - auc: 0.5713 - val_loss: 0.4222 - val_auc: 0.6007\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.42224 to 0.42222, saving model to DeepFM.h5\n",
      "Epoch 272/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4116 - auc: 0.577 - ETA: 0s - loss: 0.4252 - auc: 0.560 - 0s 20us/step - loss: 0.4240 - auc: 0.5702 - val_loss: 0.4222 - val_auc: 0.6014\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.42222 to 0.42219, saving model to DeepFM.h5\n",
      "Epoch 273/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4032 - auc: 0.590 - ETA: 0s - loss: 0.4306 - auc: 0.572 - 0s 25us/step - loss: 0.4243 - auc: 0.5748 - val_loss: 0.4222 - val_auc: 0.6030\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.42219 to 0.42216, saving model to DeepFM.h5\n",
      "Epoch 274/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4492 - auc: 0.615 - ETA: 0s - loss: 0.4298 - auc: 0.579 - 0s 20us/step - loss: 0.4235 - auc: 0.5817 - val_loss: 0.4221 - val_auc: 0.5998\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.42216 to 0.42213, saving model to DeepFM.h5\n",
      "Epoch 275/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4417 - auc: 0.528 - ETA: 0s - loss: 0.4339 - auc: 0.546 - 0s 20us/step - loss: 0.4255 - auc: 0.5515 - val_loss: 0.4221 - val_auc: 0.5997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00275: val_loss improved from 0.42213 to 0.42210, saving model to DeepFM.h5\n",
      "Epoch 276/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4809 - auc: 0.512 - ETA: 0s - loss: 0.4280 - auc: 0.555 - 0s 19us/step - loss: 0.4250 - auc: 0.5615 - val_loss: 0.4221 - val_auc: 0.6014\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.42210 to 0.42207, saving model to DeepFM.h5\n",
      "Epoch 277/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4122 - auc: 0.583 - ETA: 0s - loss: 0.4137 - auc: 0.550 - 0s 22us/step - loss: 0.4253 - auc: 0.5549 - val_loss: 0.4220 - val_auc: 0.6017\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.42207 to 0.42204, saving model to DeepFM.h5\n",
      "Epoch 278/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4214 - auc: 0.540 - ETA: 0s - loss: 0.4346 - auc: 0.581 - 0s 20us/step - loss: 0.4226 - auc: 0.5883 - val_loss: 0.4220 - val_auc: 0.6017\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.42204 to 0.42201, saving model to DeepFM.h5\n",
      "Epoch 279/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4847 - auc: 0.605 - ETA: 0s - loss: 0.4255 - auc: 0.560 - 0s 22us/step - loss: 0.4256 - auc: 0.5532 - val_loss: 0.4220 - val_auc: 0.6015\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.42201 to 0.42198, saving model to DeepFM.h5\n",
      "Epoch 280/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4272 - auc: 0.710 - ETA: 0s - loss: 0.4194 - auc: 0.581 - 0s 20us/step - loss: 0.4237 - auc: 0.5738 - val_loss: 0.4220 - val_auc: 0.6014\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.42198 to 0.42195, saving model to DeepFM.h5\n",
      "Epoch 281/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4395 - auc: 0.566 - ETA: 0s - loss: 0.4211 - auc: 0.557 - 0s 20us/step - loss: 0.4250 - auc: 0.5604 - val_loss: 0.4219 - val_auc: 0.6048\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.42195 to 0.42192, saving model to DeepFM.h5\n",
      "Epoch 282/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4119 - auc: 0.568 - ETA: 0s - loss: 0.4237 - auc: 0.555 - 0s 23us/step - loss: 0.4254 - auc: 0.5545 - val_loss: 0.4219 - val_auc: 0.6045\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.42192 to 0.42189, saving model to DeepFM.h5\n",
      "Epoch 283/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4384 - auc: 0.572 - ETA: 0s - loss: 0.4263 - auc: 0.551 - 0s 21us/step - loss: 0.4256 - auc: 0.5509 - val_loss: 0.4219 - val_auc: 0.6043\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.42189 to 0.42186, saving model to DeepFM.h5\n",
      "Epoch 284/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4685 - auc: 0.624 - ETA: 0s - loss: 0.4192 - auc: 0.565 - 0s 22us/step - loss: 0.4246 - auc: 0.5627 - val_loss: 0.4218 - val_auc: 0.6031\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.42186 to 0.42183, saving model to DeepFM.h5\n",
      "Epoch 285/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4388 - auc: 0.578 - ETA: 0s - loss: 0.4274 - auc: 0.598 - 0s 22us/step - loss: 0.4228 - auc: 0.5887 - val_loss: 0.4218 - val_auc: 0.6048\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.42183 to 0.42181, saving model to DeepFM.h5\n",
      "Epoch 286/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4199 - auc: 0.464 - ETA: 0s - loss: 0.4223 - auc: 0.561 - 0s 23us/step - loss: 0.4247 - auc: 0.5633 - val_loss: 0.4218 - val_auc: 0.6041\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.42181 to 0.42178, saving model to DeepFM.h5\n",
      "Epoch 287/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3589 - auc: 0.645 - ETA: 0s - loss: 0.4317 - auc: 0.581 - 0s 20us/step - loss: 0.4231 - auc: 0.5840 - val_loss: 0.4217 - val_auc: 0.6036\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.42178 to 0.42174, saving model to DeepFM.h5\n",
      "Epoch 288/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4079 - auc: 0.623 - ETA: 0s - loss: 0.4220 - auc: 0.590 - 0s 21us/step - loss: 0.4229 - auc: 0.5864 - val_loss: 0.4217 - val_auc: 0.6053\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.42174 to 0.42171, saving model to DeepFM.h5\n",
      "Epoch 289/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4608 - auc: 0.556 - ETA: 0s - loss: 0.4165 - auc: 0.577 - 0s 22us/step - loss: 0.4241 - auc: 0.5679 - val_loss: 0.4217 - val_auc: 0.6032\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.42171 to 0.42168, saving model to DeepFM.h5\n",
      "Epoch 290/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3891 - auc: 0.697 - ETA: 0s - loss: 0.4295 - auc: 0.606 - 0s 22us/step - loss: 0.4217 - auc: 0.5961 - val_loss: 0.4216 - val_auc: 0.6033\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.42168 to 0.42165, saving model to DeepFM.h5\n",
      "Epoch 291/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4288 - auc: 0.591 - ETA: 0s - loss: 0.4282 - auc: 0.577 - 0s 21us/step - loss: 0.4238 - auc: 0.5733 - val_loss: 0.4216 - val_auc: 0.6023\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.42165 to 0.42162, saving model to DeepFM.h5\n",
      "Epoch 292/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3956 - auc: 0.586 - ETA: 0s - loss: 0.4253 - auc: 0.574 - 0s 22us/step - loss: 0.4242 - auc: 0.5621 - val_loss: 0.4216 - val_auc: 0.6032\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.42162 to 0.42159, saving model to DeepFM.h5\n",
      "Epoch 293/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4104 - auc: 0.581 - ETA: 0s - loss: 0.4312 - auc: 0.573 - 0s 20us/step - loss: 0.4243 - auc: 0.5684 - val_loss: 0.4216 - val_auc: 0.6023\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.42159 to 0.42155, saving model to DeepFM.h5\n",
      "Epoch 294/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3908 - auc: 0.577 - ETA: 0s - loss: 0.4204 - auc: 0.583 - 0s 22us/step - loss: 0.4231 - auc: 0.5819 - val_loss: 0.4215 - val_auc: 0.6040\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.42155 to 0.42152, saving model to DeepFM.h5\n",
      "Epoch 295/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4846 - auc: 0.599 - ETA: 0s - loss: 0.4221 - auc: 0.583 - 0s 20us/step - loss: 0.4243 - auc: 0.5678 - val_loss: 0.4215 - val_auc: 0.6037\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.42152 to 0.42149, saving model to DeepFM.h5\n",
      "Epoch 296/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4368 - auc: 0.597 - ETA: 0s - loss: 0.4153 - auc: 0.588 - 0s 21us/step - loss: 0.4235 - auc: 0.5778 - val_loss: 0.4215 - val_auc: 0.6037\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.42149 to 0.42146, saving model to DeepFM.h5\n",
      "Epoch 297/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4221 - auc: 0.575 - ETA: 0s - loss: 0.4311 - auc: 0.558 - 0s 20us/step - loss: 0.4243 - auc: 0.5638 - val_loss: 0.4214 - val_auc: 0.6061\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.42146 to 0.42143, saving model to DeepFM.h5\n",
      "Epoch 298/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4363 - auc: 0.535 - ETA: 0s - loss: 0.4264 - auc: 0.565 - 0s 21us/step - loss: 0.4245 - auc: 0.5656 - val_loss: 0.4214 - val_auc: 0.6059\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.42143 to 0.42140, saving model to DeepFM.h5\n",
      "Epoch 299/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3785 - auc: 0.571 - ETA: 0s - loss: 0.4077 - auc: 0.538 - 0s 23us/step - loss: 0.4255 - auc: 0.5560 - val_loss: 0.4214 - val_auc: 0.6004\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.42140 to 0.42136, saving model to DeepFM.h5\n",
      "Epoch 300/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4509 - auc: 0.587 - ETA: 0s - loss: 0.4210 - auc: 0.571 - 0s 20us/step - loss: 0.4245 - auc: 0.5626 - val_loss: 0.4213 - val_auc: 0.6014\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.42136 to 0.42134, saving model to DeepFM.h5\n",
      "Epoch 301/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4440 - auc: 0.587 - ETA: 0s - loss: 0.4208 - auc: 0.599 - 0s 22us/step - loss: 0.4226 - auc: 0.5828 - val_loss: 0.4213 - val_auc: 0.6039\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.42134 to 0.42130, saving model to DeepFM.h5\n",
      "Epoch 302/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5154 - auc: 0.557 - ETA: 0s - loss: 0.4244 - auc: 0.572 - 0s 20us/step - loss: 0.4240 - auc: 0.5665 - val_loss: 0.4213 - val_auc: 0.6043\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.42130 to 0.42127, saving model to DeepFM.h5\n",
      "Epoch 303/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.561 - ETA: 0s - loss: 0.4235 - auc: 0.571 - 0s 22us/step - loss: 0.4239 - auc: 0.5700 - val_loss: 0.4212 - val_auc: 0.6048\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.42127 to 0.42125, saving model to DeepFM.h5\n",
      "Epoch 304/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4490 - auc: 0.538 - ETA: 0s - loss: 0.4255 - auc: 0.574 - 0s 22us/step - loss: 0.4235 - auc: 0.5730 - val_loss: 0.4212 - val_auc: 0.6059\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.42125 to 0.42122, saving model to DeepFM.h5\n",
      "Epoch 305/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4700 - auc: 0.512 - ETA: 0s - loss: 0.4228 - auc: 0.565 - 0s 22us/step - loss: 0.4231 - auc: 0.5736 - val_loss: 0.4212 - val_auc: 0.6044\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.42122 to 0.42119, saving model to DeepFM.h5\n",
      "Epoch 306/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3828 - auc: 0.682 - ETA: 0s - loss: 0.4250 - auc: 0.565 - 0s 20us/step - loss: 0.4243 - auc: 0.5617 - val_loss: 0.4212 - val_auc: 0.6032\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.42119 to 0.42116, saving model to DeepFM.h5\n",
      "Epoch 307/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4504 - auc: 0.573 - ETA: 0s - loss: 0.4318 - auc: 0.576 - 0s 20us/step - loss: 0.4240 - auc: 0.5643 - val_loss: 0.4211 - val_auc: 0.6047\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.42116 to 0.42113, saving model to DeepFM.h5\n",
      "Epoch 308/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4191 - auc: 0.471 - ETA: 0s - loss: 0.4149 - auc: 0.554 - 0s 21us/step - loss: 0.4241 - auc: 0.5706 - val_loss: 0.4211 - val_auc: 0.6052\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.42113 to 0.42110, saving model to DeepFM.h5\n",
      "Epoch 309/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4439 - auc: 0.586 - ETA: 0s - loss: 0.4199 - auc: 0.589 - 0s 20us/step - loss: 0.4226 - auc: 0.5837 - val_loss: 0.4211 - val_auc: 0.6032\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.42110 to 0.42106, saving model to DeepFM.h5\n",
      "Epoch 310/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4137 - auc: 0.635 - ETA: 0s - loss: 0.4279 - auc: 0.594 - 0s 22us/step - loss: 0.4230 - auc: 0.5772 - val_loss: 0.4210 - val_auc: 0.6053\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.42106 to 0.42103, saving model to DeepFM.h5\n",
      "Epoch 311/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4210 - auc: 0.629 - ETA: 0s - loss: 0.4136 - auc: 0.592 - 0s 19us/step - loss: 0.4230 - auc: 0.5797 - val_loss: 0.4210 - val_auc: 0.6037\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.42103 to 0.42100, saving model to DeepFM.h5\n",
      "Epoch 312/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4312 - auc: 0.650 - ETA: 0s - loss: 0.4244 - auc: 0.590 - 0s 21us/step - loss: 0.4232 - auc: 0.5762 - val_loss: 0.4210 - val_auc: 0.6068\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.42100 to 0.42097, saving model to DeepFM.h5\n",
      "Epoch 313/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4376 - auc: 0.595 - ETA: 0s - loss: 0.4334 - auc: 0.561 - 0s 20us/step - loss: 0.4242 - auc: 0.5649 - val_loss: 0.4209 - val_auc: 0.6065\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.42097 to 0.42094, saving model to DeepFM.h5\n",
      "Epoch 314/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5029 - auc: 0.504 - ETA: 0s - loss: 0.4224 - auc: 0.576 - 0s 22us/step - loss: 0.4244 - auc: 0.5643 - val_loss: 0.4209 - val_auc: 0.6038\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.42094 to 0.42090, saving model to DeepFM.h5\n",
      "Epoch 315/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3419 - auc: 0.610 - ETA: 0s - loss: 0.4145 - auc: 0.579 - 0s 21us/step - loss: 0.4228 - auc: 0.5776 - val_loss: 0.4209 - val_auc: 0.6069\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.42090 to 0.42088, saving model to DeepFM.h5\n",
      "Epoch 316/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3961 - auc: 0.581 - ETA: 0s - loss: 0.4261 - auc: 0.566 - 0s 21us/step - loss: 0.4243 - auc: 0.5658 - val_loss: 0.4208 - val_auc: 0.6052\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.42088 to 0.42085, saving model to DeepFM.h5\n",
      "Epoch 317/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3992 - auc: 0.548 - ETA: 0s - loss: 0.4257 - auc: 0.569 - 0s 21us/step - loss: 0.4233 - auc: 0.5798 - val_loss: 0.4208 - val_auc: 0.6073\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.42085 to 0.42082, saving model to DeepFM.h5\n",
      "Epoch 318/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3562 - auc: 0.587 - ETA: 0s - loss: 0.4188 - auc: 0.570 - 0s 21us/step - loss: 0.4233 - auc: 0.5751 - val_loss: 0.4208 - val_auc: 0.6065\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.42082 to 0.42079, saving model to DeepFM.h5\n",
      "Epoch 319/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4946 - auc: 0.554 - ETA: 0s - loss: 0.4243 - auc: 0.555 - 0s 22us/step - loss: 0.4241 - auc: 0.5622 - val_loss: 0.4208 - val_auc: 0.6085\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.42079 to 0.42077, saving model to DeepFM.h5\n",
      "Epoch 320/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3506 - auc: 0.560 - ETA: 0s - loss: 0.4254 - auc: 0.552 - 0s 21us/step - loss: 0.4244 - auc: 0.5621 - val_loss: 0.4207 - val_auc: 0.6055\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.42077 to 0.42074, saving model to DeepFM.h5\n",
      "Epoch 321/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3763 - auc: 0.683 - ETA: 0s - loss: 0.4257 - auc: 0.608 - 0s 23us/step - loss: 0.4220 - auc: 0.5935 - val_loss: 0.4207 - val_auc: 0.6102\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.42074 to 0.42071, saving model to DeepFM.h5\n",
      "Epoch 322/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4418 - auc: 0.593 - ETA: 0s - loss: 0.4038 - auc: 0.582 - 0s 21us/step - loss: 0.4231 - auc: 0.5762 - val_loss: 0.4207 - val_auc: 0.6084\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.42071 to 0.42068, saving model to DeepFM.h5\n",
      "Epoch 323/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4034 - auc: 0.601 - ETA: 0s - loss: 0.4253 - auc: 0.595 - 0s 21us/step - loss: 0.4221 - auc: 0.5851 - val_loss: 0.4207 - val_auc: 0.6096\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.42068 to 0.42065, saving model to DeepFM.h5\n",
      "Epoch 324/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3713 - auc: 0.538 - ETA: 0s - loss: 0.4202 - auc: 0.571 - 0s 21us/step - loss: 0.4222 - auc: 0.5853 - val_loss: 0.4206 - val_auc: 0.6103\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.42065 to 0.42062, saving model to DeepFM.h5\n",
      "Epoch 325/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4521 - auc: 0.582 - ETA: 0s - loss: 0.4211 - auc: 0.589 - 0s 22us/step - loss: 0.4223 - auc: 0.5904 - val_loss: 0.4206 - val_auc: 0.6121\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.42062 to 0.42059, saving model to DeepFM.h5\n",
      "Epoch 326/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3927 - auc: 0.540 - ETA: 0s - loss: 0.4262 - auc: 0.577 - 0s 22us/step - loss: 0.4244 - auc: 0.5608 - val_loss: 0.4206 - val_auc: 0.6097\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.42059 to 0.42055, saving model to DeepFM.h5\n",
      "Epoch 327/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4076 - auc: 0.542 - ETA: 0s - loss: 0.4085 - auc: 0.583 - 0s 26us/step - loss: 0.4222 - auc: 0.5898 - val_loss: 0.4205 - val_auc: 0.6084\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.42055 to 0.42052, saving model to DeepFM.h5\n",
      "Epoch 328/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4128 - auc: 0.549 - ETA: 0s - loss: 0.4259 - auc: 0.578 - 0s 23us/step - loss: 0.4239 - auc: 0.5705 - val_loss: 0.4205 - val_auc: 0.6076\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.42052 to 0.42048, saving model to DeepFM.h5\n",
      "Epoch 329/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4148 - auc: 0.548 - ETA: 0s - loss: 0.4114 - auc: 0.590 - 0s 23us/step - loss: 0.4218 - auc: 0.5884 - val_loss: 0.4204 - val_auc: 0.6080\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.42048 to 0.42045, saving model to DeepFM.h5\n",
      "Epoch 330/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4405 - auc: 0.633 - ETA: 0s - loss: 0.4270 - auc: 0.593 - 0s 23us/step - loss: 0.4222 - auc: 0.5925 - val_loss: 0.4204 - val_auc: 0.6129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00330: val_loss improved from 0.42045 to 0.42042, saving model to DeepFM.h5\n",
      "Epoch 331/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4047 - auc: 0.563 - ETA: 0s - loss: 0.4223 - auc: 0.558 - 0s 23us/step - loss: 0.4231 - auc: 0.5698 - val_loss: 0.4204 - val_auc: 0.6141\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.42042 to 0.42039, saving model to DeepFM.h5\n",
      "Epoch 332/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4502 - auc: 0.533 - ETA: 0s - loss: 0.4229 - auc: 0.567 - 0s 24us/step - loss: 0.4229 - auc: 0.5793 - val_loss: 0.4204 - val_auc: 0.6146\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.42039 to 0.42037, saving model to DeepFM.h5\n",
      "Epoch 333/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3984 - auc: 0.554 - ETA: 0s - loss: 0.4171 - auc: 0.569 - 0s 22us/step - loss: 0.4226 - auc: 0.5836 - val_loss: 0.4203 - val_auc: 0.6151\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.42037 to 0.42034, saving model to DeepFM.h5\n",
      "Epoch 334/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4471 - auc: 0.613 - ETA: 0s - loss: 0.4265 - auc: 0.566 - 0s 22us/step - loss: 0.4229 - auc: 0.5786 - val_loss: 0.4203 - val_auc: 0.6143\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.42034 to 0.42031, saving model to DeepFM.h5\n",
      "Epoch 335/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3980 - auc: 0.640 - ETA: 0s - loss: 0.4258 - auc: 0.596 - 0s 26us/step - loss: 0.4221 - auc: 0.5850 - val_loss: 0.4203 - val_auc: 0.6146\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.42031 to 0.42027, saving model to DeepFM.h5\n",
      "Epoch 336/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4660 - auc: 0.483 - ETA: 0s - loss: 0.4425 - auc: 0.561 - 0s 24us/step - loss: 0.4240 - auc: 0.5653 - val_loss: 0.4202 - val_auc: 0.6147\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.42027 to 0.42024, saving model to DeepFM.h5\n",
      "Epoch 337/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4164 - auc: 0.579 - ETA: 0s - loss: 0.4204 - auc: 0.583 - 0s 23us/step - loss: 0.4221 - auc: 0.5853 - val_loss: 0.4202 - val_auc: 0.6137\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.42024 to 0.42020, saving model to DeepFM.h5\n",
      "Epoch 338/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4166 - auc: 0.600 - ETA: 0s - loss: 0.4295 - auc: 0.586 - 0s 20us/step - loss: 0.4228 - auc: 0.5830 - val_loss: 0.4202 - val_auc: 0.6130\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.42020 to 0.42017, saving model to DeepFM.h5\n",
      "Epoch 339/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3856 - auc: 0.626 - ETA: 0s - loss: 0.4159 - auc: 0.594 - 0s 21us/step - loss: 0.4200 - auc: 0.6106 - val_loss: 0.4201 - val_auc: 0.6094\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.42017 to 0.42014, saving model to DeepFM.h5\n",
      "Epoch 340/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4876 - auc: 0.572 - ETA: 0s - loss: 0.4257 - auc: 0.591 - 0s 21us/step - loss: 0.4213 - auc: 0.5920 - val_loss: 0.4201 - val_auc: 0.6093\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.42014 to 0.42010, saving model to DeepFM.h5\n",
      "Epoch 341/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3595 - auc: 0.628 - ETA: 0s - loss: 0.4214 - auc: 0.595 - 0s 21us/step - loss: 0.4215 - auc: 0.5931 - val_loss: 0.4201 - val_auc: 0.6094\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.42010 to 0.42006, saving model to DeepFM.h5\n",
      "Epoch 342/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4418 - auc: 0.544 - ETA: 0s - loss: 0.4293 - auc: 0.569 - 0s 22us/step - loss: 0.4235 - auc: 0.5681 - val_loss: 0.4200 - val_auc: 0.6079\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.42006 to 0.42003, saving model to DeepFM.h5\n",
      "Epoch 343/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4292 - auc: 0.661 - ETA: 0s - loss: 0.4155 - auc: 0.597 - 0s 21us/step - loss: 0.4210 - auc: 0.5964 - val_loss: 0.4200 - val_auc: 0.6113\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.42003 to 0.42000, saving model to DeepFM.h5\n",
      "Epoch 344/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4169 - auc: 0.508 - ETA: 0s - loss: 0.4229 - auc: 0.584 - 0s 22us/step - loss: 0.4226 - auc: 0.5818 - val_loss: 0.4200 - val_auc: 0.6107\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.42000 to 0.41997, saving model to DeepFM.h5\n",
      "Epoch 345/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4723 - auc: 0.580 - ETA: 0s - loss: 0.4364 - auc: 0.578 - 0s 22us/step - loss: 0.4229 - auc: 0.5766 - val_loss: 0.4199 - val_auc: 0.6092\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.41997 to 0.41993, saving model to DeepFM.h5\n",
      "Epoch 346/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4035 - auc: 0.580 - ETA: 0s - loss: 0.4158 - auc: 0.570 - 0s 21us/step - loss: 0.4228 - auc: 0.5767 - val_loss: 0.4199 - val_auc: 0.6119\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.41993 to 0.41990, saving model to DeepFM.h5\n",
      "Epoch 347/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3868 - auc: 0.545 - ETA: 0s - loss: 0.4300 - auc: 0.579 - 0s 23us/step - loss: 0.4222 - auc: 0.5799 - val_loss: 0.4199 - val_auc: 0.6094\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.41990 to 0.41987, saving model to DeepFM.h5\n",
      "Epoch 348/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4068 - auc: 0.558 - ETA: 0s - loss: 0.4209 - auc: 0.581 - 0s 21us/step - loss: 0.4225 - auc: 0.5793 - val_loss: 0.4198 - val_auc: 0.6108\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.41987 to 0.41984, saving model to DeepFM.h5\n",
      "Epoch 349/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4535 - auc: 0.603 - ETA: 0s - loss: 0.4176 - auc: 0.563 - 0s 21us/step - loss: 0.4230 - auc: 0.5723 - val_loss: 0.4198 - val_auc: 0.6095\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.41984 to 0.41981, saving model to DeepFM.h5\n",
      "Epoch 350/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4734 - auc: 0.557 - ETA: 0s - loss: 0.4291 - auc: 0.570 - 0s 22us/step - loss: 0.4228 - auc: 0.5691 - val_loss: 0.4198 - val_auc: 0.6119\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.41981 to 0.41978, saving model to DeepFM.h5\n",
      "Epoch 351/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3550 - auc: 0.592 - ETA: 0s - loss: 0.4088 - auc: 0.611 - 0s 22us/step - loss: 0.4210 - auc: 0.5990 - val_loss: 0.4197 - val_auc: 0.6109\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.41978 to 0.41974, saving model to DeepFM.h5\n",
      "Epoch 352/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4550 - auc: 0.544 - ETA: 0s - loss: 0.4291 - auc: 0.568 - 0s 21us/step - loss: 0.4227 - auc: 0.5770 - val_loss: 0.4197 - val_auc: 0.6100\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.41974 to 0.41972, saving model to DeepFM.h5\n",
      "Epoch 353/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4434 - auc: 0.508 - ETA: 0s - loss: 0.4185 - auc: 0.573 - 0s 21us/step - loss: 0.4236 - auc: 0.5671 - val_loss: 0.4197 - val_auc: 0.6094\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.41972 to 0.41969, saving model to DeepFM.h5\n",
      "Epoch 354/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4365 - auc: 0.520 - ETA: 0s - loss: 0.4156 - auc: 0.567 - 0s 23us/step - loss: 0.4227 - auc: 0.5756 - val_loss: 0.4197 - val_auc: 0.6093\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.41969 to 0.41966, saving model to DeepFM.h5\n",
      "Epoch 355/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4063 - auc: 0.627 - ETA: 0s - loss: 0.4257 - auc: 0.580 - 0s 22us/step - loss: 0.4214 - auc: 0.5928 - val_loss: 0.4196 - val_auc: 0.6089\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.41966 to 0.41962, saving model to DeepFM.h5\n",
      "Epoch 356/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4083 - auc: 0.601 - ETA: 0s - loss: 0.4382 - auc: 0.566 - 0s 21us/step - loss: 0.4229 - auc: 0.5709 - val_loss: 0.4196 - val_auc: 0.6096\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.41962 to 0.41960, saving model to DeepFM.h5\n",
      "Epoch 357/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4443 - auc: 0.585 - ETA: 0s - loss: 0.4195 - auc: 0.585 - 0s 21us/step - loss: 0.4227 - auc: 0.5767 - val_loss: 0.4196 - val_auc: 0.6097\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.41960 to 0.41957, saving model to DeepFM.h5\n",
      "Epoch 358/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3832 - auc: 0.576 - ETA: 0s - loss: 0.4180 - auc: 0.578 - 0s 22us/step - loss: 0.4228 - auc: 0.5737 - val_loss: 0.4195 - val_auc: 0.6099\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.41957 to 0.41953, saving model to DeepFM.h5\n",
      "Epoch 359/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3631 - auc: 0.661 - ETA: 0s - loss: 0.4266 - auc: 0.587 - 0s 21us/step - loss: 0.4226 - auc: 0.5764 - val_loss: 0.4195 - val_auc: 0.6093\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.41953 to 0.41950, saving model to DeepFM.h5\n",
      "Epoch 360/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4972 - auc: 0.608 - ETA: 0s - loss: 0.4245 - auc: 0.579 - 0s 21us/step - loss: 0.4228 - auc: 0.5737 - val_loss: 0.4195 - val_auc: 0.6107\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.41950 to 0.41947, saving model to DeepFM.h5\n",
      "Epoch 361/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3885 - auc: 0.585 - ETA: 0s - loss: 0.4114 - auc: 0.591 - 0s 22us/step - loss: 0.4215 - auc: 0.5917 - val_loss: 0.4194 - val_auc: 0.6094\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.41947 to 0.41944, saving model to DeepFM.h5\n",
      "Epoch 362/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4351 - auc: 0.594 - ETA: 0s - loss: 0.4273 - auc: 0.568 - 0s 22us/step - loss: 0.4222 - auc: 0.5780 - val_loss: 0.4194 - val_auc: 0.6100\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.41944 to 0.41941, saving model to DeepFM.h5\n",
      "Epoch 363/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4553 - auc: 0.610 - ETA: 0s - loss: 0.4267 - auc: 0.585 - 0s 22us/step - loss: 0.4213 - auc: 0.5912 - val_loss: 0.4194 - val_auc: 0.6109\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.41941 to 0.41938, saving model to DeepFM.h5\n",
      "Epoch 364/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4331 - auc: 0.613 - ETA: 0s - loss: 0.4320 - auc: 0.601 - 0s 21us/step - loss: 0.4203 - auc: 0.5990 - val_loss: 0.4193 - val_auc: 0.6108\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.41938 to 0.41935, saving model to DeepFM.h5\n",
      "Epoch 365/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.661 - ETA: 0s - loss: 0.4152 - auc: 0.593 - 0s 22us/step - loss: 0.4206 - auc: 0.5979 - val_loss: 0.4193 - val_auc: 0.6128\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.41935 to 0.41932, saving model to DeepFM.h5\n",
      "Epoch 366/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4571 - auc: 0.641 - ETA: 0s - loss: 0.4216 - auc: 0.611 - 0s 21us/step - loss: 0.4203 - auc: 0.6044 - val_loss: 0.4193 - val_auc: 0.6154\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.41932 to 0.41929, saving model to DeepFM.h5\n",
      "Epoch 367/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3910 - auc: 0.620 - ETA: 0s - loss: 0.4178 - auc: 0.573 - 0s 23us/step - loss: 0.4221 - auc: 0.5778 - val_loss: 0.4193 - val_auc: 0.6143\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.41929 to 0.41926, saving model to DeepFM.h5\n",
      "Epoch 368/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4693 - auc: 0.536 - ETA: 0s - loss: 0.4237 - auc: 0.585 - 0s 21us/step - loss: 0.4218 - auc: 0.5838 - val_loss: 0.4192 - val_auc: 0.6146\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.41926 to 0.41922, saving model to DeepFM.h5\n",
      "Epoch 369/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3443 - auc: 0.655 - ETA: 0s - loss: 0.4163 - auc: 0.580 - 0s 22us/step - loss: 0.4228 - auc: 0.5736 - val_loss: 0.4192 - val_auc: 0.6138\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.41922 to 0.41921, saving model to DeepFM.h5\n",
      "Epoch 370/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3841 - auc: 0.579 - ETA: 0s - loss: 0.4308 - auc: 0.588 - 0s 22us/step - loss: 0.4213 - auc: 0.5880 - val_loss: 0.4192 - val_auc: 0.6137\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.41921 to 0.41918, saving model to DeepFM.h5\n",
      "Epoch 371/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4095 - auc: 0.591 - ETA: 0s - loss: 0.4152 - auc: 0.596 - 0s 22us/step - loss: 0.4213 - auc: 0.5856 - val_loss: 0.4191 - val_auc: 0.6140\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.41918 to 0.41914, saving model to DeepFM.h5\n",
      "Epoch 372/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4258 - auc: 0.564 - ETA: 0s - loss: 0.4217 - auc: 0.614 - 0s 22us/step - loss: 0.4197 - auc: 0.6078 - val_loss: 0.4191 - val_auc: 0.6141\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.41914 to 0.41911, saving model to DeepFM.h5\n",
      "Epoch 373/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3948 - auc: 0.591 - ETA: 0s - loss: 0.4223 - auc: 0.585 - 0s 22us/step - loss: 0.4218 - auc: 0.5855 - val_loss: 0.4191 - val_auc: 0.6118\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.41911 to 0.41908, saving model to DeepFM.h5\n",
      "Epoch 374/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4204 - auc: 0.550 - ETA: 0s - loss: 0.4196 - auc: 0.574 - 0s 20us/step - loss: 0.4224 - auc: 0.5788 - val_loss: 0.4191 - val_auc: 0.6116\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.41908 to 0.41906, saving model to DeepFM.h5\n",
      "Epoch 375/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3849 - auc: 0.561 - ETA: 0s - loss: 0.4146 - auc: 0.607 - 0s 22us/step - loss: 0.4213 - auc: 0.5922 - val_loss: 0.4190 - val_auc: 0.6135\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.41906 to 0.41903, saving model to DeepFM.h5\n",
      "Epoch 376/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4178 - auc: 0.682 - ETA: 0s - loss: 0.4134 - auc: 0.577 - 0s 20us/step - loss: 0.4227 - auc: 0.5796 - val_loss: 0.4190 - val_auc: 0.6131\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.41903 to 0.41899, saving model to DeepFM.h5\n",
      "Epoch 377/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3569 - auc: 0.544 - ETA: 0s - loss: 0.4109 - auc: 0.583 - 0s 21us/step - loss: 0.4213 - auc: 0.5844 - val_loss: 0.4190 - val_auc: 0.6129\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.41899 to 0.41896, saving model to DeepFM.h5\n",
      "Epoch 378/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4362 - auc: 0.580 - ETA: 0s - loss: 0.4206 - auc: 0.580 - 0s 20us/step - loss: 0.4217 - auc: 0.5828 - val_loss: 0.4189 - val_auc: 0.6145\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.41896 to 0.41893, saving model to DeepFM.h5\n",
      "Epoch 379/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3990 - auc: 0.620 - ETA: 0s - loss: 0.4170 - auc: 0.580 - 0s 22us/step - loss: 0.4210 - auc: 0.5891 - val_loss: 0.4189 - val_auc: 0.6150\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.41893 to 0.41889, saving model to DeepFM.h5\n",
      "Epoch 380/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4088 - auc: 0.594 - ETA: 0s - loss: 0.4257 - auc: 0.589 - 0s 23us/step - loss: 0.4214 - auc: 0.5845 - val_loss: 0.4189 - val_auc: 0.6144\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.41889 to 0.41886, saving model to DeepFM.h5\n",
      "Epoch 381/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4478 - auc: 0.610 - ETA: 0s - loss: 0.4196 - auc: 0.578 - 0s 23us/step - loss: 0.4216 - auc: 0.5832 - val_loss: 0.4188 - val_auc: 0.6151\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.41886 to 0.41883, saving model to DeepFM.h5\n",
      "Epoch 382/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4071 - auc: 0.684 - ETA: 0s - loss: 0.4198 - auc: 0.593 - 0s 21us/step - loss: 0.4207 - auc: 0.5925 - val_loss: 0.4188 - val_auc: 0.6151\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.41883 to 0.41880, saving model to DeepFM.h5\n",
      "Epoch 383/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4112 - auc: 0.638 - ETA: 0s - loss: 0.4186 - auc: 0.610 - 0s 20us/step - loss: 0.4194 - auc: 0.6050 - val_loss: 0.4188 - val_auc: 0.6148\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.41880 to 0.41878, saving model to DeepFM.h5\n",
      "Epoch 384/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4872 - auc: 0.552 - ETA: 0s - loss: 0.4236 - auc: 0.564 - 0s 20us/step - loss: 0.4226 - auc: 0.5689 - val_loss: 0.4188 - val_auc: 0.6159\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.41878 to 0.41875, saving model to DeepFM.h5\n",
      "Epoch 385/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4353 - auc: 0.595 - ETA: 0s - loss: 0.4173 - auc: 0.608 - 0s 20us/step - loss: 0.4193 - auc: 0.6089 - val_loss: 0.4187 - val_auc: 0.6146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00385: val_loss improved from 0.41875 to 0.41872, saving model to DeepFM.h5\n",
      "Epoch 386/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4269 - auc: 0.681 - ETA: 0s - loss: 0.4164 - auc: 0.576 - 0s 19us/step - loss: 0.4221 - auc: 0.5779 - val_loss: 0.4187 - val_auc: 0.6163\n",
      "\n",
      "Epoch 00386: val_loss improved from 0.41872 to 0.41870, saving model to DeepFM.h5\n",
      "Epoch 387/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3657 - auc: 0.622 - ETA: 0s - loss: 0.4203 - auc: 0.608 - 0s 21us/step - loss: 0.4201 - auc: 0.6015 - val_loss: 0.4187 - val_auc: 0.6175\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.41870 to 0.41867, saving model to DeepFM.h5\n",
      "Epoch 388/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4175 - auc: 0.558 - ETA: 0s - loss: 0.4307 - auc: 0.582 - 0s 22us/step - loss: 0.4223 - auc: 0.5767 - val_loss: 0.4186 - val_auc: 0.6170\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.41867 to 0.41864, saving model to DeepFM.h5\n",
      "Epoch 389/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3883 - auc: 0.585 - ETA: 0s - loss: 0.4259 - auc: 0.607 - 0s 20us/step - loss: 0.4198 - auc: 0.6027 - val_loss: 0.4186 - val_auc: 0.6186\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.41864 to 0.41861, saving model to DeepFM.h5\n",
      "Epoch 390/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4526 - auc: 0.569 - ETA: 0s - loss: 0.4268 - auc: 0.585 - 0s 20us/step - loss: 0.4206 - auc: 0.5953 - val_loss: 0.4186 - val_auc: 0.6199\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.41861 to 0.41858, saving model to DeepFM.h5\n",
      "Epoch 391/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4222 - auc: 0.537 - ETA: 0s - loss: 0.4247 - auc: 0.588 - 0s 20us/step - loss: 0.4208 - auc: 0.5865 - val_loss: 0.4185 - val_auc: 0.6179\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.41858 to 0.41854, saving model to DeepFM.h5\n",
      "Epoch 392/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4474 - auc: 0.588 - ETA: 0s - loss: 0.4331 - auc: 0.571 - 0s 22us/step - loss: 0.4207 - auc: 0.5918 - val_loss: 0.4185 - val_auc: 0.6206\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.41854 to 0.41851, saving model to DeepFM.h5\n",
      "Epoch 393/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4540 - auc: 0.605 - ETA: 0s - loss: 0.4242 - auc: 0.607 - 0s 21us/step - loss: 0.4200 - auc: 0.5957 - val_loss: 0.4185 - val_auc: 0.6209\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.41851 to 0.41849, saving model to DeepFM.h5\n",
      "Epoch 394/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4723 - auc: 0.569 - ETA: 0s - loss: 0.4340 - auc: 0.579 - 0s 22us/step - loss: 0.4219 - auc: 0.5817 - val_loss: 0.4185 - val_auc: 0.6205\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.41849 to 0.41846, saving model to DeepFM.h5\n",
      "Epoch 395/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4340 - auc: 0.654 - ETA: 0s - loss: 0.4133 - auc: 0.581 - 0s 21us/step - loss: 0.4217 - auc: 0.5775 - val_loss: 0.4184 - val_auc: 0.6174\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.41846 to 0.41843, saving model to DeepFM.h5\n",
      "Epoch 396/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4631 - auc: 0.583 - ETA: 0s - loss: 0.4226 - auc: 0.576 - 0s 20us/step - loss: 0.4212 - auc: 0.5826 - val_loss: 0.4184 - val_auc: 0.6168\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.41843 to 0.41839, saving model to DeepFM.h5\n",
      "Epoch 397/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4315 - auc: 0.706 - ETA: 0s - loss: 0.4135 - auc: 0.599 - 0s 20us/step - loss: 0.4210 - auc: 0.5881 - val_loss: 0.4184 - val_auc: 0.6203\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.41839 to 0.41835, saving model to DeepFM.h5\n",
      "Epoch 398/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4746 - auc: 0.660 - ETA: 0s - loss: 0.4283 - auc: 0.584 - 0s 21us/step - loss: 0.4222 - auc: 0.5804 - val_loss: 0.4183 - val_auc: 0.6207\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.41835 to 0.41832, saving model to DeepFM.h5\n",
      "Epoch 399/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4194 - auc: 0.557 - ETA: 0s - loss: 0.4204 - auc: 0.586 - 0s 19us/step - loss: 0.4209 - auc: 0.5906 - val_loss: 0.4183 - val_auc: 0.6202\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.41832 to 0.41829, saving model to DeepFM.h5\n",
      "Epoch 400/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4043 - auc: 0.646 - ETA: 0s - loss: 0.4224 - auc: 0.576 - 0s 20us/step - loss: 0.4226 - auc: 0.5740 - val_loss: 0.4183 - val_auc: 0.6207\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.41829 to 0.41827, saving model to DeepFM.h5\n",
      "Epoch 401/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4077 - auc: 0.602 - ETA: 0s - loss: 0.4140 - auc: 0.608 - 0s 21us/step - loss: 0.4192 - auc: 0.6042 - val_loss: 0.4182 - val_auc: 0.6180\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.41827 to 0.41824, saving model to DeepFM.h5\n",
      "Epoch 402/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4672 - auc: 0.531 - ETA: 0s - loss: 0.4161 - auc: 0.590 - 0s 21us/step - loss: 0.4196 - auc: 0.5992 - val_loss: 0.4182 - val_auc: 0.6166\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.41824 to 0.41822, saving model to DeepFM.h5\n",
      "Epoch 403/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4802 - auc: 0.499 - ETA: 0s - loss: 0.4278 - auc: 0.588 - 0s 21us/step - loss: 0.4204 - auc: 0.5920 - val_loss: 0.4182 - val_auc: 0.6163\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.41822 to 0.41819, saving model to DeepFM.h5\n",
      "Epoch 404/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4892 - auc: 0.650 - ETA: 0s - loss: 0.4220 - auc: 0.641 - 0s 25us/step - loss: 0.4189 - auc: 0.6130 - val_loss: 0.4182 - val_auc: 0.6151\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.41819 to 0.41815, saving model to DeepFM.h5\n",
      "Epoch 405/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4059 - auc: 0.560 - ETA: 0s - loss: 0.4312 - auc: 0.585 - 0s 23us/step - loss: 0.4205 - auc: 0.5959 - val_loss: 0.4181 - val_auc: 0.6143\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.41815 to 0.41812, saving model to DeepFM.h5\n",
      "Epoch 406/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4078 - auc: 0.667 - ETA: 0s - loss: 0.4159 - auc: 0.602 - 0s 22us/step - loss: 0.4204 - auc: 0.5921 - val_loss: 0.4181 - val_auc: 0.6169\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.41812 to 0.41809, saving model to DeepFM.h5\n",
      "Epoch 407/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.644 - ETA: 0s - loss: 0.4139 - auc: 0.599 - 0s 23us/step - loss: 0.4203 - auc: 0.6000 - val_loss: 0.4181 - val_auc: 0.6180\n",
      "\n",
      "Epoch 00407: val_loss improved from 0.41809 to 0.41806, saving model to DeepFM.h5\n",
      "Epoch 408/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4227 - auc: 0.573 - ETA: 0s - loss: 0.4240 - auc: 0.594 - 0s 20us/step - loss: 0.4208 - auc: 0.5860 - val_loss: 0.4180 - val_auc: 0.6174\n",
      "\n",
      "Epoch 00408: val_loss improved from 0.41806 to 0.41802, saving model to DeepFM.h5\n",
      "Epoch 409/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4048 - auc: 0.645 - ETA: 0s - loss: 0.4322 - auc: 0.586 - 0s 23us/step - loss: 0.4196 - auc: 0.5932 - val_loss: 0.4180 - val_auc: 0.6165\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.41802 to 0.41799, saving model to DeepFM.h5\n",
      "Epoch 410/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3939 - auc: 0.608 - ETA: 0s - loss: 0.4203 - auc: 0.604 - 0s 22us/step - loss: 0.4190 - auc: 0.6050 - val_loss: 0.4180 - val_auc: 0.6177\n",
      "\n",
      "Epoch 00410: val_loss improved from 0.41799 to 0.41797, saving model to DeepFM.h5\n",
      "Epoch 411/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4465 - auc: 0.622 - ETA: 0s - loss: 0.4149 - auc: 0.581 - 0s 21us/step - loss: 0.4217 - auc: 0.5824 - val_loss: 0.4179 - val_auc: 0.6163\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.41797 to 0.41793, saving model to DeepFM.h5\n",
      "Epoch 412/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4494 - auc: 0.511 - ETA: 0s - loss: 0.4255 - auc: 0.569 - 0s 22us/step - loss: 0.4212 - auc: 0.5836 - val_loss: 0.4179 - val_auc: 0.6154\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.41793 to 0.41790, saving model to DeepFM.h5\n",
      "Epoch 413/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4018 - auc: 0.604 - ETA: 0s - loss: 0.4274 - auc: 0.579 - 0s 21us/step - loss: 0.4220 - auc: 0.5797 - val_loss: 0.4179 - val_auc: 0.6149\n",
      "\n",
      "Epoch 00413: val_loss improved from 0.41790 to 0.41787, saving model to DeepFM.h5\n",
      "Epoch 414/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4412 - auc: 0.655 - ETA: 0s - loss: 0.4236 - auc: 0.596 - 0s 20us/step - loss: 0.4196 - auc: 0.5965 - val_loss: 0.4178 - val_auc: 0.6150\n",
      "\n",
      "Epoch 00414: val_loss improved from 0.41787 to 0.41783, saving model to DeepFM.h5\n",
      "Epoch 415/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4483 - auc: 0.590 - ETA: 0s - loss: 0.4258 - auc: 0.599 - 0s 23us/step - loss: 0.4198 - auc: 0.5990 - val_loss: 0.4178 - val_auc: 0.6152\n",
      "\n",
      "Epoch 00415: val_loss improved from 0.41783 to 0.41780, saving model to DeepFM.h5\n",
      "Epoch 416/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4621 - auc: 0.668 - ETA: 0s - loss: 0.4184 - auc: 0.577 - 0s 21us/step - loss: 0.4208 - auc: 0.5901 - val_loss: 0.4178 - val_auc: 0.6168\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.41780 to 0.41777, saving model to DeepFM.h5\n",
      "Epoch 417/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3849 - auc: 0.693 - ETA: 0s - loss: 0.4111 - auc: 0.603 - 0s 20us/step - loss: 0.4192 - auc: 0.6040 - val_loss: 0.4177 - val_auc: 0.6159\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.41777 to 0.41774, saving model to DeepFM.h5\n",
      "Epoch 418/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4184 - auc: 0.547 - ETA: 0s - loss: 0.4184 - auc: 0.584 - 0s 20us/step - loss: 0.4195 - auc: 0.5979 - val_loss: 0.4177 - val_auc: 0.6159\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.41774 to 0.41770, saving model to DeepFM.h5\n",
      "Epoch 419/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4168 - auc: 0.581 - ETA: 0s - loss: 0.4268 - auc: 0.581 - 0s 19us/step - loss: 0.4218 - auc: 0.5780 - val_loss: 0.4177 - val_auc: 0.6164\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.41770 to 0.41768, saving model to DeepFM.h5\n",
      "Epoch 420/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4962 - auc: 0.600 - ETA: 0s - loss: 0.4243 - auc: 0.581 - 0s 21us/step - loss: 0.4209 - auc: 0.5889 - val_loss: 0.4176 - val_auc: 0.6157\n",
      "\n",
      "Epoch 00420: val_loss improved from 0.41768 to 0.41765, saving model to DeepFM.h5\n",
      "Epoch 421/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4188 - auc: 0.674 - ETA: 0s - loss: 0.4225 - auc: 0.601 - 0s 21us/step - loss: 0.4202 - auc: 0.5910 - val_loss: 0.4176 - val_auc: 0.6187\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.41765 to 0.41761, saving model to DeepFM.h5\n",
      "Epoch 422/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3900 - auc: 0.631 - ETA: 0s - loss: 0.4130 - auc: 0.601 - 0s 21us/step - loss: 0.4193 - auc: 0.6013 - val_loss: 0.4176 - val_auc: 0.6200\n",
      "\n",
      "Epoch 00422: val_loss improved from 0.41761 to 0.41758, saving model to DeepFM.h5\n",
      "Epoch 423/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4614 - auc: 0.587 - ETA: 0s - loss: 0.4297 - auc: 0.583 - 0s 21us/step - loss: 0.4208 - auc: 0.5855 - val_loss: 0.4175 - val_auc: 0.6213\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.41758 to 0.41755, saving model to DeepFM.h5\n",
      "Epoch 424/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4378 - auc: 0.643 - ETA: 0s - loss: 0.4206 - auc: 0.614 - 0s 20us/step - loss: 0.4187 - auc: 0.6060 - val_loss: 0.4175 - val_auc: 0.6180\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.41755 to 0.41752, saving model to DeepFM.h5\n",
      "Epoch 425/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4538 - auc: 0.671 - ETA: 0s - loss: 0.4063 - auc: 0.614 - 0s 21us/step - loss: 0.4175 - auc: 0.6212 - val_loss: 0.4175 - val_auc: 0.6195\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.41752 to 0.41749, saving model to DeepFM.h5\n",
      "Epoch 426/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4351 - auc: 0.555 - ETA: 0s - loss: 0.4275 - auc: 0.579 - 0s 21us/step - loss: 0.4202 - auc: 0.5970 - val_loss: 0.4175 - val_auc: 0.6207\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.41749 to 0.41746, saving model to DeepFM.h5\n",
      "Epoch 427/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3970 - auc: 0.643 - ETA: 0s - loss: 0.4224 - auc: 0.620 - 0s 21us/step - loss: 0.4183 - auc: 0.6113 - val_loss: 0.4174 - val_auc: 0.6202\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.41746 to 0.41744, saving model to DeepFM.h5\n",
      "Epoch 428/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4547 - auc: 0.639 - ETA: 0s - loss: 0.4107 - auc: 0.618 - 0s 22us/step - loss: 0.4184 - auc: 0.6047 - val_loss: 0.4174 - val_auc: 0.6187\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.41744 to 0.41741, saving model to DeepFM.h5\n",
      "Epoch 429/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3929 - auc: 0.632 - ETA: 0s - loss: 0.4088 - auc: 0.595 - 0s 21us/step - loss: 0.4192 - auc: 0.6069 - val_loss: 0.4174 - val_auc: 0.6203\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.41741 to 0.41737, saving model to DeepFM.h5\n",
      "Epoch 430/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4196 - auc: 0.620 - ETA: 0s - loss: 0.4250 - auc: 0.612 - 0s 22us/step - loss: 0.4188 - auc: 0.6113 - val_loss: 0.4173 - val_auc: 0.6214\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.41737 to 0.41734, saving model to DeepFM.h5\n",
      "Epoch 431/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3924 - auc: 0.535 - ETA: 0s - loss: 0.4234 - auc: 0.614 - 0s 21us/step - loss: 0.4185 - auc: 0.6100 - val_loss: 0.4173 - val_auc: 0.6206\n",
      "\n",
      "Epoch 00431: val_loss improved from 0.41734 to 0.41731, saving model to DeepFM.h5\n",
      "Epoch 432/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4103 - auc: 0.624 - ETA: 0s - loss: 0.4163 - auc: 0.586 - 0s 22us/step - loss: 0.4193 - auc: 0.5967 - val_loss: 0.4173 - val_auc: 0.6206\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.41731 to 0.41729, saving model to DeepFM.h5\n",
      "Epoch 433/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3986 - auc: 0.614 - ETA: 0s - loss: 0.4253 - auc: 0.596 - 0s 24us/step - loss: 0.4188 - auc: 0.6016 - val_loss: 0.4172 - val_auc: 0.6240\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.41729 to 0.41725, saving model to DeepFM.h5\n",
      "Epoch 434/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3722 - auc: 0.616 - ETA: 0s - loss: 0.4121 - auc: 0.605 - 0s 22us/step - loss: 0.4187 - auc: 0.6042 - val_loss: 0.4172 - val_auc: 0.6229\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.41725 to 0.41721, saving model to DeepFM.h5\n",
      "Epoch 435/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4747 - auc: 0.560 - ETA: 0s - loss: 0.4109 - auc: 0.607 - 0s 22us/step - loss: 0.4196 - auc: 0.5987 - val_loss: 0.4172 - val_auc: 0.6239\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.41721 to 0.41718, saving model to DeepFM.h5\n",
      "Epoch 436/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3888 - auc: 0.648 - ETA: 0s - loss: 0.4240 - auc: 0.606 - 0s 25us/step - loss: 0.4196 - auc: 0.5969 - val_loss: 0.4171 - val_auc: 0.6254\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.41718 to 0.41714, saving model to DeepFM.h5\n",
      "Epoch 437/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3971 - auc: 0.576 - ETA: 0s - loss: 0.4179 - auc: 0.589 - 0s 24us/step - loss: 0.4191 - auc: 0.6008 - val_loss: 0.4171 - val_auc: 0.6257\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.41714 to 0.41711, saving model to DeepFM.h5\n",
      "Epoch 438/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4808 - auc: 0.662 - ETA: 0s - loss: 0.4234 - auc: 0.585 - 0s 20us/step - loss: 0.4195 - auc: 0.5997 - val_loss: 0.4171 - val_auc: 0.6255\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.41711 to 0.41708, saving model to DeepFM.h5\n",
      "Epoch 439/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4552 - auc: 0.651 - ETA: 0s - loss: 0.4209 - auc: 0.601 - 0s 23us/step - loss: 0.4196 - auc: 0.5970 - val_loss: 0.4170 - val_auc: 0.6257\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.41708 to 0.41705, saving model to DeepFM.h5\n",
      "Epoch 440/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4278 - auc: 0.565 - ETA: 0s - loss: 0.4187 - auc: 0.610 - 0s 23us/step - loss: 0.4188 - auc: 0.6033 - val_loss: 0.4170 - val_auc: 0.6241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00440: val_loss improved from 0.41705 to 0.41702, saving model to DeepFM.h5\n",
      "Epoch 441/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4256 - auc: 0.612 - ETA: 0s - loss: 0.4155 - auc: 0.623 - 0s 22us/step - loss: 0.4175 - auc: 0.6152 - val_loss: 0.4170 - val_auc: 0.6211\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.41702 to 0.41699, saving model to DeepFM.h5\n",
      "Epoch 442/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4278 - auc: 0.609 - ETA: 0s - loss: 0.4373 - auc: 0.579 - 0s 27us/step - loss: 0.4200 - auc: 0.5898 - val_loss: 0.4170 - val_auc: 0.6252\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.41699 to 0.41696, saving model to DeepFM.h5\n",
      "Epoch 443/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4312 - auc: 0.630 - ETA: 0s - loss: 0.4068 - auc: 0.602 - 0s 24us/step - loss: 0.4202 - auc: 0.5895 - val_loss: 0.4169 - val_auc: 0.6264\n",
      "\n",
      "Epoch 00443: val_loss improved from 0.41696 to 0.41692, saving model to DeepFM.h5\n",
      "Epoch 444/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3887 - auc: 0.579 - ETA: 0s - loss: 0.4193 - auc: 0.575 - 0s 22us/step - loss: 0.4203 - auc: 0.5866 - val_loss: 0.4169 - val_auc: 0.6285\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.41692 to 0.41689, saving model to DeepFM.h5\n",
      "Epoch 445/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4640 - auc: 0.624 - ETA: 0s - loss: 0.4210 - auc: 0.611 - 0s 21us/step - loss: 0.4183 - auc: 0.6067 - val_loss: 0.4169 - val_auc: 0.6273\n",
      "\n",
      "Epoch 00445: val_loss improved from 0.41689 to 0.41686, saving model to DeepFM.h5\n",
      "Epoch 446/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4488 - auc: 0.606 - ETA: 0s - loss: 0.4205 - auc: 0.591 - 0s 21us/step - loss: 0.4192 - auc: 0.6009 - val_loss: 0.4168 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.41686 to 0.41684, saving model to DeepFM.h5\n",
      "Epoch 447/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.640 - ETA: 0s - loss: 0.4110 - auc: 0.599 - 0s 19us/step - loss: 0.4187 - auc: 0.6064 - val_loss: 0.4168 - val_auc: 0.6270\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.41684 to 0.41680, saving model to DeepFM.h5\n",
      "Epoch 448/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4320 - auc: 0.693 - ETA: 0s - loss: 0.4206 - auc: 0.618 - 0s 20us/step - loss: 0.4176 - auc: 0.6193 - val_loss: 0.4168 - val_auc: 0.6282\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.41680 to 0.41677, saving model to DeepFM.h5\n",
      "Epoch 449/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4019 - auc: 0.649 - ETA: 0s - loss: 0.4180 - auc: 0.600 - 0s 22us/step - loss: 0.4196 - auc: 0.5909 - val_loss: 0.4167 - val_auc: 0.6278\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.41677 to 0.41673, saving model to DeepFM.h5\n",
      "Epoch 450/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4393 - auc: 0.552 - ETA: 0s - loss: 0.4188 - auc: 0.603 - 0s 24us/step - loss: 0.4185 - auc: 0.6090 - val_loss: 0.4167 - val_auc: 0.6262\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.41673 to 0.41669, saving model to DeepFM.h5\n",
      "Epoch 451/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4957 - auc: 0.595 - ETA: 0s - loss: 0.4228 - auc: 0.594 - 0s 20us/step - loss: 0.4204 - auc: 0.5890 - val_loss: 0.4167 - val_auc: 0.6279\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.41669 to 0.41666, saving model to DeepFM.h5\n",
      "Epoch 452/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3486 - auc: 0.566 - ETA: 0s - loss: 0.4090 - auc: 0.587 - 0s 20us/step - loss: 0.4194 - auc: 0.6007 - val_loss: 0.4166 - val_auc: 0.6259\n",
      "\n",
      "Epoch 00452: val_loss improved from 0.41666 to 0.41663, saving model to DeepFM.h5\n",
      "Epoch 453/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4260 - auc: 0.612 - ETA: 0s - loss: 0.4186 - auc: 0.580 - 0s 21us/step - loss: 0.4205 - auc: 0.5838 - val_loss: 0.4166 - val_auc: 0.6267\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.41663 to 0.41660, saving model to DeepFM.h5\n",
      "Epoch 454/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4323 - auc: 0.672 - ETA: 0s - loss: 0.4263 - auc: 0.603 - 0s 22us/step - loss: 0.4186 - auc: 0.6007 - val_loss: 0.4166 - val_auc: 0.6275\n",
      "\n",
      "Epoch 00454: val_loss improved from 0.41660 to 0.41658, saving model to DeepFM.h5\n",
      "Epoch 455/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4873 - auc: 0.612 - ETA: 0s - loss: 0.4162 - auc: 0.603 - 0s 20us/step - loss: 0.4178 - auc: 0.6085 - val_loss: 0.4165 - val_auc: 0.6269\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.41658 to 0.41654, saving model to DeepFM.h5\n",
      "Epoch 456/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4533 - auc: 0.634 - ETA: 0s - loss: 0.4180 - auc: 0.621 - 0s 21us/step - loss: 0.4174 - auc: 0.6137 - val_loss: 0.4165 - val_auc: 0.6269\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.41654 to 0.41651, saving model to DeepFM.h5\n",
      "Epoch 457/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3849 - auc: 0.606 - ETA: 0s - loss: 0.4143 - auc: 0.603 - 0s 20us/step - loss: 0.4177 - auc: 0.6165 - val_loss: 0.4165 - val_auc: 0.6264\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.41651 to 0.41648, saving model to DeepFM.h5\n",
      "Epoch 458/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4759 - auc: 0.642 - ETA: 0s - loss: 0.4221 - auc: 0.597 - 0s 20us/step - loss: 0.4195 - auc: 0.5943 - val_loss: 0.4165 - val_auc: 0.6262\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.41648 to 0.41646, saving model to DeepFM.h5\n",
      "Epoch 459/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4602 - auc: 0.674 - ETA: 0s - loss: 0.4207 - auc: 0.608 - 0s 19us/step - loss: 0.4170 - auc: 0.6154 - val_loss: 0.4164 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.41646 to 0.41642, saving model to DeepFM.h5\n",
      "Epoch 460/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4426 - auc: 0.585 - ETA: 0s - loss: 0.4186 - auc: 0.577 - 0s 20us/step - loss: 0.4208 - auc: 0.5805 - val_loss: 0.4164 - val_auc: 0.6254\n",
      "\n",
      "Epoch 00460: val_loss improved from 0.41642 to 0.41639, saving model to DeepFM.h5\n",
      "Epoch 461/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4153 - auc: 0.573 - ETA: 0s - loss: 0.4230 - auc: 0.591 - 0s 20us/step - loss: 0.4185 - auc: 0.6010 - val_loss: 0.4164 - val_auc: 0.6266\n",
      "\n",
      "Epoch 00461: val_loss improved from 0.41639 to 0.41636, saving model to DeepFM.h5\n",
      "Epoch 462/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4002 - auc: 0.668 - ETA: 0s - loss: 0.4051 - auc: 0.623 - 0s 19us/step - loss: 0.4175 - auc: 0.6130 - val_loss: 0.4163 - val_auc: 0.6270\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.41636 to 0.41632, saving model to DeepFM.h5\n",
      "Epoch 463/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4528 - auc: 0.491 - ETA: 0s - loss: 0.4136 - auc: 0.601 - 0s 20us/step - loss: 0.4199 - auc: 0.5936 - val_loss: 0.4163 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00463: val_loss improved from 0.41632 to 0.41629, saving model to DeepFM.h5\n",
      "Epoch 464/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4863 - auc: 0.570 - ETA: 0s - loss: 0.4163 - auc: 0.598 - 0s 20us/step - loss: 0.4188 - auc: 0.5971 - val_loss: 0.4163 - val_auc: 0.6271\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.41629 to 0.41626, saving model to DeepFM.h5\n",
      "Epoch 465/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3209 - auc: 0.586 - ETA: 0s - loss: 0.4135 - auc: 0.598 - 0s 21us/step - loss: 0.4175 - auc: 0.6057 - val_loss: 0.4162 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.41626 to 0.41623, saving model to DeepFM.h5\n",
      "Epoch 466/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4281 - auc: 0.519 - ETA: 0s - loss: 0.4179 - auc: 0.588 - 0s 22us/step - loss: 0.4201 - auc: 0.5830 - val_loss: 0.4162 - val_auc: 0.6273\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.41623 to 0.41619, saving model to DeepFM.h5\n",
      "Epoch 467/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3930 - auc: 0.592 - ETA: 0s - loss: 0.4181 - auc: 0.611 - 0s 21us/step - loss: 0.4173 - auc: 0.6097 - val_loss: 0.4162 - val_auc: 0.6270\n",
      "\n",
      "Epoch 00467: val_loss improved from 0.41619 to 0.41616, saving model to DeepFM.h5\n",
      "Epoch 468/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3563 - auc: 0.624 - ETA: 0s - loss: 0.4243 - auc: 0.597 - 0s 21us/step - loss: 0.4183 - auc: 0.6087 - val_loss: 0.4161 - val_auc: 0.6277\n",
      "\n",
      "Epoch 00468: val_loss improved from 0.41616 to 0.41613, saving model to DeepFM.h5\n",
      "Epoch 469/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5002 - auc: 0.651 - ETA: 0s - loss: 0.4241 - auc: 0.627 - 0s 20us/step - loss: 0.4154 - auc: 0.6249 - val_loss: 0.4161 - val_auc: 0.6261\n",
      "\n",
      "Epoch 00469: val_loss improved from 0.41613 to 0.41610, saving model to DeepFM.h5\n",
      "Epoch 470/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4557 - auc: 0.580 - ETA: 0s - loss: 0.4166 - auc: 0.607 - 0s 20us/step - loss: 0.4176 - auc: 0.6049 - val_loss: 0.4161 - val_auc: 0.6265\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.41610 to 0.41607, saving model to DeepFM.h5\n",
      "Epoch 471/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3947 - auc: 0.578 - ETA: 0s - loss: 0.4245 - auc: 0.592 - 0s 23us/step - loss: 0.4189 - auc: 0.6017 - val_loss: 0.4160 - val_auc: 0.6267\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.41607 to 0.41604, saving model to DeepFM.h5\n",
      "Epoch 472/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4079 - auc: 0.497 - ETA: 0s - loss: 0.4204 - auc: 0.604 - 0s 21us/step - loss: 0.4187 - auc: 0.5973 - val_loss: 0.4160 - val_auc: 0.6280\n",
      "\n",
      "Epoch 00472: val_loss improved from 0.41604 to 0.41601, saving model to DeepFM.h5\n",
      "Epoch 473/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4259 - auc: 0.624 - ETA: 0s - loss: 0.4207 - auc: 0.616 - 0s 21us/step - loss: 0.4167 - auc: 0.6195 - val_loss: 0.4160 - val_auc: 0.6271\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.41601 to 0.41597, saving model to DeepFM.h5\n",
      "Epoch 474/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4641 - auc: 0.634 - ETA: 0s - loss: 0.4172 - auc: 0.617 - 0s 19us/step - loss: 0.4178 - auc: 0.6100 - val_loss: 0.4159 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.41597 to 0.41594, saving model to DeepFM.h5\n",
      "Epoch 475/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4701 - auc: 0.575 - ETA: 0s - loss: 0.4141 - auc: 0.621 - 0s 19us/step - loss: 0.4170 - auc: 0.6187 - val_loss: 0.4159 - val_auc: 0.6255\n",
      "\n",
      "Epoch 00475: val_loss improved from 0.41594 to 0.41591, saving model to DeepFM.h5\n",
      "Epoch 476/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4043 - auc: 0.614 - ETA: 0s - loss: 0.4141 - auc: 0.605 - 0s 21us/step - loss: 0.4178 - auc: 0.6050 - val_loss: 0.4159 - val_auc: 0.6264\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.41591 to 0.41589, saving model to DeepFM.h5\n",
      "Epoch 477/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3633 - auc: 0.627 - ETA: 0s - loss: 0.4193 - auc: 0.596 - 0s 22us/step - loss: 0.4196 - auc: 0.5922 - val_loss: 0.4158 - val_auc: 0.6273\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.41589 to 0.41584, saving model to DeepFM.h5\n",
      "Epoch 478/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4147 - auc: 0.613 - ETA: 0s - loss: 0.4146 - auc: 0.621 - 0s 22us/step - loss: 0.4160 - auc: 0.6233 - val_loss: 0.4158 - val_auc: 0.6261\n",
      "\n",
      "Epoch 00478: val_loss improved from 0.41584 to 0.41581, saving model to DeepFM.h5\n",
      "Epoch 479/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4537 - auc: 0.678 - ETA: 0s - loss: 0.4176 - auc: 0.594 - 0s 20us/step - loss: 0.4199 - auc: 0.5867 - val_loss: 0.4158 - val_auc: 0.6276\n",
      "\n",
      "Epoch 00479: val_loss improved from 0.41581 to 0.41578, saving model to DeepFM.h5\n",
      "Epoch 480/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3823 - auc: 0.577 - ETA: 0s - loss: 0.4109 - auc: 0.597 - 0s 21us/step - loss: 0.4197 - auc: 0.5938 - val_loss: 0.4157 - val_auc: 0.6279\n",
      "\n",
      "Epoch 00480: val_loss improved from 0.41578 to 0.41575, saving model to DeepFM.h5\n",
      "Epoch 481/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4487 - auc: 0.589 - ETA: 0s - loss: 0.4198 - auc: 0.629 - 0s 21us/step - loss: 0.4166 - auc: 0.6205 - val_loss: 0.4157 - val_auc: 0.6280\n",
      "\n",
      "Epoch 00481: val_loss improved from 0.41575 to 0.41572, saving model to DeepFM.h5\n",
      "Epoch 482/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3970 - auc: 0.555 - ETA: 0s - loss: 0.4066 - auc: 0.610 - 0s 24us/step - loss: 0.4178 - auc: 0.6061 - val_loss: 0.4157 - val_auc: 0.6275\n",
      "\n",
      "Epoch 00482: val_loss improved from 0.41572 to 0.41570, saving model to DeepFM.h5\n",
      "Epoch 483/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4608 - auc: 0.653 - ETA: 0s - loss: 0.4158 - auc: 0.613 - 0s 22us/step - loss: 0.4178 - auc: 0.6107 - val_loss: 0.4157 - val_auc: 0.6283\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.41570 to 0.41566, saving model to DeepFM.h5\n",
      "Epoch 484/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4662 - auc: 0.611 - ETA: 0s - loss: 0.4175 - auc: 0.603 - 0s 20us/step - loss: 0.4175 - auc: 0.6121 - val_loss: 0.4156 - val_auc: 0.6295\n",
      "\n",
      "Epoch 00484: val_loss improved from 0.41566 to 0.41563, saving model to DeepFM.h5\n",
      "Epoch 485/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3755 - auc: 0.598 - ETA: 0s - loss: 0.4111 - auc: 0.589 - 0s 20us/step - loss: 0.4185 - auc: 0.6020 - val_loss: 0.4156 - val_auc: 0.6294\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.41563 to 0.41559, saving model to DeepFM.h5\n",
      "Epoch 486/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4781 - auc: 0.582 - ETA: 0s - loss: 0.4151 - auc: 0.603 - 0s 20us/step - loss: 0.4180 - auc: 0.6088 - val_loss: 0.4156 - val_auc: 0.6294\n",
      "\n",
      "Epoch 00486: val_loss improved from 0.41559 to 0.41556, saving model to DeepFM.h5\n",
      "Epoch 487/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4169 - auc: 0.560 - ETA: 0s - loss: 0.4225 - auc: 0.603 - 0s 22us/step - loss: 0.4169 - auc: 0.6124 - val_loss: 0.4155 - val_auc: 0.6303\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.41556 to 0.41552, saving model to DeepFM.h5\n",
      "Epoch 488/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3938 - auc: 0.669 - ETA: 0s - loss: 0.4113 - auc: 0.608 - 0s 20us/step - loss: 0.4175 - auc: 0.6100 - val_loss: 0.4155 - val_auc: 0.6285\n",
      "\n",
      "Epoch 00488: val_loss improved from 0.41552 to 0.41549, saving model to DeepFM.h5\n",
      "Epoch 489/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3677 - auc: 0.642 - ETA: 0s - loss: 0.4259 - auc: 0.588 - 0s 20us/step - loss: 0.4183 - auc: 0.6012 - val_loss: 0.4155 - val_auc: 0.6297\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.41549 to 0.41546, saving model to DeepFM.h5\n",
      "Epoch 490/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3693 - auc: 0.589 - ETA: 0s - loss: 0.4165 - auc: 0.604 - 0s 20us/step - loss: 0.4173 - auc: 0.6063 - val_loss: 0.4154 - val_auc: 0.6294\n",
      "\n",
      "Epoch 00490: val_loss improved from 0.41546 to 0.41542, saving model to DeepFM.h5\n",
      "Epoch 491/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4226 - auc: 0.639 - ETA: 0s - loss: 0.4165 - auc: 0.617 - 0s 21us/step - loss: 0.4178 - auc: 0.6068 - val_loss: 0.4154 - val_auc: 0.6292\n",
      "\n",
      "Epoch 00491: val_loss improved from 0.41542 to 0.41539, saving model to DeepFM.h5\n",
      "Epoch 492/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4264 - auc: 0.525 - ETA: 0s - loss: 0.4204 - auc: 0.598 - 0s 21us/step - loss: 0.4179 - auc: 0.6038 - val_loss: 0.4154 - val_auc: 0.6305\n",
      "\n",
      "Epoch 00492: val_loss improved from 0.41539 to 0.41536, saving model to DeepFM.h5\n",
      "Epoch 493/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4380 - auc: 0.641 - ETA: 0s - loss: 0.4222 - auc: 0.606 - 0s 19us/step - loss: 0.4186 - auc: 0.6026 - val_loss: 0.4153 - val_auc: 0.6271\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.41536 to 0.41532, saving model to DeepFM.h5\n",
      "Epoch 494/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3885 - auc: 0.660 - ETA: 0s - loss: 0.4089 - auc: 0.619 - 0s 20us/step - loss: 0.4169 - auc: 0.6142 - val_loss: 0.4153 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.41532 to 0.41529, saving model to DeepFM.h5\n",
      "Epoch 495/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3998 - auc: 0.590 - ETA: 0s - loss: 0.4180 - auc: 0.623 - 0s 20us/step - loss: 0.4161 - auc: 0.6236 - val_loss: 0.4153 - val_auc: 0.6280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00495: val_loss improved from 0.41529 to 0.41526, saving model to DeepFM.h5\n",
      "Epoch 496/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5005 - auc: 0.556 - ETA: 0s - loss: 0.4192 - auc: 0.612 - 0s 20us/step - loss: 0.4172 - auc: 0.6139 - val_loss: 0.4152 - val_auc: 0.6300\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.41526 to 0.41523, saving model to DeepFM.h5\n",
      "Epoch 497/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3615 - auc: 0.638 - ETA: 0s - loss: 0.4101 - auc: 0.629 - 0s 22us/step - loss: 0.4152 - auc: 0.6273 - val_loss: 0.4152 - val_auc: 0.6295\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.41523 to 0.41519, saving model to DeepFM.h5\n",
      "Epoch 498/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4198 - auc: 0.685 - ETA: 0s - loss: 0.4089 - auc: 0.628 - 0s 23us/step - loss: 0.4152 - auc: 0.6322 - val_loss: 0.4152 - val_auc: 0.6312\n",
      "\n",
      "Epoch 00498: val_loss improved from 0.41519 to 0.41515, saving model to DeepFM.h5\n",
      "Epoch 499/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4054 - auc: 0.610 - ETA: 0s - loss: 0.4166 - auc: 0.625 - 0s 23us/step - loss: 0.4161 - auc: 0.6264 - val_loss: 0.4151 - val_auc: 0.6307\n",
      "\n",
      "Epoch 00499: val_loss improved from 0.41515 to 0.41512, saving model to DeepFM.h5\n",
      "Epoch 500/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4246 - auc: 0.605 - ETA: 0s - loss: 0.4228 - auc: 0.600 - 0s 28us/step - loss: 0.4154 - auc: 0.6251 - val_loss: 0.4151 - val_auc: 0.6320\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.41512 to 0.41508, saving model to DeepFM.h5\n",
      "Epoch 501/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3814 - auc: 0.685 - ETA: 0s - loss: 0.4110 - auc: 0.618 - 0s 20us/step - loss: 0.4168 - auc: 0.6119 - val_loss: 0.4151 - val_auc: 0.6311\n",
      "\n",
      "Epoch 00501: val_loss improved from 0.41508 to 0.41505, saving model to DeepFM.h5\n",
      "Epoch 502/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4674 - auc: 0.551 - ETA: 0s - loss: 0.4155 - auc: 0.617 - 0s 21us/step - loss: 0.4164 - auc: 0.6179 - val_loss: 0.4150 - val_auc: 0.6318\n",
      "\n",
      "Epoch 00502: val_loss improved from 0.41505 to 0.41502, saving model to DeepFM.h5\n",
      "Epoch 503/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4384 - auc: 0.604 - ETA: 0s - loss: 0.4267 - auc: 0.580 - 0s 21us/step - loss: 0.4190 - auc: 0.6003 - val_loss: 0.4150 - val_auc: 0.6307\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.41502 to 0.41498, saving model to DeepFM.h5\n",
      "Epoch 504/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4719 - auc: 0.654 - ETA: 0s - loss: 0.4003 - auc: 0.631 - 0s 21us/step - loss: 0.4156 - auc: 0.6234 - val_loss: 0.4150 - val_auc: 0.6300\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.41498 to 0.41495, saving model to DeepFM.h5\n",
      "Epoch 505/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3698 - auc: 0.607 - ETA: 0s - loss: 0.4219 - auc: 0.615 - 0s 21us/step - loss: 0.4173 - auc: 0.6065 - val_loss: 0.4149 - val_auc: 0.6290\n",
      "\n",
      "Epoch 00505: val_loss improved from 0.41495 to 0.41491, saving model to DeepFM.h5\n",
      "Epoch 506/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4299 - auc: 0.618 - ETA: 0s - loss: 0.4217 - auc: 0.599 - 0s 20us/step - loss: 0.4174 - auc: 0.6028 - val_loss: 0.4149 - val_auc: 0.6291\n",
      "\n",
      "Epoch 00506: val_loss improved from 0.41491 to 0.41489, saving model to DeepFM.h5\n",
      "Epoch 507/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3525 - auc: 0.609 - ETA: 0s - loss: 0.4164 - auc: 0.619 - 0s 20us/step - loss: 0.4166 - auc: 0.6199 - val_loss: 0.4149 - val_auc: 0.6298\n",
      "\n",
      "Epoch 00507: val_loss improved from 0.41489 to 0.41486, saving model to DeepFM.h5\n",
      "Epoch 508/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4477 - auc: 0.642 - ETA: 0s - loss: 0.4069 - auc: 0.614 - 0s 22us/step - loss: 0.4168 - auc: 0.6099 - val_loss: 0.4148 - val_auc: 0.6293\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.41486 to 0.41482, saving model to DeepFM.h5\n",
      "Epoch 509/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4630 - auc: 0.558 - ETA: 0s - loss: 0.4228 - auc: 0.615 - 0s 21us/step - loss: 0.4164 - auc: 0.6161 - val_loss: 0.4148 - val_auc: 0.6288\n",
      "\n",
      "Epoch 00509: val_loss improved from 0.41482 to 0.41480, saving model to DeepFM.h5\n",
      "Epoch 510/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3697 - auc: 0.546 - ETA: 0s - loss: 0.4112 - auc: 0.620 - 0s 20us/step - loss: 0.4161 - auc: 0.6172 - val_loss: 0.4148 - val_auc: 0.6281\n",
      "\n",
      "Epoch 00510: val_loss improved from 0.41480 to 0.41476, saving model to DeepFM.h5\n",
      "Epoch 511/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4099 - auc: 0.622 - ETA: 0s - loss: 0.4168 - auc: 0.610 - 0s 21us/step - loss: 0.4160 - auc: 0.6174 - val_loss: 0.4147 - val_auc: 0.6300\n",
      "\n",
      "Epoch 00511: val_loss improved from 0.41476 to 0.41473, saving model to DeepFM.h5\n",
      "Epoch 512/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4161 - auc: 0.563 - ETA: 0s - loss: 0.4213 - auc: 0.627 - 0s 20us/step - loss: 0.4160 - auc: 0.6222 - val_loss: 0.4147 - val_auc: 0.6279\n",
      "\n",
      "Epoch 00512: val_loss improved from 0.41473 to 0.41469, saving model to DeepFM.h5\n",
      "Epoch 513/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4461 - auc: 0.546 - ETA: 0s - loss: 0.4229 - auc: 0.601 - 0s 23us/step - loss: 0.4160 - auc: 0.6167 - val_loss: 0.4147 - val_auc: 0.6281\n",
      "\n",
      "Epoch 00513: val_loss improved from 0.41469 to 0.41466, saving model to DeepFM.h5\n",
      "Epoch 514/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3995 - auc: 0.611 - ETA: 0s - loss: 0.4121 - auc: 0.632 - 0s 22us/step - loss: 0.4149 - auc: 0.6255 - val_loss: 0.4146 - val_auc: 0.6280\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.41466 to 0.41463, saving model to DeepFM.h5\n",
      "Epoch 515/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5355 - auc: 0.587 - ETA: 0s - loss: 0.4274 - auc: 0.638 - 0s 23us/step - loss: 0.4146 - auc: 0.6297 - val_loss: 0.4146 - val_auc: 0.6282\n",
      "\n",
      "Epoch 00515: val_loss improved from 0.41463 to 0.41461, saving model to DeepFM.h5\n",
      "Epoch 516/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3867 - auc: 0.598 - ETA: 0s - loss: 0.4173 - auc: 0.626 - 0s 21us/step - loss: 0.4167 - auc: 0.6144 - val_loss: 0.4146 - val_auc: 0.6270\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.41461 to 0.41458, saving model to DeepFM.h5\n",
      "Epoch 517/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4001 - auc: 0.612 - ETA: 0s - loss: 0.4194 - auc: 0.617 - 0s 20us/step - loss: 0.4161 - auc: 0.6239 - val_loss: 0.4145 - val_auc: 0.6278\n",
      "\n",
      "Epoch 00517: val_loss improved from 0.41458 to 0.41454, saving model to DeepFM.h5\n",
      "Epoch 518/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4127 - auc: 0.599 - ETA: 0s - loss: 0.4147 - auc: 0.614 - 0s 20us/step - loss: 0.4166 - auc: 0.6154 - val_loss: 0.4145 - val_auc: 0.6292\n",
      "\n",
      "Epoch 00518: val_loss improved from 0.41454 to 0.41452, saving model to DeepFM.h5\n",
      "Epoch 519/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4297 - auc: 0.650 - ETA: 0s - loss: 0.4052 - auc: 0.607 - 0s 22us/step - loss: 0.4173 - auc: 0.6104 - val_loss: 0.4145 - val_auc: 0.6278\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.41452 to 0.41447, saving model to DeepFM.h5\n",
      "Epoch 520/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4213 - auc: 0.536 - ETA: 0s - loss: 0.4203 - auc: 0.613 - 0s 20us/step - loss: 0.4170 - auc: 0.6079 - val_loss: 0.4144 - val_auc: 0.6264\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.41447 to 0.41444, saving model to DeepFM.h5\n",
      "Epoch 521/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3752 - auc: 0.625 - ETA: 0s - loss: 0.4019 - auc: 0.599 - 0s 21us/step - loss: 0.4179 - auc: 0.6081 - val_loss: 0.4144 - val_auc: 0.6280\n",
      "\n",
      "Epoch 00521: val_loss improved from 0.41444 to 0.41439, saving model to DeepFM.h5\n",
      "Epoch 522/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4218 - auc: 0.646 - ETA: 0s - loss: 0.4195 - auc: 0.608 - 0s 21us/step - loss: 0.4170 - auc: 0.6196 - val_loss: 0.4144 - val_auc: 0.6265\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.41439 to 0.41437, saving model to DeepFM.h5\n",
      "Epoch 523/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4072 - auc: 0.578 - ETA: 0s - loss: 0.4172 - auc: 0.598 - 0s 21us/step - loss: 0.4171 - auc: 0.6076 - val_loss: 0.4143 - val_auc: 0.6271\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.41437 to 0.41434, saving model to DeepFM.h5\n",
      "Epoch 524/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4095 - auc: 0.685 - ETA: 0s - loss: 0.4109 - auc: 0.618 - 0s 21us/step - loss: 0.4144 - auc: 0.6274 - val_loss: 0.4143 - val_auc: 0.6285\n",
      "\n",
      "Epoch 00524: val_loss improved from 0.41434 to 0.41431, saving model to DeepFM.h5\n",
      "Epoch 525/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3840 - auc: 0.613 - ETA: 0s - loss: 0.4085 - auc: 0.599 - 0s 20us/step - loss: 0.4172 - auc: 0.6056 - val_loss: 0.4143 - val_auc: 0.6277\n",
      "\n",
      "Epoch 00525: val_loss improved from 0.41431 to 0.41428, saving model to DeepFM.h5\n",
      "Epoch 526/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3734 - auc: 0.514 - ETA: 0s - loss: 0.4150 - auc: 0.611 - 0s 21us/step - loss: 0.4157 - auc: 0.6160 - val_loss: 0.4142 - val_auc: 0.6278\n",
      "\n",
      "Epoch 00526: val_loss improved from 0.41428 to 0.41424, saving model to DeepFM.h5\n",
      "Epoch 527/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4538 - auc: 0.593 - ETA: 0s - loss: 0.4143 - auc: 0.606 - 0s 20us/step - loss: 0.4165 - auc: 0.6157 - val_loss: 0.4142 - val_auc: 0.6274\n",
      "\n",
      "Epoch 00527: val_loss improved from 0.41424 to 0.41421, saving model to DeepFM.h5\n",
      "Epoch 528/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4331 - auc: 0.584 - ETA: 0s - loss: 0.4221 - auc: 0.614 - 0s 22us/step - loss: 0.4152 - auc: 0.6271 - val_loss: 0.4142 - val_auc: 0.6281\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.41421 to 0.41418, saving model to DeepFM.h5\n",
      "Epoch 529/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4565 - auc: 0.464 - ETA: 0s - loss: 0.4200 - auc: 0.602 - 0s 20us/step - loss: 0.4168 - auc: 0.6115 - val_loss: 0.4141 - val_auc: 0.6279\n",
      "\n",
      "Epoch 00529: val_loss improved from 0.41418 to 0.41414, saving model to DeepFM.h5\n",
      "Epoch 530/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4783 - auc: 0.631 - ETA: 0s - loss: 0.4179 - auc: 0.620 - 0s 19us/step - loss: 0.4153 - auc: 0.6235 - val_loss: 0.4141 - val_auc: 0.6272\n",
      "\n",
      "Epoch 00530: val_loss improved from 0.41414 to 0.41411, saving model to DeepFM.h5\n",
      "Epoch 531/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4208 - auc: 0.633 - ETA: 0s - loss: 0.4189 - auc: 0.615 - 0s 20us/step - loss: 0.4163 - auc: 0.6138 - val_loss: 0.4141 - val_auc: 0.6289\n",
      "\n",
      "Epoch 00531: val_loss improved from 0.41411 to 0.41409, saving model to DeepFM.h5\n",
      "Epoch 532/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3972 - auc: 0.604 - ETA: 0s - loss: 0.4093 - auc: 0.608 - 0s 21us/step - loss: 0.4168 - auc: 0.6089 - val_loss: 0.4141 - val_auc: 0.6286\n",
      "\n",
      "Epoch 00532: val_loss improved from 0.41409 to 0.41406, saving model to DeepFM.h5\n",
      "Epoch 533/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4115 - auc: 0.550 - ETA: 0s - loss: 0.4161 - auc: 0.642 - 0s 21us/step - loss: 0.4153 - auc: 0.6239 - val_loss: 0.4140 - val_auc: 0.6282\n",
      "\n",
      "Epoch 00533: val_loss improved from 0.41406 to 0.41403, saving model to DeepFM.h5\n",
      "Epoch 534/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4007 - auc: 0.650 - ETA: 0s - loss: 0.4148 - auc: 0.627 - 0s 21us/step - loss: 0.4151 - auc: 0.6194 - val_loss: 0.4140 - val_auc: 0.6268\n",
      "\n",
      "Epoch 00534: val_loss improved from 0.41403 to 0.41400, saving model to DeepFM.h5\n",
      "Epoch 535/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4111 - auc: 0.700 - ETA: 0s - loss: 0.4141 - auc: 0.626 - 0s 19us/step - loss: 0.4160 - auc: 0.6172 - val_loss: 0.4140 - val_auc: 0.6273\n",
      "\n",
      "Epoch 00535: val_loss improved from 0.41400 to 0.41397, saving model to DeepFM.h5\n",
      "Epoch 536/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3601 - auc: 0.635 - ETA: 0s - loss: 0.4040 - auc: 0.647 - 0s 22us/step - loss: 0.4147 - auc: 0.6274 - val_loss: 0.4139 - val_auc: 0.6287\n",
      "\n",
      "Epoch 00536: val_loss improved from 0.41397 to 0.41393, saving model to DeepFM.h5\n",
      "Epoch 537/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4408 - auc: 0.601 - ETA: 0s - loss: 0.4131 - auc: 0.628 - 0s 20us/step - loss: 0.4153 - auc: 0.6230 - val_loss: 0.4139 - val_auc: 0.6283\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.41393 to 0.41391, saving model to DeepFM.h5\n",
      "Epoch 538/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4094 - auc: 0.492 - ETA: 0s - loss: 0.4232 - auc: 0.621 - 0s 21us/step - loss: 0.4155 - auc: 0.6217 - val_loss: 0.4139 - val_auc: 0.6281\n",
      "\n",
      "Epoch 00538: val_loss improved from 0.41391 to 0.41387, saving model to DeepFM.h5\n",
      "Epoch 539/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4155 - auc: 0.609 - ETA: 0s - loss: 0.4182 - auc: 0.618 - 0s 21us/step - loss: 0.4159 - auc: 0.6160 - val_loss: 0.4138 - val_auc: 0.6282\n",
      "\n",
      "Epoch 00539: val_loss improved from 0.41387 to 0.41384, saving model to DeepFM.h5\n",
      "Epoch 540/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4695 - auc: 0.589 - ETA: 0s - loss: 0.4157 - auc: 0.618 - 0s 20us/step - loss: 0.4152 - auc: 0.6204 - val_loss: 0.4138 - val_auc: 0.6291\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.41384 to 0.41381, saving model to DeepFM.h5\n",
      "Epoch 541/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4315 - auc: 0.654 - ETA: 0s - loss: 0.4085 - auc: 0.641 - 0s 21us/step - loss: 0.4146 - auc: 0.6264 - val_loss: 0.4138 - val_auc: 0.6289\n",
      "\n",
      "Epoch 00541: val_loss improved from 0.41381 to 0.41377, saving model to DeepFM.h5\n",
      "Epoch 542/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4310 - auc: 0.619 - ETA: 0s - loss: 0.4179 - auc: 0.621 - 0s 21us/step - loss: 0.4155 - auc: 0.6220 - val_loss: 0.4137 - val_auc: 0.6296\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.41377 to 0.41374, saving model to DeepFM.h5\n",
      "Epoch 543/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3769 - auc: 0.611 - ETA: 0s - loss: 0.4161 - auc: 0.618 - 0s 22us/step - loss: 0.4161 - auc: 0.6140 - val_loss: 0.4137 - val_auc: 0.6309\n",
      "\n",
      "Epoch 00543: val_loss improved from 0.41374 to 0.41370, saving model to DeepFM.h5\n",
      "Epoch 544/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4296 - auc: 0.574 - ETA: 0s - loss: 0.4112 - auc: 0.626 - 0s 22us/step - loss: 0.4147 - auc: 0.6251 - val_loss: 0.4137 - val_auc: 0.6299\n",
      "\n",
      "Epoch 00544: val_loss improved from 0.41370 to 0.41368, saving model to DeepFM.h5\n",
      "Epoch 545/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4813 - auc: 0.671 - ETA: 0s - loss: 0.4271 - auc: 0.618 - 0s 22us/step - loss: 0.4162 - auc: 0.6176 - val_loss: 0.4136 - val_auc: 0.6315\n",
      "\n",
      "Epoch 00545: val_loss improved from 0.41368 to 0.41364, saving model to DeepFM.h5\n",
      "Epoch 546/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4301 - auc: 0.603 - ETA: 0s - loss: 0.4193 - auc: 0.619 - 0s 21us/step - loss: 0.4148 - auc: 0.6265 - val_loss: 0.4136 - val_auc: 0.6311\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.41364 to 0.41361, saving model to DeepFM.h5\n",
      "Epoch 547/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3960 - auc: 0.624 - ETA: 0s - loss: 0.4139 - auc: 0.629 - 0s 23us/step - loss: 0.4152 - auc: 0.6204 - val_loss: 0.4136 - val_auc: 0.6298\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.41361 to 0.41357, saving model to DeepFM.h5\n",
      "Epoch 548/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3798 - auc: 0.651 - ETA: 0s - loss: 0.4056 - auc: 0.619 - 0s 26us/step - loss: 0.4157 - auc: 0.6113 - val_loss: 0.4135 - val_auc: 0.6286\n",
      "\n",
      "Epoch 00548: val_loss improved from 0.41357 to 0.41353, saving model to DeepFM.h5\n",
      "Epoch 549/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4449 - auc: 0.680 - ETA: 0s - loss: 0.4205 - auc: 0.624 - 0s 24us/step - loss: 0.4146 - auc: 0.6255 - val_loss: 0.4135 - val_auc: 0.6287\n",
      "\n",
      "Epoch 00549: val_loss improved from 0.41353 to 0.41350, saving model to DeepFM.h5\n",
      "Epoch 550/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4233 - auc: 0.714 - ETA: 0s - loss: 0.4253 - auc: 0.635 - 0s 20us/step - loss: 0.4141 - auc: 0.6290 - val_loss: 0.4135 - val_auc: 0.6305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00550: val_loss improved from 0.41350 to 0.41347, saving model to DeepFM.h5\n",
      "Epoch 551/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3691 - auc: 0.670 - ETA: 0s - loss: 0.4146 - auc: 0.612 - 0s 23us/step - loss: 0.4154 - auc: 0.6230 - val_loss: 0.4134 - val_auc: 0.6301\n",
      "\n",
      "Epoch 00551: val_loss improved from 0.41347 to 0.41344, saving model to DeepFM.h5\n",
      "Epoch 552/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3996 - auc: 0.637 - ETA: 0s - loss: 0.4179 - auc: 0.618 - 0s 22us/step - loss: 0.4154 - auc: 0.6259 - val_loss: 0.4134 - val_auc: 0.6294\n",
      "\n",
      "Epoch 00552: val_loss improved from 0.41344 to 0.41341, saving model to DeepFM.h5\n",
      "Epoch 553/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3525 - auc: 0.626 - ETA: 0s - loss: 0.4152 - auc: 0.624 - 0s 19us/step - loss: 0.4144 - auc: 0.6317 - val_loss: 0.4134 - val_auc: 0.6286\n",
      "\n",
      "Epoch 00553: val_loss improved from 0.41341 to 0.41336, saving model to DeepFM.h5\n",
      "Epoch 554/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3681 - auc: 0.639 - ETA: 0s - loss: 0.4131 - auc: 0.628 - 0s 19us/step - loss: 0.4143 - auc: 0.6268 - val_loss: 0.4133 - val_auc: 0.6285\n",
      "\n",
      "Epoch 00554: val_loss improved from 0.41336 to 0.41332, saving model to DeepFM.h5\n",
      "Epoch 555/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3810 - auc: 0.645 - ETA: 0s - loss: 0.3978 - auc: 0.642 - 0s 23us/step - loss: 0.4133 - auc: 0.6350 - val_loss: 0.4133 - val_auc: 0.6292\n",
      "\n",
      "Epoch 00555: val_loss improved from 0.41332 to 0.41330, saving model to DeepFM.h5\n",
      "Epoch 556/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3832 - auc: 0.624 - ETA: 0s - loss: 0.4220 - auc: 0.622 - 0s 22us/step - loss: 0.4155 - auc: 0.6205 - val_loss: 0.4133 - val_auc: 0.6306\n",
      "\n",
      "Epoch 00556: val_loss improved from 0.41330 to 0.41327, saving model to DeepFM.h5\n",
      "Epoch 557/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4592 - auc: 0.668 - ETA: 0s - loss: 0.4114 - auc: 0.626 - 0s 21us/step - loss: 0.4146 - auc: 0.6252 - val_loss: 0.4132 - val_auc: 0.6319\n",
      "\n",
      "Epoch 00557: val_loss improved from 0.41327 to 0.41324, saving model to DeepFM.h5\n",
      "Epoch 558/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4488 - auc: 0.661 - ETA: 0s - loss: 0.4224 - auc: 0.634 - 0s 20us/step - loss: 0.4132 - auc: 0.6372 - val_loss: 0.4132 - val_auc: 0.6303\n",
      "\n",
      "Epoch 00558: val_loss improved from 0.41324 to 0.41322, saving model to DeepFM.h5\n",
      "Epoch 559/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4384 - auc: 0.574 - ETA: 0s - loss: 0.4251 - auc: 0.616 - 0s 20us/step - loss: 0.4153 - auc: 0.6202 - val_loss: 0.4132 - val_auc: 0.6320\n",
      "\n",
      "Epoch 00559: val_loss improved from 0.41322 to 0.41317, saving model to DeepFM.h5\n",
      "Epoch 560/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4126 - auc: 0.655 - ETA: 0s - loss: 0.4088 - auc: 0.632 - 0s 21us/step - loss: 0.4143 - auc: 0.6269 - val_loss: 0.4131 - val_auc: 0.6319\n",
      "\n",
      "Epoch 00560: val_loss improved from 0.41317 to 0.41314, saving model to DeepFM.h5\n",
      "Epoch 561/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3749 - auc: 0.578 - ETA: 0s - loss: 0.4073 - auc: 0.622 - 0s 22us/step - loss: 0.4141 - auc: 0.6302 - val_loss: 0.4131 - val_auc: 0.6316\n",
      "\n",
      "Epoch 00561: val_loss improved from 0.41314 to 0.41311, saving model to DeepFM.h5\n",
      "Epoch 562/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4289 - auc: 0.636 - ETA: 0s - loss: 0.4038 - auc: 0.603 - 0s 21us/step - loss: 0.4149 - auc: 0.6190 - val_loss: 0.4131 - val_auc: 0.6309\n",
      "\n",
      "Epoch 00562: val_loss improved from 0.41311 to 0.41309, saving model to DeepFM.h5\n",
      "Epoch 563/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3764 - auc: 0.565 - ETA: 0s - loss: 0.4123 - auc: 0.625 - 0s 20us/step - loss: 0.4140 - auc: 0.6276 - val_loss: 0.4131 - val_auc: 0.6309\n",
      "\n",
      "Epoch 00563: val_loss improved from 0.41309 to 0.41305, saving model to DeepFM.h5\n",
      "Epoch 564/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3490 - auc: 0.691 - ETA: 0s - loss: 0.4065 - auc: 0.630 - 0s 19us/step - loss: 0.4132 - auc: 0.6352 - val_loss: 0.4130 - val_auc: 0.6306\n",
      "\n",
      "Epoch 00564: val_loss improved from 0.41305 to 0.41302, saving model to DeepFM.h5\n",
      "Epoch 565/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4438 - auc: 0.619 - ETA: 0s - loss: 0.4081 - auc: 0.623 - 0s 21us/step - loss: 0.4144 - auc: 0.6266 - val_loss: 0.4130 - val_auc: 0.6307\n",
      "\n",
      "Epoch 00565: val_loss improved from 0.41302 to 0.41298, saving model to DeepFM.h5\n",
      "Epoch 566/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3432 - auc: 0.660 - ETA: 0s - loss: 0.4151 - auc: 0.630 - 0s 22us/step - loss: 0.4149 - auc: 0.6211 - val_loss: 0.4129 - val_auc: 0.6315\n",
      "\n",
      "Epoch 00566: val_loss improved from 0.41298 to 0.41294, saving model to DeepFM.h5\n",
      "Epoch 567/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3860 - auc: 0.605 - ETA: 0s - loss: 0.4118 - auc: 0.613 - 0s 21us/step - loss: 0.4160 - auc: 0.6202 - val_loss: 0.4129 - val_auc: 0.6309\n",
      "\n",
      "Epoch 00567: val_loss improved from 0.41294 to 0.41291, saving model to DeepFM.h5\n",
      "Epoch 568/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3556 - auc: 0.638 - ETA: 0s - loss: 0.4134 - auc: 0.612 - 0s 20us/step - loss: 0.4151 - auc: 0.6179 - val_loss: 0.4129 - val_auc: 0.6312\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.41291 to 0.41288, saving model to DeepFM.h5\n",
      "Epoch 569/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4080 - auc: 0.592 - ETA: 0s - loss: 0.4141 - auc: 0.624 - 0s 21us/step - loss: 0.4145 - auc: 0.6242 - val_loss: 0.4128 - val_auc: 0.6320\n",
      "\n",
      "Epoch 00569: val_loss improved from 0.41288 to 0.41285, saving model to DeepFM.h5\n",
      "Epoch 570/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3549 - auc: 0.629 - ETA: 0s - loss: 0.4093 - auc: 0.613 - 0s 19us/step - loss: 0.4157 - auc: 0.6163 - val_loss: 0.4128 - val_auc: 0.6325\n",
      "\n",
      "Epoch 00570: val_loss improved from 0.41285 to 0.41280, saving model to DeepFM.h5\n",
      "Epoch 571/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4108 - auc: 0.717 - ETA: 0s - loss: 0.4223 - auc: 0.615 - 0s 21us/step - loss: 0.4153 - auc: 0.6162 - val_loss: 0.4128 - val_auc: 0.6317\n",
      "\n",
      "Epoch 00571: val_loss improved from 0.41280 to 0.41277, saving model to DeepFM.h5\n",
      "Epoch 572/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4455 - auc: 0.628 - ETA: 0s - loss: 0.4256 - auc: 0.619 - 0s 22us/step - loss: 0.4158 - auc: 0.6159 - val_loss: 0.4127 - val_auc: 0.6317\n",
      "\n",
      "Epoch 00572: val_loss improved from 0.41277 to 0.41274, saving model to DeepFM.h5\n",
      "Epoch 573/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3894 - auc: 0.677 - ETA: 0s - loss: 0.4095 - auc: 0.604 - 0s 22us/step - loss: 0.4157 - auc: 0.6150 - val_loss: 0.4127 - val_auc: 0.6312\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.41274 to 0.41271, saving model to DeepFM.h5\n",
      "Epoch 574/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3676 - auc: 0.749 - ETA: 0s - loss: 0.4062 - auc: 0.613 - 0s 22us/step - loss: 0.4149 - auc: 0.6211 - val_loss: 0.4127 - val_auc: 0.6322\n",
      "\n",
      "Epoch 00574: val_loss improved from 0.41271 to 0.41268, saving model to DeepFM.h5\n",
      "Epoch 575/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4055 - auc: 0.663 - ETA: 0s - loss: 0.4145 - auc: 0.637 - 0s 19us/step - loss: 0.4137 - auc: 0.6284 - val_loss: 0.4127 - val_auc: 0.6317\n",
      "\n",
      "Epoch 00575: val_loss improved from 0.41268 to 0.41265, saving model to DeepFM.h5\n",
      "Epoch 576/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4470 - auc: 0.646 - ETA: 0s - loss: 0.4150 - auc: 0.622 - 0s 22us/step - loss: 0.4140 - auc: 0.6250 - val_loss: 0.4126 - val_auc: 0.6320\n",
      "\n",
      "Epoch 00576: val_loss improved from 0.41265 to 0.41262, saving model to DeepFM.h5\n",
      "Epoch 577/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3870 - auc: 0.638 - ETA: 0s - loss: 0.4091 - auc: 0.637 - 0s 20us/step - loss: 0.4133 - auc: 0.6295 - val_loss: 0.4126 - val_auc: 0.6316\n",
      "\n",
      "Epoch 00577: val_loss improved from 0.41262 to 0.41259, saving model to DeepFM.h5\n",
      "Epoch 578/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3847 - auc: 0.654 - ETA: 0s - loss: 0.4086 - auc: 0.609 - 0s 21us/step - loss: 0.4151 - auc: 0.6213 - val_loss: 0.4126 - val_auc: 0.6310\n",
      "\n",
      "Epoch 00578: val_loss improved from 0.41259 to 0.41257, saving model to DeepFM.h5\n",
      "Epoch 579/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4154 - auc: 0.663 - ETA: 0s - loss: 0.4101 - auc: 0.628 - 0s 19us/step - loss: 0.4145 - auc: 0.6249 - val_loss: 0.4125 - val_auc: 0.6311\n",
      "\n",
      "Epoch 00579: val_loss improved from 0.41257 to 0.41254, saving model to DeepFM.h5\n",
      "Epoch 580/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3585 - auc: 0.743 - ETA: 0s - loss: 0.4205 - auc: 0.639 - 0s 21us/step - loss: 0.4135 - auc: 0.6330 - val_loss: 0.4125 - val_auc: 0.6317\n",
      "\n",
      "Epoch 00580: val_loss improved from 0.41254 to 0.41251, saving model to DeepFM.h5\n",
      "Epoch 581/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4235 - auc: 0.600 - ETA: 0s - loss: 0.4083 - auc: 0.622 - 0s 20us/step - loss: 0.4166 - auc: 0.6106 - val_loss: 0.4125 - val_auc: 0.6321\n",
      "\n",
      "Epoch 00581: val_loss improved from 0.41251 to 0.41248, saving model to DeepFM.h5\n",
      "Epoch 582/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3914 - auc: 0.640 - ETA: 0s - loss: 0.3935 - auc: 0.624 - 0s 26us/step - loss: 0.4125 - auc: 0.6337 - val_loss: 0.4124 - val_auc: 0.6319\n",
      "\n",
      "Epoch 00582: val_loss improved from 0.41248 to 0.41244, saving model to DeepFM.h5\n",
      "Epoch 583/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4308 - auc: 0.654 - ETA: 0s - loss: 0.4104 - auc: 0.624 - 0s 20us/step - loss: 0.4142 - auc: 0.6271 - val_loss: 0.4124 - val_auc: 0.6319\n",
      "\n",
      "Epoch 00583: val_loss improved from 0.41244 to 0.41241, saving model to DeepFM.h5\n",
      "Epoch 584/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4414 - auc: 0.596 - ETA: 0s - loss: 0.4239 - auc: 0.612 - 0s 20us/step - loss: 0.4166 - auc: 0.6075 - val_loss: 0.4124 - val_auc: 0.6328\n",
      "\n",
      "Epoch 00584: val_loss improved from 0.41241 to 0.41239, saving model to DeepFM.h5\n",
      "Epoch 585/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3378 - auc: 0.687 - ETA: 0s - loss: 0.4065 - auc: 0.619 - 0s 23us/step - loss: 0.4134 - auc: 0.6269 - val_loss: 0.4123 - val_auc: 0.6321\n",
      "\n",
      "Epoch 00585: val_loss improved from 0.41239 to 0.41235, saving model to DeepFM.h5\n",
      "Epoch 586/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4728 - auc: 0.695 - ETA: 0s - loss: 0.4088 - auc: 0.641 - 0s 21us/step - loss: 0.4128 - auc: 0.6357 - val_loss: 0.4123 - val_auc: 0.6314\n",
      "\n",
      "Epoch 00586: val_loss improved from 0.41235 to 0.41231, saving model to DeepFM.h5\n",
      "Epoch 587/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3610 - auc: 0.572 - ETA: 0s - loss: 0.4140 - auc: 0.631 - 0s 21us/step - loss: 0.4142 - auc: 0.6232 - val_loss: 0.4123 - val_auc: 0.6313\n",
      "\n",
      "Epoch 00587: val_loss improved from 0.41231 to 0.41228, saving model to DeepFM.h5\n",
      "Epoch 588/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4204 - auc: 0.703 - ETA: 0s - loss: 0.4145 - auc: 0.632 - 0s 21us/step - loss: 0.4125 - auc: 0.6381 - val_loss: 0.4122 - val_auc: 0.6318\n",
      "\n",
      "Epoch 00588: val_loss improved from 0.41228 to 0.41224, saving model to DeepFM.h5\n",
      "Epoch 589/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3972 - auc: 0.646 - ETA: 0s - loss: 0.4194 - auc: 0.613 - 0s 20us/step - loss: 0.4156 - auc: 0.6188 - val_loss: 0.4122 - val_auc: 0.6328\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.41224 to 0.41222, saving model to DeepFM.h5\n",
      "Epoch 590/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4049 - auc: 0.575 - ETA: 0s - loss: 0.4139 - auc: 0.630 - 0s 20us/step - loss: 0.4127 - auc: 0.6353 - val_loss: 0.4122 - val_auc: 0.6320\n",
      "\n",
      "Epoch 00590: val_loss improved from 0.41222 to 0.41218, saving model to DeepFM.h5\n",
      "Epoch 591/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3750 - auc: 0.687 - ETA: 0s - loss: 0.4103 - auc: 0.639 - 0s 21us/step - loss: 0.4121 - auc: 0.6403 - val_loss: 0.4121 - val_auc: 0.6321\n",
      "\n",
      "Epoch 00591: val_loss improved from 0.41218 to 0.41214, saving model to DeepFM.h5\n",
      "Epoch 592/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4041 - auc: 0.664 - ETA: 0s - loss: 0.4231 - auc: 0.615 - 0s 21us/step - loss: 0.4131 - auc: 0.6329 - val_loss: 0.4121 - val_auc: 0.6321\n",
      "\n",
      "Epoch 00592: val_loss improved from 0.41214 to 0.41211, saving model to DeepFM.h5\n",
      "Epoch 593/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4699 - auc: 0.602 - ETA: 0s - loss: 0.4144 - auc: 0.631 - 0s 19us/step - loss: 0.4124 - auc: 0.6359 - val_loss: 0.4121 - val_auc: 0.6317\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.41211 to 0.41208, saving model to DeepFM.h5\n",
      "Epoch 594/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4397 - auc: 0.606 - ETA: 0s - loss: 0.4214 - auc: 0.637 - 0s 22us/step - loss: 0.4124 - auc: 0.6406 - val_loss: 0.4121 - val_auc: 0.6319\n",
      "\n",
      "Epoch 00594: val_loss improved from 0.41208 to 0.41205, saving model to DeepFM.h5\n",
      "Epoch 595/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4010 - auc: 0.581 - ETA: 0s - loss: 0.4146 - auc: 0.615 - 0s 20us/step - loss: 0.4133 - auc: 0.6294 - val_loss: 0.4120 - val_auc: 0.6337\n",
      "\n",
      "Epoch 00595: val_loss improved from 0.41205 to 0.41202, saving model to DeepFM.h5\n",
      "Epoch 596/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3538 - auc: 0.678 - ETA: 0s - loss: 0.4177 - auc: 0.623 - 0s 20us/step - loss: 0.4149 - auc: 0.6158 - val_loss: 0.4120 - val_auc: 0.6328\n",
      "\n",
      "Epoch 00596: val_loss improved from 0.41202 to 0.41199, saving model to DeepFM.h5\n",
      "Epoch 597/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3603 - auc: 0.667 - ETA: 0s - loss: 0.4034 - auc: 0.630 - 0s 23us/step - loss: 0.4130 - auc: 0.6297 - val_loss: 0.4120 - val_auc: 0.6320\n",
      "\n",
      "Epoch 00597: val_loss improved from 0.41199 to 0.41195, saving model to DeepFM.h5\n",
      "Epoch 598/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4359 - auc: 0.657 - ETA: 0s - loss: 0.4236 - auc: 0.626 - 0s 20us/step - loss: 0.4111 - auc: 0.6423 - val_loss: 0.4119 - val_auc: 0.6332\n",
      "\n",
      "Epoch 00598: val_loss improved from 0.41195 to 0.41192, saving model to DeepFM.h5\n",
      "Epoch 599/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4185 - auc: 0.642 - ETA: 0s - loss: 0.4201 - auc: 0.643 - 0s 21us/step - loss: 0.4131 - auc: 0.6311 - val_loss: 0.4119 - val_auc: 0.6336\n",
      "\n",
      "Epoch 00599: val_loss improved from 0.41192 to 0.41189, saving model to DeepFM.h5\n",
      "Epoch 600/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4408 - auc: 0.652 - ETA: 0s - loss: 0.4240 - auc: 0.625 - 0s 20us/step - loss: 0.4139 - auc: 0.6274 - val_loss: 0.4119 - val_auc: 0.6348\n",
      "\n",
      "Epoch 00600: val_loss improved from 0.41189 to 0.41187, saving model to DeepFM.h5\n",
      "Epoch 601/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3943 - auc: 0.595 - ETA: 0s - loss: 0.4040 - auc: 0.626 - 0s 20us/step - loss: 0.4139 - auc: 0.6283 - val_loss: 0.4118 - val_auc: 0.6341\n",
      "\n",
      "Epoch 00601: val_loss improved from 0.41187 to 0.41183, saving model to DeepFM.h5\n",
      "Epoch 602/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3813 - auc: 0.710 - ETA: 0s - loss: 0.4227 - auc: 0.623 - 0s 21us/step - loss: 0.4135 - auc: 0.6265 - val_loss: 0.4118 - val_auc: 0.6355\n",
      "\n",
      "Epoch 00602: val_loss improved from 0.41183 to 0.41180, saving model to DeepFM.h5\n",
      "Epoch 603/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3672 - auc: 0.628 - ETA: 0s - loss: 0.4173 - auc: 0.620 - 0s 21us/step - loss: 0.4140 - auc: 0.6240 - val_loss: 0.4118 - val_auc: 0.6358\n",
      "\n",
      "Epoch 00603: val_loss improved from 0.41180 to 0.41178, saving model to DeepFM.h5\n",
      "Epoch 604/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3910 - auc: 0.498 - ETA: 0s - loss: 0.4058 - auc: 0.618 - 0s 20us/step - loss: 0.4146 - auc: 0.6230 - val_loss: 0.4117 - val_auc: 0.6355\n",
      "\n",
      "Epoch 00604: val_loss improved from 0.41178 to 0.41174, saving model to DeepFM.h5\n",
      "Epoch 605/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4649 - auc: 0.551 - ETA: 0s - loss: 0.4114 - auc: 0.618 - 0s 21us/step - loss: 0.4137 - auc: 0.6251 - val_loss: 0.4117 - val_auc: 0.6324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00605: val_loss improved from 0.41174 to 0.41170, saving model to DeepFM.h5\n",
      "Epoch 606/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.616 - ETA: 0s - loss: 0.4044 - auc: 0.651 - 0s 19us/step - loss: 0.4107 - auc: 0.6513 - val_loss: 0.4117 - val_auc: 0.6368\n",
      "\n",
      "Epoch 00606: val_loss improved from 0.41170 to 0.41167, saving model to DeepFM.h5\n",
      "Epoch 607/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3583 - auc: 0.640 - ETA: 0s - loss: 0.4065 - auc: 0.617 - 0s 21us/step - loss: 0.4130 - auc: 0.6331 - val_loss: 0.4116 - val_auc: 0.6378\n",
      "\n",
      "Epoch 00607: val_loss improved from 0.41167 to 0.41164, saving model to DeepFM.h5\n",
      "Epoch 608/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4415 - auc: 0.623 - ETA: 0s - loss: 0.4061 - auc: 0.638 - 0s 20us/step - loss: 0.4135 - auc: 0.6260 - val_loss: 0.4116 - val_auc: 0.6353\n",
      "\n",
      "Epoch 00608: val_loss improved from 0.41164 to 0.41161, saving model to DeepFM.h5\n",
      "Epoch 609/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3763 - auc: 0.601 - ETA: 0s - loss: 0.4129 - auc: 0.629 - 0s 20us/step - loss: 0.4138 - auc: 0.6261 - val_loss: 0.4116 - val_auc: 0.6348\n",
      "\n",
      "Epoch 00609: val_loss improved from 0.41161 to 0.41157, saving model to DeepFM.h5\n",
      "Epoch 610/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4250 - auc: 0.713 - ETA: 0s - loss: 0.4137 - auc: 0.640 - 0s 20us/step - loss: 0.4116 - auc: 0.6439 - val_loss: 0.4115 - val_auc: 0.6352\n",
      "\n",
      "Epoch 00610: val_loss improved from 0.41157 to 0.41153, saving model to DeepFM.h5\n",
      "Epoch 611/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4304 - auc: 0.630 - ETA: 0s - loss: 0.4176 - auc: 0.614 - 0s 21us/step - loss: 0.4139 - auc: 0.6247 - val_loss: 0.4115 - val_auc: 0.6355\n",
      "\n",
      "Epoch 00611: val_loss improved from 0.41153 to 0.41150, saving model to DeepFM.h5\n",
      "Epoch 612/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3690 - auc: 0.688 - ETA: 0s - loss: 0.4154 - auc: 0.635 - 0s 21us/step - loss: 0.4104 - auc: 0.6463 - val_loss: 0.4115 - val_auc: 0.6344\n",
      "\n",
      "Epoch 00612: val_loss improved from 0.41150 to 0.41146, saving model to DeepFM.h5\n",
      "Epoch 613/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3918 - auc: 0.673 - ETA: 0s - loss: 0.4206 - auc: 0.632 - 0s 20us/step - loss: 0.4123 - auc: 0.6391 - val_loss: 0.4114 - val_auc: 0.6355\n",
      "\n",
      "Epoch 00613: val_loss improved from 0.41146 to 0.41143, saving model to DeepFM.h5\n",
      "Epoch 614/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4220 - auc: 0.669 - ETA: 0s - loss: 0.4150 - auc: 0.623 - 0s 20us/step - loss: 0.4143 - auc: 0.6287 - val_loss: 0.4114 - val_auc: 0.6365\n",
      "\n",
      "Epoch 00614: val_loss improved from 0.41143 to 0.41141, saving model to DeepFM.h5\n",
      "Epoch 615/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4679 - auc: 0.607 - ETA: 0s - loss: 0.4174 - auc: 0.638 - 0s 21us/step - loss: 0.4123 - auc: 0.6357 - val_loss: 0.4114 - val_auc: 0.6372\n",
      "\n",
      "Epoch 00615: val_loss improved from 0.41141 to 0.41138, saving model to DeepFM.h5\n",
      "Epoch 616/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4383 - auc: 0.623 - ETA: 0s - loss: 0.4068 - auc: 0.629 - 0s 20us/step - loss: 0.4129 - auc: 0.6278 - val_loss: 0.4113 - val_auc: 0.6354\n",
      "\n",
      "Epoch 00616: val_loss improved from 0.41138 to 0.41133, saving model to DeepFM.h5\n",
      "Epoch 617/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4412 - auc: 0.577 - ETA: 0s - loss: 0.4074 - auc: 0.648 - 0s 21us/step - loss: 0.4111 - auc: 0.6413 - val_loss: 0.4113 - val_auc: 0.6352\n",
      "\n",
      "Epoch 00617: val_loss improved from 0.41133 to 0.41130, saving model to DeepFM.h5\n",
      "Epoch 618/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4622 - auc: 0.580 - ETA: 0s - loss: 0.4118 - auc: 0.625 - 0s 21us/step - loss: 0.4112 - auc: 0.6398 - val_loss: 0.4113 - val_auc: 0.6375\n",
      "\n",
      "Epoch 00618: val_loss improved from 0.41130 to 0.41127, saving model to DeepFM.h5\n",
      "Epoch 619/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4054 - auc: 0.689 - ETA: 0s - loss: 0.4234 - auc: 0.638 - 0s 24us/step - loss: 0.4119 - auc: 0.6345 - val_loss: 0.4112 - val_auc: 0.6370\n",
      "\n",
      "Epoch 00619: val_loss improved from 0.41127 to 0.41123, saving model to DeepFM.h5\n",
      "Epoch 620/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4914 - auc: 0.606 - ETA: 0s - loss: 0.4133 - auc: 0.626 - 0s 19us/step - loss: 0.4138 - auc: 0.6255 - val_loss: 0.4112 - val_auc: 0.6373\n",
      "\n",
      "Epoch 00620: val_loss improved from 0.41123 to 0.41120, saving model to DeepFM.h5\n",
      "Epoch 621/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3881 - auc: 0.534 - ETA: 0s - loss: 0.4187 - auc: 0.634 - 0s 20us/step - loss: 0.4132 - auc: 0.6297 - val_loss: 0.4112 - val_auc: 0.6387\n",
      "\n",
      "Epoch 00621: val_loss improved from 0.41120 to 0.41118, saving model to DeepFM.h5\n",
      "Epoch 622/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4320 - auc: 0.726 - ETA: 0s - loss: 0.4250 - auc: 0.643 - 0s 21us/step - loss: 0.4121 - auc: 0.6363 - val_loss: 0.4112 - val_auc: 0.6393\n",
      "\n",
      "Epoch 00622: val_loss improved from 0.41118 to 0.41116, saving model to DeepFM.h5\n",
      "Epoch 623/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3615 - auc: 0.709 - ETA: 0s - loss: 0.3980 - auc: 0.648 - 0s 20us/step - loss: 0.4116 - auc: 0.6387 - val_loss: 0.4111 - val_auc: 0.6382\n",
      "\n",
      "Epoch 00623: val_loss improved from 0.41116 to 0.41112, saving model to DeepFM.h5\n",
      "Epoch 624/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3543 - auc: 0.564 - ETA: 0s - loss: 0.4078 - auc: 0.634 - 0s 20us/step - loss: 0.4112 - auc: 0.6404 - val_loss: 0.4111 - val_auc: 0.6385\n",
      "\n",
      "Epoch 00624: val_loss improved from 0.41112 to 0.41108, saving model to DeepFM.h5\n",
      "Epoch 625/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4708 - auc: 0.547 - ETA: 0s - loss: 0.4191 - auc: 0.627 - 0s 19us/step - loss: 0.4130 - auc: 0.6323 - val_loss: 0.4111 - val_auc: 0.6401\n",
      "\n",
      "Epoch 00625: val_loss improved from 0.41108 to 0.41107, saving model to DeepFM.h5\n",
      "Epoch 626/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4060 - auc: 0.661 - ETA: 0s - loss: 0.4130 - auc: 0.625 - 0s 20us/step - loss: 0.4133 - auc: 0.6232 - val_loss: 0.4110 - val_auc: 0.6395\n",
      "\n",
      "Epoch 00626: val_loss improved from 0.41107 to 0.41103, saving model to DeepFM.h5\n",
      "Epoch 627/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3866 - auc: 0.586 - ETA: 0s - loss: 0.4145 - auc: 0.618 - 0s 21us/step - loss: 0.4139 - auc: 0.6194 - val_loss: 0.4110 - val_auc: 0.6386\n",
      "\n",
      "Epoch 00627: val_loss improved from 0.41103 to 0.41099, saving model to DeepFM.h5\n",
      "Epoch 628/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4297 - auc: 0.631 - ETA: 0s - loss: 0.4049 - auc: 0.630 - 0s 20us/step - loss: 0.4134 - auc: 0.6210 - val_loss: 0.4110 - val_auc: 0.6390\n",
      "\n",
      "Epoch 00628: val_loss improved from 0.41099 to 0.41096, saving model to DeepFM.h5\n",
      "Epoch 629/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3773 - auc: 0.615 - ETA: 0s - loss: 0.4070 - auc: 0.649 - 0s 20us/step - loss: 0.4104 - auc: 0.6487 - val_loss: 0.4109 - val_auc: 0.6393\n",
      "\n",
      "Epoch 00629: val_loss improved from 0.41096 to 0.41093, saving model to DeepFM.h5\n",
      "Epoch 630/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.693 - ETA: 0s - loss: 0.4039 - auc: 0.645 - 0s 22us/step - loss: 0.4111 - auc: 0.6433 - val_loss: 0.4109 - val_auc: 0.6375\n",
      "\n",
      "Epoch 00630: val_loss improved from 0.41093 to 0.41089, saving model to DeepFM.h5\n",
      "Epoch 631/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4992 - auc: 0.674 - ETA: 0s - loss: 0.4112 - auc: 0.650 - 0s 21us/step - loss: 0.4103 - auc: 0.6450 - val_loss: 0.4109 - val_auc: 0.6389\n",
      "\n",
      "Epoch 00631: val_loss improved from 0.41089 to 0.41085, saving model to DeepFM.h5\n",
      "Epoch 632/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4133 - auc: 0.671 - ETA: 0s - loss: 0.4097 - auc: 0.636 - 0s 22us/step - loss: 0.4112 - auc: 0.6385 - val_loss: 0.4108 - val_auc: 0.6367\n",
      "\n",
      "Epoch 00632: val_loss improved from 0.41085 to 0.41081, saving model to DeepFM.h5\n",
      "Epoch 633/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3316 - auc: 0.643 - ETA: 0s - loss: 0.4187 - auc: 0.635 - 0s 21us/step - loss: 0.4109 - auc: 0.6404 - val_loss: 0.4108 - val_auc: 0.6382\n",
      "\n",
      "Epoch 00633: val_loss improved from 0.41081 to 0.41078, saving model to DeepFM.h5\n",
      "Epoch 634/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4205 - auc: 0.684 - ETA: 0s - loss: 0.4123 - auc: 0.641 - 0s 19us/step - loss: 0.4118 - auc: 0.6351 - val_loss: 0.4107 - val_auc: 0.6386\n",
      "\n",
      "Epoch 00634: val_loss improved from 0.41078 to 0.41074, saving model to DeepFM.h5\n",
      "Epoch 635/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3664 - auc: 0.522 - ETA: 0s - loss: 0.3927 - auc: 0.634 - 0s 21us/step - loss: 0.4115 - auc: 0.6381 - val_loss: 0.4107 - val_auc: 0.6397\n",
      "\n",
      "Epoch 00635: val_loss improved from 0.41074 to 0.41072, saving model to DeepFM.h5\n",
      "Epoch 636/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3813 - auc: 0.666 - ETA: 0s - loss: 0.4112 - auc: 0.638 - 0s 20us/step - loss: 0.4114 - auc: 0.6403 - val_loss: 0.4107 - val_auc: 0.6389\n",
      "\n",
      "Epoch 00636: val_loss improved from 0.41072 to 0.41068, saving model to DeepFM.h5\n",
      "Epoch 637/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4120 - auc: 0.686 - ETA: 0s - loss: 0.4144 - auc: 0.636 - 0s 20us/step - loss: 0.4105 - auc: 0.6441 - val_loss: 0.4106 - val_auc: 0.6399\n",
      "\n",
      "Epoch 00637: val_loss improved from 0.41068 to 0.41065, saving model to DeepFM.h5\n",
      "Epoch 638/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4609 - auc: 0.635 - ETA: 0s - loss: 0.4004 - auc: 0.648 - 0s 21us/step - loss: 0.4120 - auc: 0.6348 - val_loss: 0.4106 - val_auc: 0.6395\n",
      "\n",
      "Epoch 00638: val_loss improved from 0.41065 to 0.41061, saving model to DeepFM.h5\n",
      "Epoch 639/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4461 - auc: 0.644 - ETA: 0s - loss: 0.4226 - auc: 0.620 - 0s 21us/step - loss: 0.4132 - auc: 0.6286 - val_loss: 0.4106 - val_auc: 0.6394\n",
      "\n",
      "Epoch 00639: val_loss improved from 0.41061 to 0.41058, saving model to DeepFM.h5\n",
      "Epoch 640/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3920 - auc: 0.636 - ETA: 0s - loss: 0.4171 - auc: 0.649 - 0s 21us/step - loss: 0.4108 - auc: 0.6454 - val_loss: 0.4106 - val_auc: 0.6411\n",
      "\n",
      "Epoch 00640: val_loss improved from 0.41058 to 0.41055, saving model to DeepFM.h5\n",
      "Epoch 641/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3965 - auc: 0.704 - ETA: 0s - loss: 0.4113 - auc: 0.635 - 0s 19us/step - loss: 0.4103 - auc: 0.6429 - val_loss: 0.4105 - val_auc: 0.6416\n",
      "\n",
      "Epoch 00641: val_loss improved from 0.41055 to 0.41052, saving model to DeepFM.h5\n",
      "Epoch 642/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3964 - auc: 0.602 - ETA: 0s - loss: 0.4078 - auc: 0.628 - 0s 20us/step - loss: 0.4123 - auc: 0.6314 - val_loss: 0.4105 - val_auc: 0.6399\n",
      "\n",
      "Epoch 00642: val_loss improved from 0.41052 to 0.41048, saving model to DeepFM.h5\n",
      "Epoch 643/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3586 - auc: 0.617 - ETA: 0s - loss: 0.4131 - auc: 0.631 - 0s 20us/step - loss: 0.4114 - auc: 0.6371 - val_loss: 0.4104 - val_auc: 0.6398\n",
      "\n",
      "Epoch 00643: val_loss improved from 0.41048 to 0.41044, saving model to DeepFM.h5\n",
      "Epoch 644/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4058 - auc: 0.587 - ETA: 0s - loss: 0.4035 - auc: 0.649 - 0s 21us/step - loss: 0.4096 - auc: 0.6531 - val_loss: 0.4104 - val_auc: 0.6408\n",
      "\n",
      "Epoch 00644: val_loss improved from 0.41044 to 0.41042, saving model to DeepFM.h5\n",
      "Epoch 645/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3651 - auc: 0.733 - ETA: 0s - loss: 0.4221 - auc: 0.652 - 0s 20us/step - loss: 0.4092 - auc: 0.6520 - val_loss: 0.4104 - val_auc: 0.6415\n",
      "\n",
      "Epoch 00645: val_loss improved from 0.41042 to 0.41039, saving model to DeepFM.h5\n",
      "Epoch 646/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3902 - auc: 0.614 - ETA: 0s - loss: 0.4104 - auc: 0.643 - 0s 22us/step - loss: 0.4113 - auc: 0.6405 - val_loss: 0.4103 - val_auc: 0.6397\n",
      "\n",
      "Epoch 00646: val_loss improved from 0.41039 to 0.41034, saving model to DeepFM.h5\n",
      "Epoch 647/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4011 - auc: 0.657 - ETA: 0s - loss: 0.4056 - auc: 0.623 - 0s 23us/step - loss: 0.4136 - auc: 0.6226 - val_loss: 0.4103 - val_auc: 0.6393\n",
      "\n",
      "Epoch 00647: val_loss improved from 0.41034 to 0.41030, saving model to DeepFM.h5\n",
      "Epoch 648/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3662 - auc: 0.628 - ETA: 0s - loss: 0.4182 - auc: 0.645 - 0s 22us/step - loss: 0.4118 - auc: 0.6388 - val_loss: 0.4103 - val_auc: 0.6396\n",
      "\n",
      "Epoch 00648: val_loss improved from 0.41030 to 0.41027, saving model to DeepFM.h5\n",
      "Epoch 649/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3976 - auc: 0.703 - ETA: 0s - loss: 0.4214 - auc: 0.651 - 0s 23us/step - loss: 0.4126 - auc: 0.6340 - val_loss: 0.4102 - val_auc: 0.6397\n",
      "\n",
      "Epoch 00649: val_loss improved from 0.41027 to 0.41023, saving model to DeepFM.h5\n",
      "Epoch 650/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3827 - auc: 0.674 - ETA: 0s - loss: 0.4095 - auc: 0.629 - 0s 20us/step - loss: 0.4118 - auc: 0.6382 - val_loss: 0.4102 - val_auc: 0.6392\n",
      "\n",
      "Epoch 00650: val_loss improved from 0.41023 to 0.41020, saving model to DeepFM.h5\n",
      "Epoch 651/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4016 - auc: 0.614 - ETA: 0s - loss: 0.4186 - auc: 0.623 - 0s 20us/step - loss: 0.4123 - auc: 0.6331 - val_loss: 0.4102 - val_auc: 0.6396\n",
      "\n",
      "Epoch 00651: val_loss improved from 0.41020 to 0.41017, saving model to DeepFM.h5\n",
      "Epoch 652/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4382 - auc: 0.552 - ETA: 0s - loss: 0.4080 - auc: 0.655 - 0s 20us/step - loss: 0.4095 - auc: 0.6526 - val_loss: 0.4101 - val_auc: 0.6399\n",
      "\n",
      "Epoch 00652: val_loss improved from 0.41017 to 0.41014, saving model to DeepFM.h5\n",
      "Epoch 653/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4371 - auc: 0.631 - ETA: 0s - loss: 0.3998 - auc: 0.647 - 0s 23us/step - loss: 0.4110 - auc: 0.6379 - val_loss: 0.4101 - val_auc: 0.6401\n",
      "\n",
      "Epoch 00653: val_loss improved from 0.41014 to 0.41010, saving model to DeepFM.h5\n",
      "Epoch 654/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3937 - auc: 0.678 - ETA: 0s - loss: 0.4130 - auc: 0.653 - 0s 20us/step - loss: 0.4102 - auc: 0.6468 - val_loss: 0.4101 - val_auc: 0.6403\n",
      "\n",
      "Epoch 00654: val_loss improved from 0.41010 to 0.41007, saving model to DeepFM.h5\n",
      "Epoch 655/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4683 - auc: 0.586 - ETA: 0s - loss: 0.4164 - auc: 0.628 - 0s 20us/step - loss: 0.4125 - auc: 0.6321 - val_loss: 0.4100 - val_auc: 0.6406\n",
      "\n",
      "Epoch 00655: val_loss improved from 0.41007 to 0.41004, saving model to DeepFM.h5\n",
      "Epoch 656/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4206 - auc: 0.629 - ETA: 0s - loss: 0.4079 - auc: 0.653 - 0s 22us/step - loss: 0.4112 - auc: 0.6379 - val_loss: 0.4100 - val_auc: 0.6418\n",
      "\n",
      "Epoch 00656: val_loss improved from 0.41004 to 0.41000, saving model to DeepFM.h5\n",
      "Epoch 657/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3903 - auc: 0.567 - ETA: 0s - loss: 0.4160 - auc: 0.641 - 0s 21us/step - loss: 0.4116 - auc: 0.6328 - val_loss: 0.4100 - val_auc: 0.6412\n",
      "\n",
      "Epoch 00657: val_loss improved from 0.41000 to 0.40998, saving model to DeepFM.h5\n",
      "Epoch 658/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3046 - auc: 0.649 - ETA: 0s - loss: 0.4065 - auc: 0.654 - 0s 21us/step - loss: 0.4107 - auc: 0.6437 - val_loss: 0.4099 - val_auc: 0.6424\n",
      "\n",
      "Epoch 00658: val_loss improved from 0.40998 to 0.40994, saving model to DeepFM.h5\n",
      "Epoch 659/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3688 - auc: 0.678 - ETA: 0s - loss: 0.4199 - auc: 0.632 - 0s 22us/step - loss: 0.4114 - auc: 0.6379 - val_loss: 0.4099 - val_auc: 0.6424\n",
      "\n",
      "Epoch 00659: val_loss improved from 0.40994 to 0.40991, saving model to DeepFM.h5\n",
      "Epoch 660/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4073 - auc: 0.665 - ETA: 0s - loss: 0.3970 - auc: 0.642 - 0s 20us/step - loss: 0.4108 - auc: 0.6441 - val_loss: 0.4099 - val_auc: 0.6416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00660: val_loss improved from 0.40991 to 0.40986, saving model to DeepFM.h5\n",
      "Epoch 661/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3865 - auc: 0.564 - ETA: 0s - loss: 0.4159 - auc: 0.646 - 0s 21us/step - loss: 0.4095 - auc: 0.6447 - val_loss: 0.4098 - val_auc: 0.6423\n",
      "\n",
      "Epoch 00661: val_loss improved from 0.40986 to 0.40983, saving model to DeepFM.h5\n",
      "Epoch 662/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4704 - auc: 0.642 - ETA: 0s - loss: 0.4239 - auc: 0.639 - 0s 25us/step - loss: 0.4104 - auc: 0.6426 - val_loss: 0.4098 - val_auc: 0.6425\n",
      "\n",
      "Epoch 00662: val_loss improved from 0.40983 to 0.40979, saving model to DeepFM.h5\n",
      "Epoch 663/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3440 - auc: 0.700 - ETA: 0s - loss: 0.4132 - auc: 0.651 - 0s 22us/step - loss: 0.4102 - auc: 0.6431 - val_loss: 0.4098 - val_auc: 0.6418\n",
      "\n",
      "Epoch 00663: val_loss improved from 0.40979 to 0.40976, saving model to DeepFM.h5\n",
      "Epoch 664/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4473 - auc: 0.696 - ETA: 0s - loss: 0.4264 - auc: 0.628 - 0s 21us/step - loss: 0.4127 - auc: 0.6248 - val_loss: 0.4097 - val_auc: 0.6419\n",
      "\n",
      "Epoch 00664: val_loss improved from 0.40976 to 0.40972, saving model to DeepFM.h5\n",
      "Epoch 665/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3836 - auc: 0.592 - ETA: 0s - loss: 0.4060 - auc: 0.638 - 0s 19us/step - loss: 0.4104 - auc: 0.6406 - val_loss: 0.4097 - val_auc: 0.6425\n",
      "\n",
      "Epoch 00665: val_loss improved from 0.40972 to 0.40969, saving model to DeepFM.h5\n",
      "Epoch 666/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3966 - auc: 0.632 - ETA: 0s - loss: 0.4108 - auc: 0.627 - 0s 20us/step - loss: 0.4101 - auc: 0.6433 - val_loss: 0.4097 - val_auc: 0.6431\n",
      "\n",
      "Epoch 00666: val_loss improved from 0.40969 to 0.40966, saving model to DeepFM.h5\n",
      "Epoch 667/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4867 - auc: 0.600 - ETA: 0s - loss: 0.4113 - auc: 0.643 - 0s 20us/step - loss: 0.4107 - auc: 0.6420 - val_loss: 0.4096 - val_auc: 0.6431\n",
      "\n",
      "Epoch 00667: val_loss improved from 0.40966 to 0.40963, saving model to DeepFM.h5\n",
      "Epoch 668/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3986 - auc: 0.687 - ETA: 0s - loss: 0.4078 - auc: 0.647 - 0s 21us/step - loss: 0.4091 - auc: 0.6520 - val_loss: 0.4096 - val_auc: 0.6433\n",
      "\n",
      "Epoch 00668: val_loss improved from 0.40963 to 0.40960, saving model to DeepFM.h5\n",
      "Epoch 669/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4333 - auc: 0.628 - ETA: 0s - loss: 0.4152 - auc: 0.640 - 0s 19us/step - loss: 0.4106 - auc: 0.6389 - val_loss: 0.4096 - val_auc: 0.6435\n",
      "\n",
      "Epoch 00669: val_loss improved from 0.40960 to 0.40957, saving model to DeepFM.h5\n",
      "Epoch 670/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4230 - auc: 0.693 - ETA: 0s - loss: 0.4086 - auc: 0.640 - 0s 20us/step - loss: 0.4110 - auc: 0.6388 - val_loss: 0.4095 - val_auc: 0.6442\n",
      "\n",
      "Epoch 00670: val_loss improved from 0.40957 to 0.40952, saving model to DeepFM.h5\n",
      "Epoch 671/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4007 - auc: 0.719 - ETA: 0s - loss: 0.4112 - auc: 0.651 - 0s 20us/step - loss: 0.4089 - auc: 0.6509 - val_loss: 0.4095 - val_auc: 0.6437\n",
      "\n",
      "Epoch 00671: val_loss improved from 0.40952 to 0.40949, saving model to DeepFM.h5\n",
      "Epoch 672/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4199 - auc: 0.625 - ETA: 0s - loss: 0.4141 - auc: 0.628 - 0s 19us/step - loss: 0.4120 - auc: 0.6309 - val_loss: 0.4095 - val_auc: 0.6450\n",
      "\n",
      "Epoch 00672: val_loss improved from 0.40949 to 0.40945, saving model to DeepFM.h5\n",
      "Epoch 673/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3857 - auc: 0.708 - ETA: 0s - loss: 0.4059 - auc: 0.642 - 0s 23us/step - loss: 0.4087 - auc: 0.6519 - val_loss: 0.4094 - val_auc: 0.6432\n",
      "\n",
      "Epoch 00673: val_loss improved from 0.40945 to 0.40940, saving model to DeepFM.h5\n",
      "Epoch 674/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3678 - auc: 0.598 - ETA: 0s - loss: 0.4124 - auc: 0.644 - 0s 20us/step - loss: 0.4105 - auc: 0.6473 - val_loss: 0.4094 - val_auc: 0.6436\n",
      "\n",
      "Epoch 00674: val_loss improved from 0.40940 to 0.40936, saving model to DeepFM.h5\n",
      "Epoch 675/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4081 - auc: 0.629 - ETA: 0s - loss: 0.4034 - auc: 0.652 - 0s 20us/step - loss: 0.4104 - auc: 0.6430 - val_loss: 0.4093 - val_auc: 0.6432\n",
      "\n",
      "Epoch 00675: val_loss improved from 0.40936 to 0.40932, saving model to DeepFM.h5\n",
      "Epoch 676/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3707 - auc: 0.601 - ETA: 0s - loss: 0.4074 - auc: 0.633 - 0s 22us/step - loss: 0.4104 - auc: 0.6403 - val_loss: 0.4093 - val_auc: 0.6429\n",
      "\n",
      "Epoch 00676: val_loss improved from 0.40932 to 0.40928, saving model to DeepFM.h5\n",
      "Epoch 677/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3790 - auc: 0.666 - ETA: 0s - loss: 0.3998 - auc: 0.645 - 0s 21us/step - loss: 0.4089 - auc: 0.6510 - val_loss: 0.4092 - val_auc: 0.6432\n",
      "\n",
      "Epoch 00677: val_loss improved from 0.40928 to 0.40925, saving model to DeepFM.h5\n",
      "Epoch 678/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4345 - auc: 0.642 - ETA: 0s - loss: 0.4264 - auc: 0.645 - 0s 22us/step - loss: 0.4100 - auc: 0.6492 - val_loss: 0.4092 - val_auc: 0.6456\n",
      "\n",
      "Epoch 00678: val_loss improved from 0.40925 to 0.40922, saving model to DeepFM.h5\n",
      "Epoch 679/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3985 - auc: 0.625 - ETA: 0s - loss: 0.4193 - auc: 0.646 - 0s 20us/step - loss: 0.4110 - auc: 0.6395 - val_loss: 0.4092 - val_auc: 0.6454\n",
      "\n",
      "Epoch 00679: val_loss improved from 0.40922 to 0.40918, saving model to DeepFM.h5\n",
      "Epoch 680/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5230 - auc: 0.709 - ETA: 0s - loss: 0.4190 - auc: 0.653 - 0s 21us/step - loss: 0.4094 - auc: 0.6509 - val_loss: 0.4092 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00680: val_loss improved from 0.40918 to 0.40915, saving model to DeepFM.h5\n",
      "Epoch 681/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3802 - auc: 0.581 - ETA: 0s - loss: 0.4069 - auc: 0.645 - 0s 20us/step - loss: 0.4107 - auc: 0.6396 - val_loss: 0.4091 - val_auc: 0.6458\n",
      "\n",
      "Epoch 00681: val_loss improved from 0.40915 to 0.40912, saving model to DeepFM.h5\n",
      "Epoch 682/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3689 - auc: 0.666 - ETA: 0s - loss: 0.4129 - auc: 0.647 - 0s 22us/step - loss: 0.4089 - auc: 0.6479 - val_loss: 0.4091 - val_auc: 0.6464\n",
      "\n",
      "Epoch 00682: val_loss improved from 0.40912 to 0.40909, saving model to DeepFM.h5\n",
      "Epoch 683/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3952 - auc: 0.658 - ETA: 0s - loss: 0.4133 - auc: 0.648 - 0s 20us/step - loss: 0.4090 - auc: 0.6514 - val_loss: 0.4090 - val_auc: 0.6456\n",
      "\n",
      "Epoch 00683: val_loss improved from 0.40909 to 0.40905, saving model to DeepFM.h5\n",
      "Epoch 684/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4025 - auc: 0.608 - ETA: 0s - loss: 0.4085 - auc: 0.635 - 0s 22us/step - loss: 0.4101 - auc: 0.6432 - val_loss: 0.4090 - val_auc: 0.6456\n",
      "\n",
      "Epoch 00684: val_loss improved from 0.40905 to 0.40900, saving model to DeepFM.h5\n",
      "Epoch 685/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3556 - auc: 0.645 - ETA: 0s - loss: 0.4010 - auc: 0.643 - 0s 20us/step - loss: 0.4085 - auc: 0.6521 - val_loss: 0.4090 - val_auc: 0.6447\n",
      "\n",
      "Epoch 00685: val_loss improved from 0.40900 to 0.40897, saving model to DeepFM.h5\n",
      "Epoch 686/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4451 - auc: 0.515 - ETA: 0s - loss: 0.4087 - auc: 0.654 - 0s 22us/step - loss: 0.4082 - auc: 0.6537 - val_loss: 0.4089 - val_auc: 0.6458\n",
      "\n",
      "Epoch 00686: val_loss improved from 0.40897 to 0.40893, saving model to DeepFM.h5\n",
      "Epoch 687/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3705 - auc: 0.647 - ETA: 0s - loss: 0.4048 - auc: 0.640 - 0s 20us/step - loss: 0.4098 - auc: 0.6427 - val_loss: 0.4089 - val_auc: 0.6457\n",
      "\n",
      "Epoch 00687: val_loss improved from 0.40893 to 0.40889, saving model to DeepFM.h5\n",
      "Epoch 688/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4066 - auc: 0.683 - ETA: 0s - loss: 0.3995 - auc: 0.646 - 0s 21us/step - loss: 0.4095 - auc: 0.6485 - val_loss: 0.4089 - val_auc: 0.6458\n",
      "\n",
      "Epoch 00688: val_loss improved from 0.40889 to 0.40885, saving model to DeepFM.h5\n",
      "Epoch 689/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4476 - auc: 0.592 - ETA: 0s - loss: 0.4134 - auc: 0.624 - 0s 21us/step - loss: 0.4109 - auc: 0.6357 - val_loss: 0.4088 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00689: val_loss improved from 0.40885 to 0.40882, saving model to DeepFM.h5\n",
      "Epoch 690/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3679 - auc: 0.624 - ETA: 0s - loss: 0.3998 - auc: 0.669 - 0s 20us/step - loss: 0.4077 - auc: 0.6596 - val_loss: 0.4088 - val_auc: 0.6457\n",
      "\n",
      "Epoch 00690: val_loss improved from 0.40882 to 0.40878, saving model to DeepFM.h5\n",
      "Epoch 691/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4186 - auc: 0.628 - ETA: 0s - loss: 0.4077 - auc: 0.666 - 0s 20us/step - loss: 0.4066 - auc: 0.6611 - val_loss: 0.4087 - val_auc: 0.6462\n",
      "\n",
      "Epoch 00691: val_loss improved from 0.40878 to 0.40875, saving model to DeepFM.h5\n",
      "Epoch 692/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4687 - auc: 0.595 - ETA: 0s - loss: 0.4101 - auc: 0.645 - 0s 20us/step - loss: 0.4090 - auc: 0.6499 - val_loss: 0.4087 - val_auc: 0.6458\n",
      "\n",
      "Epoch 00692: val_loss improved from 0.40875 to 0.40871, saving model to DeepFM.h5\n",
      "Epoch 693/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4389 - auc: 0.602 - ETA: 0s - loss: 0.4126 - auc: 0.637 - 0s 21us/step - loss: 0.4099 - auc: 0.6416 - val_loss: 0.4087 - val_auc: 0.6457\n",
      "\n",
      "Epoch 00693: val_loss improved from 0.40871 to 0.40868, saving model to DeepFM.h5\n",
      "Epoch 694/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3814 - auc: 0.601 - ETA: 0s - loss: 0.4060 - auc: 0.645 - 0s 20us/step - loss: 0.4096 - auc: 0.6446 - val_loss: 0.4086 - val_auc: 0.6456\n",
      "\n",
      "Epoch 00694: val_loss improved from 0.40868 to 0.40865, saving model to DeepFM.h5\n",
      "Epoch 695/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4019 - auc: 0.659 - ETA: 0s - loss: 0.4039 - auc: 0.664 - 0s 20us/step - loss: 0.4097 - auc: 0.6443 - val_loss: 0.4086 - val_auc: 0.6463\n",
      "\n",
      "Epoch 00695: val_loss improved from 0.40865 to 0.40861, saving model to DeepFM.h5\n",
      "Epoch 696/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3950 - auc: 0.701 - ETA: 0s - loss: 0.4052 - auc: 0.639 - 0s 21us/step - loss: 0.4100 - auc: 0.6391 - val_loss: 0.4086 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00696: val_loss improved from 0.40861 to 0.40857, saving model to DeepFM.h5\n",
      "Epoch 697/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4246 - auc: 0.668 - ETA: 0s - loss: 0.3994 - auc: 0.640 - 0s 21us/step - loss: 0.4114 - auc: 0.6330 - val_loss: 0.4085 - val_auc: 0.6442\n",
      "\n",
      "Epoch 00697: val_loss improved from 0.40857 to 0.40853, saving model to DeepFM.h5\n",
      "Epoch 698/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3808 - auc: 0.615 - ETA: 0s - loss: 0.4110 - auc: 0.638 - 0s 20us/step - loss: 0.4095 - auc: 0.6452 - val_loss: 0.4085 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00698: val_loss improved from 0.40853 to 0.40850, saving model to DeepFM.h5\n",
      "Epoch 699/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4908 - auc: 0.651 - ETA: 0s - loss: 0.4119 - auc: 0.651 - 0s 19us/step - loss: 0.4085 - auc: 0.6537 - val_loss: 0.4085 - val_auc: 0.6453\n",
      "\n",
      "Epoch 00699: val_loss improved from 0.40850 to 0.40847, saving model to DeepFM.h5\n",
      "Epoch 700/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3500 - auc: 0.645 - ETA: 0s - loss: 0.4121 - auc: 0.640 - 0s 21us/step - loss: 0.4086 - auc: 0.6511 - val_loss: 0.4084 - val_auc: 0.6456\n",
      "\n",
      "Epoch 00700: val_loss improved from 0.40847 to 0.40843, saving model to DeepFM.h5\n",
      "Epoch 701/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4498 - auc: 0.640 - ETA: 0s - loss: 0.4118 - auc: 0.652 - 0s 21us/step - loss: 0.4099 - auc: 0.6468 - val_loss: 0.4084 - val_auc: 0.6459\n",
      "\n",
      "Epoch 00701: val_loss improved from 0.40843 to 0.40840, saving model to DeepFM.h5\n",
      "Epoch 702/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4318 - auc: 0.690 - ETA: 0s - loss: 0.4202 - auc: 0.622 - 0s 21us/step - loss: 0.4105 - auc: 0.6361 - val_loss: 0.4084 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00702: val_loss improved from 0.40840 to 0.40836, saving model to DeepFM.h5\n",
      "Epoch 703/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4145 - auc: 0.609 - ETA: 0s - loss: 0.4116 - auc: 0.665 - 0s 21us/step - loss: 0.4072 - auc: 0.6591 - val_loss: 0.4083 - val_auc: 0.6467\n",
      "\n",
      "Epoch 00703: val_loss improved from 0.40836 to 0.40833, saving model to DeepFM.h5\n",
      "Epoch 704/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3939 - auc: 0.701 - ETA: 0s - loss: 0.4129 - auc: 0.629 - 0s 19us/step - loss: 0.4117 - auc: 0.6293 - val_loss: 0.4083 - val_auc: 0.6455\n",
      "\n",
      "Epoch 00704: val_loss improved from 0.40833 to 0.40829, saving model to DeepFM.h5\n",
      "Epoch 705/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3385 - auc: 0.687 - ETA: 0s - loss: 0.4082 - auc: 0.657 - 0s 20us/step - loss: 0.4079 - auc: 0.6559 - val_loss: 0.4083 - val_auc: 0.6452\n",
      "\n",
      "Epoch 00705: val_loss improved from 0.40829 to 0.40825, saving model to DeepFM.h5\n",
      "Epoch 706/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4026 - auc: 0.705 - ETA: 0s - loss: 0.4095 - auc: 0.640 - 0s 22us/step - loss: 0.4092 - auc: 0.6503 - val_loss: 0.4082 - val_auc: 0.6457\n",
      "\n",
      "Epoch 00706: val_loss improved from 0.40825 to 0.40822, saving model to DeepFM.h5\n",
      "Epoch 707/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4027 - auc: 0.658 - ETA: 0s - loss: 0.4063 - auc: 0.668 - 0s 21us/step - loss: 0.4072 - auc: 0.6561 - val_loss: 0.4082 - val_auc: 0.6476\n",
      "\n",
      "Epoch 00707: val_loss improved from 0.40822 to 0.40819, saving model to DeepFM.h5\n",
      "Epoch 708/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3240 - auc: 0.544 - ETA: 0s - loss: 0.3999 - auc: 0.647 - 0s 22us/step - loss: 0.4081 - auc: 0.6523 - val_loss: 0.4081 - val_auc: 0.6473\n",
      "\n",
      "Epoch 00708: val_loss improved from 0.40819 to 0.40815, saving model to DeepFM.h5\n",
      "Epoch 709/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4123 - auc: 0.592 - ETA: 0s - loss: 0.4109 - auc: 0.649 - 0s 20us/step - loss: 0.4087 - auc: 0.6452 - val_loss: 0.4081 - val_auc: 0.6471\n",
      "\n",
      "Epoch 00709: val_loss improved from 0.40815 to 0.40811, saving model to DeepFM.h5\n",
      "Epoch 710/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3975 - auc: 0.662 - ETA: 0s - loss: 0.4137 - auc: 0.650 - 0s 19us/step - loss: 0.4097 - auc: 0.6432 - val_loss: 0.4081 - val_auc: 0.6471\n",
      "\n",
      "Epoch 00710: val_loss improved from 0.40811 to 0.40807, saving model to DeepFM.h5\n",
      "Epoch 711/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4081 - auc: 0.671 - ETA: 0s - loss: 0.4027 - auc: 0.661 - 0s 22us/step - loss: 0.4074 - auc: 0.6616 - val_loss: 0.4080 - val_auc: 0.6447\n",
      "\n",
      "Epoch 00711: val_loss improved from 0.40807 to 0.40802, saving model to DeepFM.h5\n",
      "Epoch 712/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4160 - auc: 0.626 - ETA: 0s - loss: 0.4191 - auc: 0.638 - 0s 20us/step - loss: 0.4094 - auc: 0.6454 - val_loss: 0.4080 - val_auc: 0.6448\n",
      "\n",
      "Epoch 00712: val_loss improved from 0.40802 to 0.40798, saving model to DeepFM.h5\n",
      "Epoch 713/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3523 - auc: 0.649 - ETA: 0s - loss: 0.4080 - auc: 0.640 - 0s 20us/step - loss: 0.4103 - auc: 0.6411 - val_loss: 0.4080 - val_auc: 0.6467\n",
      "\n",
      "Epoch 00713: val_loss improved from 0.40798 to 0.40795, saving model to DeepFM.h5\n",
      "Epoch 714/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4304 - auc: 0.612 - ETA: 0s - loss: 0.4125 - auc: 0.643 - 0s 21us/step - loss: 0.4084 - auc: 0.6540 - val_loss: 0.4079 - val_auc: 0.6471\n",
      "\n",
      "Epoch 00714: val_loss improved from 0.40795 to 0.40791, saving model to DeepFM.h5\n",
      "Epoch 715/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4612 - auc: 0.625 - ETA: 0s - loss: 0.4157 - auc: 0.646 - 0s 21us/step - loss: 0.4115 - auc: 0.6355 - val_loss: 0.4079 - val_auc: 0.6449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00715: val_loss improved from 0.40791 to 0.40787, saving model to DeepFM.h5\n",
      "Epoch 716/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5037 - auc: 0.678 - ETA: 0s - loss: 0.4120 - auc: 0.664 - 0s 21us/step - loss: 0.4084 - auc: 0.6542 - val_loss: 0.4078 - val_auc: 0.6452\n",
      "\n",
      "Epoch 00716: val_loss improved from 0.40787 to 0.40784, saving model to DeepFM.h5\n",
      "Epoch 717/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3786 - auc: 0.588 - ETA: 0s - loss: 0.4080 - auc: 0.668 - 0s 21us/step - loss: 0.4073 - auc: 0.6608 - val_loss: 0.4078 - val_auc: 0.6472\n",
      "\n",
      "Epoch 00717: val_loss improved from 0.40784 to 0.40780, saving model to DeepFM.h5\n",
      "Epoch 718/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4518 - auc: 0.612 - ETA: 0s - loss: 0.4069 - auc: 0.649 - 0s 21us/step - loss: 0.4082 - auc: 0.6511 - val_loss: 0.4078 - val_auc: 0.6483\n",
      "\n",
      "Epoch 00718: val_loss improved from 0.40780 to 0.40776, saving model to DeepFM.h5\n",
      "Epoch 719/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3702 - auc: 0.642 - ETA: 0s - loss: 0.4073 - auc: 0.652 - 0s 20us/step - loss: 0.4079 - auc: 0.6473 - val_loss: 0.4077 - val_auc: 0.6486\n",
      "\n",
      "Epoch 00719: val_loss improved from 0.40776 to 0.40772, saving model to DeepFM.h5\n",
      "Epoch 720/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4002 - auc: 0.681 - ETA: 0s - loss: 0.4088 - auc: 0.648 - 0s 20us/step - loss: 0.4088 - auc: 0.6445 - val_loss: 0.4077 - val_auc: 0.6480\n",
      "\n",
      "Epoch 00720: val_loss improved from 0.40772 to 0.40768, saving model to DeepFM.h5\n",
      "Epoch 721/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3816 - auc: 0.608 - ETA: 0s - loss: 0.4043 - auc: 0.654 - 0s 20us/step - loss: 0.4075 - auc: 0.6547 - val_loss: 0.4076 - val_auc: 0.6471\n",
      "\n",
      "Epoch 00721: val_loss improved from 0.40768 to 0.40764, saving model to DeepFM.h5\n",
      "Epoch 722/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4958 - auc: 0.652 - ETA: 0s - loss: 0.4112 - auc: 0.645 - 0s 20us/step - loss: 0.4078 - auc: 0.6552 - val_loss: 0.4076 - val_auc: 0.6483\n",
      "\n",
      "Epoch 00722: val_loss improved from 0.40764 to 0.40761, saving model to DeepFM.h5\n",
      "Epoch 723/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4205 - auc: 0.736 - ETA: 0s - loss: 0.4093 - auc: 0.669 - 0s 21us/step - loss: 0.4044 - auc: 0.6722 - val_loss: 0.4076 - val_auc: 0.6488\n",
      "\n",
      "Epoch 00723: val_loss improved from 0.40761 to 0.40758, saving model to DeepFM.h5\n",
      "Epoch 724/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3748 - auc: 0.626 - ETA: 0s - loss: 0.3997 - auc: 0.667 - 0s 21us/step - loss: 0.4077 - auc: 0.6567 - val_loss: 0.4075 - val_auc: 0.6484\n",
      "\n",
      "Epoch 00724: val_loss improved from 0.40758 to 0.40753, saving model to DeepFM.h5\n",
      "Epoch 725/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3842 - auc: 0.682 - ETA: 0s - loss: 0.4129 - auc: 0.663 - 0s 22us/step - loss: 0.4065 - auc: 0.6632 - val_loss: 0.4075 - val_auc: 0.6497\n",
      "\n",
      "Epoch 00725: val_loss improved from 0.40753 to 0.40750, saving model to DeepFM.h5\n",
      "Epoch 726/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4567 - auc: 0.665 - ETA: 0s - loss: 0.4165 - auc: 0.649 - 0s 20us/step - loss: 0.4093 - auc: 0.6442 - val_loss: 0.4075 - val_auc: 0.6490\n",
      "\n",
      "Epoch 00726: val_loss improved from 0.40750 to 0.40747, saving model to DeepFM.h5\n",
      "Epoch 727/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3712 - auc: 0.652 - ETA: 0s - loss: 0.4058 - auc: 0.657 - 0s 20us/step - loss: 0.4079 - auc: 0.6610 - val_loss: 0.4074 - val_auc: 0.6487\n",
      "\n",
      "Epoch 00727: val_loss improved from 0.40747 to 0.40742, saving model to DeepFM.h5\n",
      "Epoch 728/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4070 - auc: 0.606 - ETA: 0s - loss: 0.4077 - auc: 0.634 - 0s 20us/step - loss: 0.4076 - auc: 0.6519 - val_loss: 0.4074 - val_auc: 0.6492\n",
      "\n",
      "Epoch 00728: val_loss improved from 0.40742 to 0.40738, saving model to DeepFM.h5\n",
      "Epoch 729/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4678 - auc: 0.685 - ETA: 0s - loss: 0.4107 - auc: 0.650 - 0s 20us/step - loss: 0.4083 - auc: 0.6498 - val_loss: 0.4073 - val_auc: 0.6489\n",
      "\n",
      "Epoch 00729: val_loss improved from 0.40738 to 0.40734, saving model to DeepFM.h5\n",
      "Epoch 730/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3981 - auc: 0.675 - ETA: 0s - loss: 0.4087 - auc: 0.658 - 0s 21us/step - loss: 0.4065 - auc: 0.6619 - val_loss: 0.4073 - val_auc: 0.6495\n",
      "\n",
      "Epoch 00730: val_loss improved from 0.40734 to 0.40731, saving model to DeepFM.h5\n",
      "Epoch 731/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3688 - auc: 0.621 - ETA: 0s - loss: 0.3978 - auc: 0.662 - 0s 20us/step - loss: 0.4067 - auc: 0.6601 - val_loss: 0.4073 - val_auc: 0.6492\n",
      "\n",
      "Epoch 00731: val_loss improved from 0.40731 to 0.40726, saving model to DeepFM.h5\n",
      "Epoch 732/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3565 - auc: 0.642 - ETA: 0s - loss: 0.4142 - auc: 0.653 - 0s 19us/step - loss: 0.4094 - auc: 0.6488 - val_loss: 0.4072 - val_auc: 0.6494\n",
      "\n",
      "Epoch 00732: val_loss improved from 0.40726 to 0.40722, saving model to DeepFM.h5\n",
      "Epoch 733/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3896 - auc: 0.693 - ETA: 0s - loss: 0.3984 - auc: 0.664 - 0s 20us/step - loss: 0.4056 - auc: 0.6636 - val_loss: 0.4072 - val_auc: 0.6499\n",
      "\n",
      "Epoch 00733: val_loss improved from 0.40722 to 0.40718, saving model to DeepFM.h5\n",
      "Epoch 734/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3961 - auc: 0.681 - ETA: 0s - loss: 0.4084 - auc: 0.666 - 0s 22us/step - loss: 0.4050 - auc: 0.6677 - val_loss: 0.4071 - val_auc: 0.6504\n",
      "\n",
      "Epoch 00734: val_loss improved from 0.40718 to 0.40715, saving model to DeepFM.h5\n",
      "Epoch 735/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4413 - auc: 0.697 - ETA: 0s - loss: 0.4061 - auc: 0.649 - 0s 20us/step - loss: 0.4070 - auc: 0.6560 - val_loss: 0.4071 - val_auc: 0.6504\n",
      "\n",
      "Epoch 00735: val_loss improved from 0.40715 to 0.40710, saving model to DeepFM.h5\n",
      "Epoch 736/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4062 - auc: 0.628 - ETA: 0s - loss: 0.4101 - auc: 0.661 - 0s 20us/step - loss: 0.4059 - auc: 0.6613 - val_loss: 0.4071 - val_auc: 0.6503\n",
      "\n",
      "Epoch 00736: val_loss improved from 0.40710 to 0.40707, saving model to DeepFM.h5\n",
      "Epoch 737/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4293 - auc: 0.729 - ETA: 0s - loss: 0.4115 - auc: 0.664 - 0s 19us/step - loss: 0.4068 - auc: 0.6567 - val_loss: 0.4070 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00737: val_loss improved from 0.40707 to 0.40705, saving model to DeepFM.h5\n",
      "Epoch 738/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3763 - auc: 0.686 - ETA: 0s - loss: 0.4070 - auc: 0.657 - 0s 21us/step - loss: 0.4060 - auc: 0.6592 - val_loss: 0.4070 - val_auc: 0.6509\n",
      "\n",
      "Epoch 00738: val_loss improved from 0.40705 to 0.40700, saving model to DeepFM.h5\n",
      "Epoch 739/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3896 - auc: 0.709 - ETA: 0s - loss: 0.4077 - auc: 0.657 - 0s 20us/step - loss: 0.4073 - auc: 0.6563 - val_loss: 0.4070 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00739: val_loss improved from 0.40700 to 0.40696, saving model to DeepFM.h5\n",
      "Epoch 740/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4149 - auc: 0.712 - ETA: 0s - loss: 0.4146 - auc: 0.654 - 0s 20us/step - loss: 0.4060 - auc: 0.6608 - val_loss: 0.4069 - val_auc: 0.6504\n",
      "\n",
      "Epoch 00740: val_loss improved from 0.40696 to 0.40691, saving model to DeepFM.h5\n",
      "Epoch 741/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3952 - auc: 0.740 - ETA: 0s - loss: 0.4088 - auc: 0.654 - 0s 20us/step - loss: 0.4070 - auc: 0.6550 - val_loss: 0.4069 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00741: val_loss improved from 0.40691 to 0.40687, saving model to DeepFM.h5\n",
      "Epoch 742/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3882 - auc: 0.716 - ETA: 0s - loss: 0.4067 - auc: 0.665 - 0s 20us/step - loss: 0.4070 - auc: 0.6607 - val_loss: 0.4068 - val_auc: 0.6510\n",
      "\n",
      "Epoch 00742: val_loss improved from 0.40687 to 0.40683, saving model to DeepFM.h5\n",
      "Epoch 743/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4161 - auc: 0.582 - ETA: 0s - loss: 0.4048 - auc: 0.653 - 0s 22us/step - loss: 0.4076 - auc: 0.6538 - val_loss: 0.4068 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00743: val_loss improved from 0.40683 to 0.40679, saving model to DeepFM.h5\n",
      "Epoch 744/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4189 - auc: 0.587 - ETA: 0s - loss: 0.4048 - auc: 0.668 - 0s 20us/step - loss: 0.4071 - auc: 0.6588 - val_loss: 0.4067 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00744: val_loss improved from 0.40679 to 0.40674, saving model to DeepFM.h5\n",
      "Epoch 745/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4380 - auc: 0.696 - ETA: 0s - loss: 0.4061 - auc: 0.662 - 0s 20us/step - loss: 0.4084 - auc: 0.6495 - val_loss: 0.4067 - val_auc: 0.6518\n",
      "\n",
      "Epoch 00745: val_loss improved from 0.40674 to 0.40670, saving model to DeepFM.h5\n",
      "Epoch 746/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4108 - auc: 0.655 - ETA: 0s - loss: 0.3976 - auc: 0.677 - 0s 21us/step - loss: 0.4049 - auc: 0.6702 - val_loss: 0.4067 - val_auc: 0.6511\n",
      "\n",
      "Epoch 00746: val_loss improved from 0.40670 to 0.40667, saving model to DeepFM.h5\n",
      "Epoch 747/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4249 - auc: 0.711 - ETA: 0s - loss: 0.4074 - auc: 0.671 - 0s 20us/step - loss: 0.4048 - auc: 0.6693 - val_loss: 0.4066 - val_auc: 0.6526\n",
      "\n",
      "Epoch 00747: val_loss improved from 0.40667 to 0.40662, saving model to DeepFM.h5\n",
      "Epoch 748/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3636 - auc: 0.653 - ETA: 0s - loss: 0.4081 - auc: 0.657 - 0s 20us/step - loss: 0.4068 - auc: 0.6569 - val_loss: 0.4066 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00748: val_loss improved from 0.40662 to 0.40658, saving model to DeepFM.h5\n",
      "Epoch 749/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3951 - auc: 0.646 - ETA: 0s - loss: 0.4057 - auc: 0.665 - 0s 20us/step - loss: 0.4055 - auc: 0.6659 - val_loss: 0.4065 - val_auc: 0.6506\n",
      "\n",
      "Epoch 00749: val_loss improved from 0.40658 to 0.40653, saving model to DeepFM.h5\n",
      "Epoch 750/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3800 - auc: 0.712 - ETA: 0s - loss: 0.4040 - auc: 0.655 - 0s 23us/step - loss: 0.4063 - auc: 0.6622 - val_loss: 0.4065 - val_auc: 0.6529\n",
      "\n",
      "Epoch 00750: val_loss improved from 0.40653 to 0.40648, saving model to DeepFM.h5\n",
      "Epoch 751/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4125 - auc: 0.581 - ETA: 0s - loss: 0.3962 - auc: 0.689 - 0s 19us/step - loss: 0.4045 - auc: 0.6682 - val_loss: 0.4064 - val_auc: 0.6515\n",
      "\n",
      "Epoch 00751: val_loss improved from 0.40648 to 0.40644, saving model to DeepFM.h5\n",
      "Epoch 752/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4880 - auc: 0.700 - ETA: 0s - loss: 0.4109 - auc: 0.663 - 0s 20us/step - loss: 0.4058 - auc: 0.6624 - val_loss: 0.4064 - val_auc: 0.6517\n",
      "\n",
      "Epoch 00752: val_loss improved from 0.40644 to 0.40640, saving model to DeepFM.h5\n",
      "Epoch 753/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3400 - auc: 0.693 - ETA: 0s - loss: 0.4065 - auc: 0.646 - 0s 20us/step - loss: 0.4051 - auc: 0.6607 - val_loss: 0.4064 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00753: val_loss improved from 0.40640 to 0.40637, saving model to DeepFM.h5\n",
      "Epoch 754/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4007 - auc: 0.718 - ETA: 0s - loss: 0.3991 - auc: 0.670 - 0s 21us/step - loss: 0.4047 - auc: 0.6670 - val_loss: 0.4063 - val_auc: 0.6517\n",
      "\n",
      "Epoch 00754: val_loss improved from 0.40637 to 0.40633, saving model to DeepFM.h5\n",
      "Epoch 755/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4727 - auc: 0.709 - ETA: 0s - loss: 0.4109 - auc: 0.673 - 0s 20us/step - loss: 0.4067 - auc: 0.6602 - val_loss: 0.4063 - val_auc: 0.6517\n",
      "\n",
      "Epoch 00755: val_loss improved from 0.40633 to 0.40627, saving model to DeepFM.h5\n",
      "Epoch 756/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3668 - auc: 0.764 - ETA: 0s - loss: 0.4029 - auc: 0.670 - 0s 21us/step - loss: 0.4041 - auc: 0.6698 - val_loss: 0.4062 - val_auc: 0.6518\n",
      "\n",
      "Epoch 00756: val_loss improved from 0.40627 to 0.40624, saving model to DeepFM.h5\n",
      "Epoch 757/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3912 - auc: 0.730 - ETA: 0s - loss: 0.3984 - auc: 0.668 - 0s 20us/step - loss: 0.4057 - auc: 0.6648 - val_loss: 0.4062 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00757: val_loss improved from 0.40624 to 0.40619, saving model to DeepFM.h5\n",
      "Epoch 758/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4773 - auc: 0.598 - ETA: 0s - loss: 0.4055 - auc: 0.664 - 0s 20us/step - loss: 0.4061 - auc: 0.6579 - val_loss: 0.4061 - val_auc: 0.6518\n",
      "\n",
      "Epoch 00758: val_loss improved from 0.40619 to 0.40615, saving model to DeepFM.h5\n",
      "Epoch 759/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3542 - auc: 0.607 - ETA: 0s - loss: 0.4072 - auc: 0.663 - 0s 20us/step - loss: 0.4062 - auc: 0.6611 - val_loss: 0.4061 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00759: val_loss improved from 0.40615 to 0.40609, saving model to DeepFM.h5\n",
      "Epoch 760/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4763 - auc: 0.691 - ETA: 0s - loss: 0.4108 - auc: 0.650 - 0s 20us/step - loss: 0.4050 - auc: 0.6632 - val_loss: 0.4061 - val_auc: 0.6532\n",
      "\n",
      "Epoch 00760: val_loss improved from 0.40609 to 0.40605, saving model to DeepFM.h5\n",
      "Epoch 761/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3879 - auc: 0.712 - ETA: 0s - loss: 0.4035 - auc: 0.675 - 0s 21us/step - loss: 0.4052 - auc: 0.6643 - val_loss: 0.4060 - val_auc: 0.6528\n",
      "\n",
      "Epoch 00761: val_loss improved from 0.40605 to 0.40600, saving model to DeepFM.h5\n",
      "Epoch 762/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4120 - auc: 0.759 - ETA: 0s - loss: 0.4172 - auc: 0.659 - 0s 20us/step - loss: 0.4066 - auc: 0.6551 - val_loss: 0.4060 - val_auc: 0.6538\n",
      "\n",
      "Epoch 00762: val_loss improved from 0.40600 to 0.40595, saving model to DeepFM.h5\n",
      "Epoch 763/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4220 - auc: 0.662 - ETA: 0s - loss: 0.4031 - auc: 0.672 - 0s 22us/step - loss: 0.4053 - auc: 0.6663 - val_loss: 0.4059 - val_auc: 0.6534\n",
      "\n",
      "Epoch 00763: val_loss improved from 0.40595 to 0.40590, saving model to DeepFM.h5\n",
      "Epoch 764/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4279 - auc: 0.578 - ETA: 0s - loss: 0.3976 - auc: 0.680 - 0s 22us/step - loss: 0.4040 - auc: 0.6721 - val_loss: 0.4058 - val_auc: 0.6535\n",
      "\n",
      "Epoch 00764: val_loss improved from 0.40590 to 0.40585, saving model to DeepFM.h5\n",
      "Epoch 765/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3678 - auc: 0.714 - ETA: 0s - loss: 0.4113 - auc: 0.671 - 0s 20us/step - loss: 0.4053 - auc: 0.6654 - val_loss: 0.4058 - val_auc: 0.6523\n",
      "\n",
      "Epoch 00765: val_loss improved from 0.40585 to 0.40582, saving model to DeepFM.h5\n",
      "Epoch 766/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3972 - auc: 0.717 - ETA: 0s - loss: 0.4126 - auc: 0.667 - 0s 21us/step - loss: 0.4051 - auc: 0.6649 - val_loss: 0.4058 - val_auc: 0.6536\n",
      "\n",
      "Epoch 00766: val_loss improved from 0.40582 to 0.40576, saving model to DeepFM.h5\n",
      "Epoch 767/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3975 - auc: 0.693 - ETA: 0s - loss: 0.4055 - auc: 0.663 - 0s 22us/step - loss: 0.4037 - auc: 0.6724 - val_loss: 0.4057 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00767: val_loss improved from 0.40576 to 0.40573, saving model to DeepFM.h5\n",
      "Epoch 768/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3940 - auc: 0.695 - ETA: 0s - loss: 0.4178 - auc: 0.652 - 0s 23us/step - loss: 0.4058 - auc: 0.6616 - val_loss: 0.4057 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00768: val_loss improved from 0.40573 to 0.40568, saving model to DeepFM.h5\n",
      "Epoch 769/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4247 - auc: 0.587 - ETA: 0s - loss: 0.4080 - auc: 0.661 - 0s 21us/step - loss: 0.4045 - auc: 0.6682 - val_loss: 0.4057 - val_auc: 0.6505\n",
      "\n",
      "Epoch 00769: val_loss improved from 0.40568 to 0.40565, saving model to DeepFM.h5\n",
      "Epoch 770/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4289 - auc: 0.653 - ETA: 0s - loss: 0.3993 - auc: 0.660 - 0s 20us/step - loss: 0.4069 - auc: 0.6538 - val_loss: 0.4056 - val_auc: 0.6503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00770: val_loss improved from 0.40565 to 0.40560, saving model to DeepFM.h5\n",
      "Epoch 771/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4115 - auc: 0.721 - ETA: 0s - loss: 0.3944 - auc: 0.674 - 0s 20us/step - loss: 0.4025 - auc: 0.6707 - val_loss: 0.4055 - val_auc: 0.6513\n",
      "\n",
      "Epoch 00771: val_loss improved from 0.40560 to 0.40555, saving model to DeepFM.h5\n",
      "Epoch 772/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4176 - auc: 0.715 - ETA: 0s - loss: 0.4121 - auc: 0.668 - 0s 20us/step - loss: 0.4052 - auc: 0.6642 - val_loss: 0.4055 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00772: val_loss improved from 0.40555 to 0.40549, saving model to DeepFM.h5\n",
      "Epoch 773/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3617 - auc: 0.635 - ETA: 0s - loss: 0.3888 - auc: 0.659 - 0s 26us/step - loss: 0.4051 - auc: 0.6657 - val_loss: 0.4054 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00773: val_loss improved from 0.40549 to 0.40544, saving model to DeepFM.h5\n",
      "Epoch 774/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3913 - auc: 0.682 - ETA: 0s - loss: 0.4116 - auc: 0.662 - 0s 19us/step - loss: 0.4051 - auc: 0.6651 - val_loss: 0.4054 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00774: val_loss improved from 0.40544 to 0.40540, saving model to DeepFM.h5\n",
      "Epoch 775/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3779 - auc: 0.677 - ETA: 0s - loss: 0.4014 - auc: 0.681 - 0s 21us/step - loss: 0.4047 - auc: 0.6696 - val_loss: 0.4054 - val_auc: 0.6516\n",
      "\n",
      "Epoch 00775: val_loss improved from 0.40540 to 0.40536, saving model to DeepFM.h5\n",
      "Epoch 776/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3906 - auc: 0.709 - ETA: 0s - loss: 0.4076 - auc: 0.667 - 0s 20us/step - loss: 0.4044 - auc: 0.6694 - val_loss: 0.4053 - val_auc: 0.6525\n",
      "\n",
      "Epoch 00776: val_loss improved from 0.40536 to 0.40531, saving model to DeepFM.h5\n",
      "Epoch 777/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4116 - auc: 0.606 - ETA: 0s - loss: 0.3982 - auc: 0.688 - 0s 23us/step - loss: 0.4024 - auc: 0.6767 - val_loss: 0.4053 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00777: val_loss improved from 0.40531 to 0.40527, saving model to DeepFM.h5\n",
      "Epoch 778/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3893 - auc: 0.552 - ETA: 0s - loss: 0.3938 - auc: 0.679 - 0s 22us/step - loss: 0.4032 - auc: 0.6741 - val_loss: 0.4052 - val_auc: 0.6522\n",
      "\n",
      "Epoch 00778: val_loss improved from 0.40527 to 0.40522, saving model to DeepFM.h5\n",
      "Epoch 779/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.660 - ETA: 0s - loss: 0.4050 - auc: 0.657 - 0s 20us/step - loss: 0.4057 - auc: 0.6598 - val_loss: 0.4052 - val_auc: 0.6514\n",
      "\n",
      "Epoch 00779: val_loss improved from 0.40522 to 0.40516, saving model to DeepFM.h5\n",
      "Epoch 780/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3525 - auc: 0.623 - ETA: 0s - loss: 0.4062 - auc: 0.659 - 0s 20us/step - loss: 0.4040 - auc: 0.6644 - val_loss: 0.4051 - val_auc: 0.6519\n",
      "\n",
      "Epoch 00780: val_loss improved from 0.40516 to 0.40511, saving model to DeepFM.h5\n",
      "Epoch 781/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3823 - auc: 0.760 - ETA: 0s - loss: 0.4131 - auc: 0.674 - 0s 22us/step - loss: 0.4041 - auc: 0.6673 - val_loss: 0.4051 - val_auc: 0.6524\n",
      "\n",
      "Epoch 00781: val_loss improved from 0.40511 to 0.40506, saving model to DeepFM.h5\n",
      "Epoch 782/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4076 - auc: 0.718 - ETA: 0s - loss: 0.4041 - auc: 0.668 - 0s 21us/step - loss: 0.4034 - auc: 0.6788 - val_loss: 0.4050 - val_auc: 0.6521\n",
      "\n",
      "Epoch 00782: val_loss improved from 0.40506 to 0.40501, saving model to DeepFM.h5\n",
      "Epoch 783/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4654 - auc: 0.674 - ETA: 0s - loss: 0.4050 - auc: 0.677 - 0s 19us/step - loss: 0.4019 - auc: 0.6823 - val_loss: 0.4050 - val_auc: 0.6523\n",
      "\n",
      "Epoch 00783: val_loss improved from 0.40501 to 0.40496, saving model to DeepFM.h5\n",
      "Epoch 784/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3988 - auc: 0.682 - ETA: 0s - loss: 0.3982 - auc: 0.677 - 0s 22us/step - loss: 0.4035 - auc: 0.6753 - val_loss: 0.4049 - val_auc: 0.6530\n",
      "\n",
      "Epoch 00784: val_loss improved from 0.40496 to 0.40491, saving model to DeepFM.h5\n",
      "Epoch 785/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3548 - auc: 0.732 - ETA: 0s - loss: 0.4022 - auc: 0.679 - 0s 20us/step - loss: 0.4032 - auc: 0.6774 - val_loss: 0.4049 - val_auc: 0.6544\n",
      "\n",
      "Epoch 00785: val_loss improved from 0.40491 to 0.40487, saving model to DeepFM.h5\n",
      "Epoch 786/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4772 - auc: 0.639 - ETA: 0s - loss: 0.4095 - auc: 0.656 - 0s 21us/step - loss: 0.4064 - auc: 0.6570 - val_loss: 0.4048 - val_auc: 0.6543\n",
      "\n",
      "Epoch 00786: val_loss improved from 0.40487 to 0.40483, saving model to DeepFM.h5\n",
      "Epoch 787/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3733 - auc: 0.746 - ETA: 0s - loss: 0.4082 - auc: 0.665 - 0s 21us/step - loss: 0.4071 - auc: 0.6609 - val_loss: 0.4048 - val_auc: 0.6537\n",
      "\n",
      "Epoch 00787: val_loss improved from 0.40483 to 0.40478, saving model to DeepFM.h5\n",
      "Epoch 788/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4002 - auc: 0.693 - ETA: 0s - loss: 0.4020 - auc: 0.672 - 0s 20us/step - loss: 0.4024 - auc: 0.6778 - val_loss: 0.4047 - val_auc: 0.6543\n",
      "\n",
      "Epoch 00788: val_loss improved from 0.40478 to 0.40473, saving model to DeepFM.h5\n",
      "Epoch 789/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3854 - auc: 0.681 - ETA: 0s - loss: 0.3997 - auc: 0.657 - 0s 20us/step - loss: 0.4047 - auc: 0.6599 - val_loss: 0.4047 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00789: val_loss improved from 0.40473 to 0.40469, saving model to DeepFM.h5\n",
      "Epoch 790/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3940 - auc: 0.660 - ETA: 0s - loss: 0.4055 - auc: 0.663 - 0s 21us/step - loss: 0.4038 - auc: 0.6668 - val_loss: 0.4047 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00790: val_loss improved from 0.40469 to 0.40466, saving model to DeepFM.h5\n",
      "Epoch 791/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4090 - auc: 0.691 - ETA: 0s - loss: 0.3978 - auc: 0.664 - 0s 20us/step - loss: 0.4047 - auc: 0.6609 - val_loss: 0.4046 - val_auc: 0.6542\n",
      "\n",
      "Epoch 00791: val_loss improved from 0.40466 to 0.40461, saving model to DeepFM.h5\n",
      "Epoch 792/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4846 - auc: 0.684 - ETA: 0s - loss: 0.4119 - auc: 0.664 - 0s 20us/step - loss: 0.4055 - auc: 0.6591 - val_loss: 0.4045 - val_auc: 0.6550\n",
      "\n",
      "Epoch 00792: val_loss improved from 0.40461 to 0.40455, saving model to DeepFM.h5\n",
      "Epoch 793/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3858 - auc: 0.702 - ETA: 0s - loss: 0.4092 - auc: 0.679 - 0s 20us/step - loss: 0.4033 - auc: 0.6752 - val_loss: 0.4045 - val_auc: 0.6545\n",
      "\n",
      "Epoch 00793: val_loss improved from 0.40455 to 0.40449, saving model to DeepFM.h5\n",
      "Epoch 794/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3193 - auc: 0.759 - ETA: 0s - loss: 0.4006 - auc: 0.679 - 0s 21us/step - loss: 0.4035 - auc: 0.6737 - val_loss: 0.4045 - val_auc: 0.6549\n",
      "\n",
      "Epoch 00794: val_loss improved from 0.40449 to 0.40445, saving model to DeepFM.h5\n",
      "Epoch 795/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3727 - auc: 0.690 - ETA: 0s - loss: 0.4010 - auc: 0.661 - 0s 20us/step - loss: 0.4036 - auc: 0.6733 - val_loss: 0.4044 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00795: val_loss improved from 0.40445 to 0.40442, saving model to DeepFM.h5\n",
      "Epoch 796/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3784 - auc: 0.680 - ETA: 0s - loss: 0.3997 - auc: 0.658 - 0s 20us/step - loss: 0.4046 - auc: 0.6615 - val_loss: 0.4044 - val_auc: 0.6550\n",
      "\n",
      "Epoch 00796: val_loss improved from 0.40442 to 0.40438, saving model to DeepFM.h5\n",
      "Epoch 797/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4616 - auc: 0.679 - ETA: 0s - loss: 0.4142 - auc: 0.673 - 0s 22us/step - loss: 0.4017 - auc: 0.6715 - val_loss: 0.4044 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00797: val_loss improved from 0.40438 to 0.40435, saving model to DeepFM.h5\n",
      "Epoch 798/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4292 - auc: 0.734 - ETA: 0s - loss: 0.4039 - auc: 0.675 - 0s 23us/step - loss: 0.4030 - auc: 0.6766 - val_loss: 0.4043 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00798: val_loss improved from 0.40435 to 0.40431, saving model to DeepFM.h5\n",
      "Epoch 799/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3988 - auc: 0.703 - ETA: 0s - loss: 0.3974 - auc: 0.674 - 0s 20us/step - loss: 0.4028 - auc: 0.6712 - val_loss: 0.4042 - val_auc: 0.6556\n",
      "\n",
      "Epoch 00799: val_loss improved from 0.40431 to 0.40425, saving model to DeepFM.h5\n",
      "Epoch 800/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.662 - ETA: 0s - loss: 0.4026 - auc: 0.672 - 0s 21us/step - loss: 0.4021 - auc: 0.6791 - val_loss: 0.4042 - val_auc: 0.6559\n",
      "\n",
      "Epoch 00800: val_loss improved from 0.40425 to 0.40420, saving model to DeepFM.h5\n",
      "Epoch 801/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4057 - auc: 0.712 - ETA: 0s - loss: 0.4064 - auc: 0.674 - 0s 20us/step - loss: 0.4033 - auc: 0.6708 - val_loss: 0.4041 - val_auc: 0.6568\n",
      "\n",
      "Epoch 00801: val_loss improved from 0.40420 to 0.40413, saving model to DeepFM.h5\n",
      "Epoch 802/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4598 - auc: 0.620 - ETA: 0s - loss: 0.4050 - auc: 0.660 - 0s 20us/step - loss: 0.4050 - auc: 0.6610 - val_loss: 0.4041 - val_auc: 0.6572\n",
      "\n",
      "Epoch 00802: val_loss improved from 0.40413 to 0.40408, saving model to DeepFM.h5\n",
      "Epoch 803/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3988 - auc: 0.707 - ETA: 0s - loss: 0.3961 - auc: 0.672 - 0s 22us/step - loss: 0.4017 - auc: 0.6816 - val_loss: 0.4040 - val_auc: 0.6577\n",
      "\n",
      "Epoch 00803: val_loss improved from 0.40408 to 0.40403, saving model to DeepFM.h5\n",
      "Epoch 804/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4130 - auc: 0.603 - ETA: 0s - loss: 0.4122 - auc: 0.658 - 0s 20us/step - loss: 0.4046 - auc: 0.6645 - val_loss: 0.4040 - val_auc: 0.6576\n",
      "\n",
      "Epoch 00804: val_loss improved from 0.40403 to 0.40400, saving model to DeepFM.h5\n",
      "Epoch 805/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4176 - auc: 0.705 - ETA: 0s - loss: 0.4094 - auc: 0.674 - 0s 20us/step - loss: 0.4031 - auc: 0.6759 - val_loss: 0.4040 - val_auc: 0.6571\n",
      "\n",
      "Epoch 00805: val_loss improved from 0.40400 to 0.40395, saving model to DeepFM.h5\n",
      "Epoch 806/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3768 - auc: 0.679 - ETA: 0s - loss: 0.4082 - auc: 0.659 - 0s 21us/step - loss: 0.4036 - auc: 0.6671 - val_loss: 0.4039 - val_auc: 0.6578\n",
      "\n",
      "Epoch 00806: val_loss improved from 0.40395 to 0.40389, saving model to DeepFM.h5\n",
      "Epoch 807/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3701 - auc: 0.601 - ETA: 0s - loss: 0.3971 - auc: 0.695 - 0s 23us/step - loss: 0.4011 - auc: 0.6856 - val_loss: 0.4038 - val_auc: 0.6581\n",
      "\n",
      "Epoch 00807: val_loss improved from 0.40389 to 0.40384, saving model to DeepFM.h5\n",
      "Epoch 808/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4384 - auc: 0.766 - ETA: 0s - loss: 0.4083 - auc: 0.683 - 0s 22us/step - loss: 0.4023 - auc: 0.6761 - val_loss: 0.4038 - val_auc: 0.6578\n",
      "\n",
      "Epoch 00808: val_loss improved from 0.40384 to 0.40381, saving model to DeepFM.h5\n",
      "Epoch 809/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3345 - auc: 0.633 - ETA: 0s - loss: 0.4039 - auc: 0.679 - 0s 20us/step - loss: 0.4038 - auc: 0.6676 - val_loss: 0.4037 - val_auc: 0.6589\n",
      "\n",
      "Epoch 00809: val_loss improved from 0.40381 to 0.40374, saving model to DeepFM.h5\n",
      "Epoch 810/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4577 - auc: 0.672 - ETA: 0s - loss: 0.4000 - auc: 0.665 - 0s 19us/step - loss: 0.4030 - auc: 0.6730 - val_loss: 0.4037 - val_auc: 0.6594\n",
      "\n",
      "Epoch 00810: val_loss improved from 0.40374 to 0.40369, saving model to DeepFM.h5\n",
      "Epoch 811/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3962 - auc: 0.589 - ETA: 0s - loss: 0.4052 - auc: 0.662 - 0s 21us/step - loss: 0.4031 - auc: 0.6704 - val_loss: 0.4036 - val_auc: 0.6588\n",
      "\n",
      "Epoch 00811: val_loss improved from 0.40369 to 0.40364, saving model to DeepFM.h5\n",
      "Epoch 812/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4058 - auc: 0.657 - ETA: 0s - loss: 0.4030 - auc: 0.682 - 0s 20us/step - loss: 0.3997 - auc: 0.6861 - val_loss: 0.4036 - val_auc: 0.6596\n",
      "\n",
      "Epoch 00812: val_loss improved from 0.40364 to 0.40360, saving model to DeepFM.h5\n",
      "Epoch 813/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3604 - auc: 0.711 - ETA: 0s - loss: 0.4034 - auc: 0.671 - 0s 21us/step - loss: 0.4025 - auc: 0.6717 - val_loss: 0.4035 - val_auc: 0.6595\n",
      "\n",
      "Epoch 00813: val_loss improved from 0.40360 to 0.40353, saving model to DeepFM.h5\n",
      "Epoch 814/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3555 - auc: 0.807 - ETA: 0s - loss: 0.3976 - auc: 0.689 - 0s 20us/step - loss: 0.4018 - auc: 0.6730 - val_loss: 0.4035 - val_auc: 0.6601\n",
      "\n",
      "Epoch 00814: val_loss improved from 0.40353 to 0.40348, saving model to DeepFM.h5\n",
      "Epoch 815/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4034 - auc: 0.679 - ETA: 0s - loss: 0.4035 - auc: 0.683 - 0s 20us/step - loss: 0.4008 - auc: 0.6805 - val_loss: 0.4034 - val_auc: 0.6601\n",
      "\n",
      "Epoch 00815: val_loss improved from 0.40348 to 0.40343, saving model to DeepFM.h5\n",
      "Epoch 816/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3986 - auc: 0.677 - ETA: 0s - loss: 0.4050 - auc: 0.676 - 0s 21us/step - loss: 0.4004 - auc: 0.6860 - val_loss: 0.4034 - val_auc: 0.6607\n",
      "\n",
      "Epoch 00816: val_loss improved from 0.40343 to 0.40339, saving model to DeepFM.h5\n",
      "Epoch 817/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4513 - auc: 0.675 - ETA: 0s - loss: 0.4187 - auc: 0.679 - 0s 21us/step - loss: 0.4026 - auc: 0.6754 - val_loss: 0.4034 - val_auc: 0.6607\n",
      "\n",
      "Epoch 00817: val_loss improved from 0.40339 to 0.40337, saving model to DeepFM.h5\n",
      "Epoch 818/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4075 - auc: 0.578 - ETA: 0s - loss: 0.4002 - auc: 0.659 - 0s 21us/step - loss: 0.4027 - auc: 0.6756 - val_loss: 0.4033 - val_auc: 0.6610\n",
      "\n",
      "Epoch 00818: val_loss improved from 0.40337 to 0.40330, saving model to DeepFM.h5\n",
      "Epoch 819/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4730 - auc: 0.702 - ETA: 0s - loss: 0.4070 - auc: 0.675 - 0s 20us/step - loss: 0.4026 - auc: 0.6731 - val_loss: 0.4033 - val_auc: 0.6616\n",
      "\n",
      "Epoch 00819: val_loss improved from 0.40330 to 0.40326, saving model to DeepFM.h5\n",
      "Epoch 820/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5113 - auc: 0.662 - ETA: 0s - loss: 0.3968 - auc: 0.695 - 0s 22us/step - loss: 0.4025 - auc: 0.6753 - val_loss: 0.4032 - val_auc: 0.6611\n",
      "\n",
      "Epoch 00820: val_loss improved from 0.40326 to 0.40322, saving model to DeepFM.h5\n",
      "Epoch 821/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3508 - auc: 0.750 - ETA: 0s - loss: 0.3920 - auc: 0.678 - 0s 23us/step - loss: 0.4027 - auc: 0.6701 - val_loss: 0.4032 - val_auc: 0.6616\n",
      "\n",
      "Epoch 00821: val_loss improved from 0.40322 to 0.40317, saving model to DeepFM.h5\n",
      "Epoch 822/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3922 - auc: 0.671 - ETA: 0s - loss: 0.4075 - auc: 0.673 - 0s 21us/step - loss: 0.4011 - auc: 0.6772 - val_loss: 0.4031 - val_auc: 0.6628\n",
      "\n",
      "Epoch 00822: val_loss improved from 0.40317 to 0.40311, saving model to DeepFM.h5\n",
      "Epoch 823/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3933 - auc: 0.688 - ETA: 0s - loss: 0.4078 - auc: 0.674 - 0s 19us/step - loss: 0.4011 - auc: 0.6799 - val_loss: 0.4031 - val_auc: 0.6632\n",
      "\n",
      "Epoch 00823: val_loss improved from 0.40311 to 0.40306, saving model to DeepFM.h5\n",
      "Epoch 824/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3847 - auc: 0.689 - ETA: 0s - loss: 0.4029 - auc: 0.690 - 0s 22us/step - loss: 0.4009 - auc: 0.6823 - val_loss: 0.4030 - val_auc: 0.6624\n",
      "\n",
      "Epoch 00824: val_loss improved from 0.40306 to 0.40302, saving model to DeepFM.h5\n",
      "Epoch 825/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4306 - auc: 0.689 - ETA: 0s - loss: 0.3875 - auc: 0.704 - 0s 21us/step - loss: 0.3967 - auc: 0.7036 - val_loss: 0.4030 - val_auc: 0.6619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00825: val_loss improved from 0.40302 to 0.40298, saving model to DeepFM.h5\n",
      "Epoch 826/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3824 - auc: 0.618 - ETA: 0s - loss: 0.3996 - auc: 0.698 - 0s 20us/step - loss: 0.4015 - auc: 0.6773 - val_loss: 0.4029 - val_auc: 0.6628\n",
      "\n",
      "Epoch 00826: val_loss improved from 0.40298 to 0.40291, saving model to DeepFM.h5\n",
      "Epoch 827/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4022 - auc: 0.681 - ETA: 0s - loss: 0.3929 - auc: 0.669 - 0s 21us/step - loss: 0.4026 - auc: 0.6737 - val_loss: 0.4028 - val_auc: 0.6629\n",
      "\n",
      "Epoch 00827: val_loss improved from 0.40291 to 0.40284, saving model to DeepFM.h5\n",
      "Epoch 828/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4453 - auc: 0.746 - ETA: 0s - loss: 0.4016 - auc: 0.680 - 0s 20us/step - loss: 0.4006 - auc: 0.6871 - val_loss: 0.4028 - val_auc: 0.6635\n",
      "\n",
      "Epoch 00828: val_loss improved from 0.40284 to 0.40279, saving model to DeepFM.h5\n",
      "Epoch 829/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4244 - auc: 0.675 - ETA: 0s - loss: 0.3905 - auc: 0.682 - 0s 21us/step - loss: 0.4022 - auc: 0.6732 - val_loss: 0.4027 - val_auc: 0.6638\n",
      "\n",
      "Epoch 00829: val_loss improved from 0.40279 to 0.40274, saving model to DeepFM.h5\n",
      "Epoch 830/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3326 - auc: 0.666 - ETA: 0s - loss: 0.3979 - auc: 0.686 - 0s 21us/step - loss: 0.4002 - auc: 0.6818 - val_loss: 0.4027 - val_auc: 0.6629\n",
      "\n",
      "Epoch 00830: val_loss improved from 0.40274 to 0.40269, saving model to DeepFM.h5\n",
      "Epoch 831/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3389 - auc: 0.703 - ETA: 0s - loss: 0.3910 - auc: 0.698 - 0s 20us/step - loss: 0.4002 - auc: 0.6831 - val_loss: 0.4026 - val_auc: 0.6642\n",
      "\n",
      "Epoch 00831: val_loss improved from 0.40269 to 0.40262, saving model to DeepFM.h5\n",
      "Epoch 832/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3758 - auc: 0.679 - ETA: 0s - loss: 0.3928 - auc: 0.676 - 0s 22us/step - loss: 0.4016 - auc: 0.6744 - val_loss: 0.4026 - val_auc: 0.6624\n",
      "\n",
      "Epoch 00832: val_loss improved from 0.40262 to 0.40256, saving model to DeepFM.h5\n",
      "Epoch 833/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4618 - auc: 0.683 - ETA: 0s - loss: 0.4036 - auc: 0.682 - 0s 20us/step - loss: 0.4019 - auc: 0.6773 - val_loss: 0.4025 - val_auc: 0.6628\n",
      "\n",
      "Epoch 00833: val_loss improved from 0.40256 to 0.40251, saving model to DeepFM.h5\n",
      "Epoch 834/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4271 - auc: 0.649 - ETA: 0s - loss: 0.4033 - auc: 0.684 - 0s 22us/step - loss: 0.4029 - auc: 0.6700 - val_loss: 0.4025 - val_auc: 0.6639\n",
      "\n",
      "Epoch 00834: val_loss improved from 0.40251 to 0.40248, saving model to DeepFM.h5\n",
      "Epoch 835/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4586 - auc: 0.623 - ETA: 0s - loss: 0.3952 - auc: 0.682 - 0s 20us/step - loss: 0.4007 - auc: 0.6831 - val_loss: 0.4024 - val_auc: 0.6645\n",
      "\n",
      "Epoch 00835: val_loss improved from 0.40248 to 0.40244, saving model to DeepFM.h5\n",
      "Epoch 836/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3843 - auc: 0.721 - ETA: 0s - loss: 0.4006 - auc: 0.676 - 0s 21us/step - loss: 0.4027 - auc: 0.6726 - val_loss: 0.4024 - val_auc: 0.6647\n",
      "\n",
      "Epoch 00836: val_loss improved from 0.40244 to 0.40239, saving model to DeepFM.h5\n",
      "Epoch 837/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4459 - auc: 0.619 - ETA: 0s - loss: 0.4039 - auc: 0.696 - 0s 22us/step - loss: 0.3989 - auc: 0.6903 - val_loss: 0.4023 - val_auc: 0.6642\n",
      "\n",
      "Epoch 00837: val_loss improved from 0.40239 to 0.40233, saving model to DeepFM.h5\n",
      "Epoch 838/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3921 - auc: 0.751 - ETA: 0s - loss: 0.3969 - auc: 0.678 - 0s 20us/step - loss: 0.4006 - auc: 0.6784 - val_loss: 0.4023 - val_auc: 0.6641\n",
      "\n",
      "Epoch 00838: val_loss improved from 0.40233 to 0.40229, saving model to DeepFM.h5\n",
      "Epoch 839/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4074 - auc: 0.656 - ETA: 0s - loss: 0.4006 - auc: 0.680 - 0s 20us/step - loss: 0.4006 - auc: 0.6849 - val_loss: 0.4022 - val_auc: 0.6652\n",
      "\n",
      "Epoch 00839: val_loss improved from 0.40229 to 0.40223, saving model to DeepFM.h5\n",
      "Epoch 840/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3730 - auc: 0.647 - ETA: 0s - loss: 0.3967 - auc: 0.682 - 0s 20us/step - loss: 0.3999 - auc: 0.6804 - val_loss: 0.4022 - val_auc: 0.6652\n",
      "\n",
      "Epoch 00840: val_loss improved from 0.40223 to 0.40220, saving model to DeepFM.h5\n",
      "Epoch 841/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3664 - auc: 0.717 - ETA: 0s - loss: 0.3959 - auc: 0.692 - 0s 20us/step - loss: 0.3992 - auc: 0.6836 - val_loss: 0.4021 - val_auc: 0.6656\n",
      "\n",
      "Epoch 00841: val_loss improved from 0.40220 to 0.40214, saving model to DeepFM.h5\n",
      "Epoch 842/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4630 - auc: 0.675 - ETA: 0s - loss: 0.4073 - auc: 0.687 - 0s 21us/step - loss: 0.3997 - auc: 0.6852 - val_loss: 0.4021 - val_auc: 0.6639\n",
      "\n",
      "Epoch 00842: val_loss improved from 0.40214 to 0.40206, saving model to DeepFM.h5\n",
      "Epoch 843/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3815 - auc: 0.658 - ETA: 0s - loss: 0.4002 - auc: 0.699 - 0s 20us/step - loss: 0.3991 - auc: 0.6870 - val_loss: 0.4020 - val_auc: 0.6645\n",
      "\n",
      "Epoch 00843: val_loss improved from 0.40206 to 0.40201, saving model to DeepFM.h5\n",
      "Epoch 844/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3488 - auc: 0.708 - ETA: 0s - loss: 0.3913 - auc: 0.679 - 0s 22us/step - loss: 0.4006 - auc: 0.6763 - val_loss: 0.4020 - val_auc: 0.6652\n",
      "\n",
      "Epoch 00844: val_loss improved from 0.40201 to 0.40197, saving model to DeepFM.h5\n",
      "Epoch 845/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3786 - auc: 0.695 - ETA: 0s - loss: 0.3992 - auc: 0.683 - 0s 20us/step - loss: 0.4000 - auc: 0.6810 - val_loss: 0.4019 - val_auc: 0.6645\n",
      "\n",
      "Epoch 00845: val_loss improved from 0.40197 to 0.40191, saving model to DeepFM.h5\n",
      "Epoch 846/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3402 - auc: 0.786 - ETA: 0s - loss: 0.3956 - auc: 0.688 - 0s 21us/step - loss: 0.4007 - auc: 0.6829 - val_loss: 0.4019 - val_auc: 0.6654\n",
      "\n",
      "Epoch 00846: val_loss improved from 0.40191 to 0.40187, saving model to DeepFM.h5\n",
      "Epoch 847/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4177 - auc: 0.704 - ETA: 0s - loss: 0.3902 - auc: 0.683 - 0s 20us/step - loss: 0.4001 - auc: 0.6827 - val_loss: 0.4018 - val_auc: 0.6641\n",
      "\n",
      "Epoch 00847: val_loss improved from 0.40187 to 0.40179, saving model to DeepFM.h5\n",
      "Epoch 848/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4027 - auc: 0.711 - ETA: 0s - loss: 0.4063 - auc: 0.692 - 0s 19us/step - loss: 0.3981 - auc: 0.6918 - val_loss: 0.4017 - val_auc: 0.6640\n",
      "\n",
      "Epoch 00848: val_loss improved from 0.40179 to 0.40174, saving model to DeepFM.h5\n",
      "Epoch 849/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3813 - auc: 0.680 - ETA: 0s - loss: 0.3992 - auc: 0.688 - 0s 20us/step - loss: 0.3988 - auc: 0.6866 - val_loss: 0.4017 - val_auc: 0.6649\n",
      "\n",
      "Epoch 00849: val_loss improved from 0.40174 to 0.40168, saving model to DeepFM.h5\n",
      "Epoch 850/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4669 - auc: 0.737 - ETA: 0s - loss: 0.4062 - auc: 0.693 - 0s 21us/step - loss: 0.3990 - auc: 0.6899 - val_loss: 0.4016 - val_auc: 0.6649\n",
      "\n",
      "Epoch 00850: val_loss improved from 0.40168 to 0.40162, saving model to DeepFM.h5\n",
      "Epoch 851/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4157 - auc: 0.669 - ETA: 0s - loss: 0.3971 - auc: 0.698 - 0s 22us/step - loss: 0.4002 - auc: 0.6845 - val_loss: 0.4016 - val_auc: 0.6652\n",
      "\n",
      "Epoch 00851: val_loss improved from 0.40162 to 0.40157, saving model to DeepFM.h5\n",
      "Epoch 852/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3939 - auc: 0.693 - ETA: 0s - loss: 0.4027 - auc: 0.662 - 0s 22us/step - loss: 0.4010 - auc: 0.6807 - val_loss: 0.4015 - val_auc: 0.6661\n",
      "\n",
      "Epoch 00852: val_loss improved from 0.40157 to 0.40152, saving model to DeepFM.h5\n",
      "Epoch 853/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3617 - auc: 0.776 - ETA: 0s - loss: 0.3922 - auc: 0.690 - 0s 19us/step - loss: 0.3990 - auc: 0.6880 - val_loss: 0.4015 - val_auc: 0.6658\n",
      "\n",
      "Epoch 00853: val_loss improved from 0.40152 to 0.40146, saving model to DeepFM.h5\n",
      "Epoch 854/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3407 - auc: 0.646 - ETA: 0s - loss: 0.3972 - auc: 0.690 - 0s 20us/step - loss: 0.3987 - auc: 0.6891 - val_loss: 0.4014 - val_auc: 0.6666\n",
      "\n",
      "Epoch 00854: val_loss improved from 0.40146 to 0.40141, saving model to DeepFM.h5\n",
      "Epoch 855/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3105 - auc: 0.721 - ETA: 0s - loss: 0.3954 - auc: 0.679 - 0s 22us/step - loss: 0.3995 - auc: 0.6828 - val_loss: 0.4013 - val_auc: 0.6669\n",
      "\n",
      "Epoch 00855: val_loss improved from 0.40141 to 0.40135, saving model to DeepFM.h5\n",
      "Epoch 856/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3888 - auc: 0.655 - ETA: 0s - loss: 0.3950 - auc: 0.690 - 0s 21us/step - loss: 0.3976 - auc: 0.6926 - val_loss: 0.4013 - val_auc: 0.6671\n",
      "\n",
      "Epoch 00856: val_loss improved from 0.40135 to 0.40129, saving model to DeepFM.h5\n",
      "Epoch 857/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3798 - auc: 0.694 - ETA: 0s - loss: 0.4020 - auc: 0.681 - 0s 20us/step - loss: 0.3993 - auc: 0.6895 - val_loss: 0.4013 - val_auc: 0.6673\n",
      "\n",
      "Epoch 00857: val_loss improved from 0.40129 to 0.40126, saving model to DeepFM.h5\n",
      "Epoch 858/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4085 - auc: 0.783 - ETA: 0s - loss: 0.3885 - auc: 0.711 - 0s 23us/step - loss: 0.3986 - auc: 0.6886 - val_loss: 0.4012 - val_auc: 0.6667\n",
      "\n",
      "Epoch 00858: val_loss improved from 0.40126 to 0.40122, saving model to DeepFM.h5\n",
      "Epoch 859/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3404 - auc: 0.609 - ETA: 0s - loss: 0.4048 - auc: 0.704 - 0s 22us/step - loss: 0.3987 - auc: 0.6902 - val_loss: 0.4012 - val_auc: 0.6672\n",
      "\n",
      "Epoch 00859: val_loss improved from 0.40122 to 0.40117, saving model to DeepFM.h5\n",
      "Epoch 860/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3490 - auc: 0.722 - ETA: 0s - loss: 0.3908 - auc: 0.692 - 0s 20us/step - loss: 0.3979 - auc: 0.6967 - val_loss: 0.4011 - val_auc: 0.6669\n",
      "\n",
      "Epoch 00860: val_loss improved from 0.40117 to 0.40110, saving model to DeepFM.h5\n",
      "Epoch 861/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4453 - auc: 0.665 - ETA: 0s - loss: 0.3905 - auc: 0.697 - 0s 20us/step - loss: 0.3963 - auc: 0.7010 - val_loss: 0.4011 - val_auc: 0.6673\n",
      "\n",
      "Epoch 00861: val_loss improved from 0.40110 to 0.40107, saving model to DeepFM.h5\n",
      "Epoch 862/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4356 - auc: 0.721 - ETA: 0s - loss: 0.4082 - auc: 0.678 - 0s 20us/step - loss: 0.3999 - auc: 0.6793 - val_loss: 0.4010 - val_auc: 0.6678\n",
      "\n",
      "Epoch 00862: val_loss improved from 0.40107 to 0.40102, saving model to DeepFM.h5\n",
      "Epoch 863/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4369 - auc: 0.731 - ETA: 0s - loss: 0.4006 - auc: 0.688 - 0s 21us/step - loss: 0.3987 - auc: 0.6844 - val_loss: 0.4010 - val_auc: 0.6676\n",
      "\n",
      "Epoch 00863: val_loss improved from 0.40102 to 0.40096, saving model to DeepFM.h5\n",
      "Epoch 864/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3308 - auc: 0.733 - ETA: 0s - loss: 0.3990 - auc: 0.687 - 0s 21us/step - loss: 0.4001 - auc: 0.6803 - val_loss: 0.4009 - val_auc: 0.6673\n",
      "\n",
      "Epoch 00864: val_loss improved from 0.40096 to 0.40088, saving model to DeepFM.h5\n",
      "Epoch 865/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3982 - auc: 0.705 - ETA: 0s - loss: 0.3945 - auc: 0.705 - 0s 20us/step - loss: 0.3971 - auc: 0.6987 - val_loss: 0.4008 - val_auc: 0.6668\n",
      "\n",
      "Epoch 00865: val_loss improved from 0.40088 to 0.40081, saving model to DeepFM.h5\n",
      "Epoch 866/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3719 - auc: 0.741 - ETA: 0s - loss: 0.3975 - auc: 0.687 - 0s 21us/step - loss: 0.3989 - auc: 0.6881 - val_loss: 0.4007 - val_auc: 0.6666\n",
      "\n",
      "Epoch 00866: val_loss improved from 0.40081 to 0.40074, saving model to DeepFM.h5\n",
      "Epoch 867/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4248 - auc: 0.712 - ETA: 0s - loss: 0.3958 - auc: 0.695 - 0s 22us/step - loss: 0.3961 - auc: 0.7000 - val_loss: 0.4007 - val_auc: 0.6661\n",
      "\n",
      "Epoch 00867: val_loss improved from 0.40074 to 0.40070, saving model to DeepFM.h5\n",
      "Epoch 868/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4351 - auc: 0.625 - ETA: 0s - loss: 0.4007 - auc: 0.675 - 0s 21us/step - loss: 0.4003 - auc: 0.6788 - val_loss: 0.4006 - val_auc: 0.6665\n",
      "\n",
      "Epoch 00868: val_loss improved from 0.40070 to 0.40063, saving model to DeepFM.h5\n",
      "Epoch 869/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4100 - auc: 0.634 - ETA: 0s - loss: 0.3963 - auc: 0.683 - 0s 21us/step - loss: 0.3987 - auc: 0.6904 - val_loss: 0.4006 - val_auc: 0.6675\n",
      "\n",
      "Epoch 00869: val_loss improved from 0.40063 to 0.40058, saving model to DeepFM.h5\n",
      "Epoch 870/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3593 - auc: 0.693 - ETA: 0s - loss: 0.3965 - auc: 0.690 - 0s 23us/step - loss: 0.3991 - auc: 0.6831 - val_loss: 0.4005 - val_auc: 0.6682\n",
      "\n",
      "Epoch 00870: val_loss improved from 0.40058 to 0.40051, saving model to DeepFM.h5\n",
      "Epoch 871/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4242 - auc: 0.709 - ETA: 0s - loss: 0.3968 - auc: 0.687 - 0s 22us/step - loss: 0.3975 - auc: 0.6970 - val_loss: 0.4005 - val_auc: 0.6675\n",
      "\n",
      "Epoch 00871: val_loss improved from 0.40051 to 0.40047, saving model to DeepFM.h5\n",
      "Epoch 872/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3610 - auc: 0.673 - ETA: 0s - loss: 0.3957 - auc: 0.679 - 0s 21us/step - loss: 0.3983 - auc: 0.6921 - val_loss: 0.4004 - val_auc: 0.6676\n",
      "\n",
      "Epoch 00872: val_loss improved from 0.40047 to 0.40040, saving model to DeepFM.h5\n",
      "Epoch 873/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3570 - auc: 0.660 - ETA: 0s - loss: 0.3881 - auc: 0.697 - 0s 25us/step - loss: 0.3976 - auc: 0.6885 - val_loss: 0.4003 - val_auc: 0.6680\n",
      "\n",
      "Epoch 00873: val_loss improved from 0.40040 to 0.40034, saving model to DeepFM.h5\n",
      "Epoch 874/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4684 - auc: 0.677 - ETA: 0s - loss: 0.3953 - auc: 0.676 - 0s 22us/step - loss: 0.3990 - auc: 0.6843 - val_loss: 0.4003 - val_auc: 0.6685\n",
      "\n",
      "Epoch 00874: val_loss improved from 0.40034 to 0.40029, saving model to DeepFM.h5\n",
      "Epoch 875/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3516 - auc: 0.648 - ETA: 0s - loss: 0.3991 - auc: 0.680 - 0s 23us/step - loss: 0.3982 - auc: 0.6921 - val_loss: 0.4002 - val_auc: 0.6690\n",
      "\n",
      "Epoch 00875: val_loss improved from 0.40029 to 0.40023, saving model to DeepFM.h5\n",
      "Epoch 876/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4032 - auc: 0.716 - ETA: 0s - loss: 0.3959 - auc: 0.701 - 0s 24us/step - loss: 0.3978 - auc: 0.6922 - val_loss: 0.4002 - val_auc: 0.6683\n",
      "\n",
      "Epoch 00876: val_loss improved from 0.40023 to 0.40016, saving model to DeepFM.h5\n",
      "Epoch 877/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3996 - auc: 0.679 - ETA: 0s - loss: 0.3920 - auc: 0.703 - ETA: 0s - loss: 0.3956 - auc: 0.701 - 0s 32us/step - loss: 0.3965 - auc: 0.7009 - val_loss: 0.4001 - val_auc: 0.6690\n",
      "\n",
      "Epoch 00877: val_loss improved from 0.40016 to 0.40009, saving model to DeepFM.h5\n",
      "Epoch 878/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3935 - auc: 0.564 - ETA: 0s - loss: 0.3971 - auc: 0.684 - 0s 21us/step - loss: 0.3976 - auc: 0.6939 - val_loss: 0.4001 - val_auc: 0.6694\n",
      "\n",
      "Epoch 00878: val_loss improved from 0.40009 to 0.40005, saving model to DeepFM.h5\n",
      "Epoch 879/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3675 - auc: 0.765 - ETA: 0s - loss: 0.3993 - auc: 0.707 - 0s 23us/step - loss: 0.3945 - auc: 0.7054 - val_loss: 0.4000 - val_auc: 0.6696\n",
      "\n",
      "Epoch 00879: val_loss improved from 0.40005 to 0.39998, saving model to DeepFM.h5\n",
      "Epoch 880/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4323 - auc: 0.734 - ETA: 0s - loss: 0.3969 - auc: 0.696 - 0s 23us/step - loss: 0.3974 - auc: 0.6907 - val_loss: 0.3999 - val_auc: 0.6698\n",
      "\n",
      "Epoch 00880: val_loss improved from 0.39998 to 0.39991, saving model to DeepFM.h5\n",
      "Epoch 881/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4185 - auc: 0.717 - ETA: 0s - loss: 0.4068 - auc: 0.702 - 0s 23us/step - loss: 0.3942 - auc: 0.7082 - val_loss: 0.3998 - val_auc: 0.6710\n",
      "\n",
      "Epoch 00881: val_loss improved from 0.39991 to 0.39983, saving model to DeepFM.h5\n",
      "Epoch 882/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4316 - auc: 0.668 - ETA: 0s - loss: 0.3944 - auc: 0.698 - 0s 26us/step - loss: 0.3965 - auc: 0.6949 - val_loss: 0.3998 - val_auc: 0.6705\n",
      "\n",
      "Epoch 00882: val_loss improved from 0.39983 to 0.39976, saving model to DeepFM.h5\n",
      "Epoch 883/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4395 - auc: 0.645 - ETA: 0s - loss: 0.3957 - auc: 0.711 - 0s 20us/step - loss: 0.3956 - auc: 0.7033 - val_loss: 0.3997 - val_auc: 0.6708\n",
      "\n",
      "Epoch 00883: val_loss improved from 0.39976 to 0.39974, saving model to DeepFM.h5\n",
      "Epoch 884/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4678 - auc: 0.694 - ETA: 0s - loss: 0.3994 - auc: 0.683 - 0s 19us/step - loss: 0.3998 - auc: 0.6821 - val_loss: 0.3997 - val_auc: 0.6717\n",
      "\n",
      "Epoch 00884: val_loss improved from 0.39974 to 0.39968, saving model to DeepFM.h5\n",
      "Epoch 885/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4563 - auc: 0.647 - ETA: 0s - loss: 0.4031 - auc: 0.684 - 0s 20us/step - loss: 0.3971 - auc: 0.6939 - val_loss: 0.3996 - val_auc: 0.6711\n",
      "\n",
      "Epoch 00885: val_loss improved from 0.39968 to 0.39965, saving model to DeepFM.h5\n",
      "Epoch 886/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4568 - auc: 0.681 - ETA: 0s - loss: 0.4027 - auc: 0.687 - 0s 21us/step - loss: 0.3978 - auc: 0.6975 - val_loss: 0.3996 - val_auc: 0.6718\n",
      "\n",
      "Epoch 00886: val_loss improved from 0.39965 to 0.39959, saving model to DeepFM.h5\n",
      "Epoch 887/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3763 - auc: 0.705 - ETA: 0s - loss: 0.3858 - auc: 0.695 - 0s 21us/step - loss: 0.3968 - auc: 0.6941 - val_loss: 0.3995 - val_auc: 0.6725\n",
      "\n",
      "Epoch 00887: val_loss improved from 0.39959 to 0.39950, saving model to DeepFM.h5\n",
      "Epoch 888/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4112 - auc: 0.763 - ETA: 0s - loss: 0.3943 - auc: 0.691 - 0s 21us/step - loss: 0.3972 - auc: 0.6962 - val_loss: 0.3994 - val_auc: 0.6716\n",
      "\n",
      "Epoch 00888: val_loss improved from 0.39950 to 0.39943, saving model to DeepFM.h5\n",
      "Epoch 889/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4029 - auc: 0.752 - ETA: 0s - loss: 0.4012 - auc: 0.689 - 0s 22us/step - loss: 0.3990 - auc: 0.6869 - val_loss: 0.3994 - val_auc: 0.6721\n",
      "\n",
      "Epoch 00889: val_loss improved from 0.39943 to 0.39936, saving model to DeepFM.h5\n",
      "Epoch 890/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4819 - auc: 0.625 - ETA: 0s - loss: 0.4017 - auc: 0.695 - 0s 21us/step - loss: 0.3959 - auc: 0.6992 - val_loss: 0.3993 - val_auc: 0.6714\n",
      "\n",
      "Epoch 00890: val_loss improved from 0.39936 to 0.39927, saving model to DeepFM.h5\n",
      "Epoch 891/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4250 - auc: 0.692 - ETA: 0s - loss: 0.4024 - auc: 0.700 - 0s 20us/step - loss: 0.3963 - auc: 0.6963 - val_loss: 0.3992 - val_auc: 0.6728\n",
      "\n",
      "Epoch 00891: val_loss improved from 0.39927 to 0.39923, saving model to DeepFM.h5\n",
      "Epoch 892/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3686 - auc: 0.682 - ETA: 0s - loss: 0.3882 - auc: 0.701 - 0s 22us/step - loss: 0.3949 - auc: 0.7032 - val_loss: 0.3992 - val_auc: 0.6728\n",
      "\n",
      "Epoch 00892: val_loss improved from 0.39923 to 0.39919, saving model to DeepFM.h5\n",
      "Epoch 893/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3604 - auc: 0.736 - ETA: 0s - loss: 0.3945 - auc: 0.706 - 0s 21us/step - loss: 0.3947 - auc: 0.7055 - val_loss: 0.3991 - val_auc: 0.6736\n",
      "\n",
      "Epoch 00893: val_loss improved from 0.39919 to 0.39912, saving model to DeepFM.h5\n",
      "Epoch 894/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3507 - auc: 0.721 - ETA: 0s - loss: 0.3954 - auc: 0.705 - 0s 22us/step - loss: 0.3968 - auc: 0.6933 - val_loss: 0.3990 - val_auc: 0.6739\n",
      "\n",
      "Epoch 00894: val_loss improved from 0.39912 to 0.39904, saving model to DeepFM.h5\n",
      "Epoch 895/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4485 - auc: 0.642 - ETA: 0s - loss: 0.3901 - auc: 0.716 - 0s 21us/step - loss: 0.3930 - auc: 0.7116 - val_loss: 0.3990 - val_auc: 0.6739\n",
      "\n",
      "Epoch 00895: val_loss improved from 0.39904 to 0.39896, saving model to DeepFM.h5\n",
      "Epoch 896/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3579 - auc: 0.684 - ETA: 0s - loss: 0.3898 - auc: 0.689 - 0s 21us/step - loss: 0.3971 - auc: 0.6928 - val_loss: 0.3989 - val_auc: 0.6739\n",
      "\n",
      "Epoch 00896: val_loss improved from 0.39896 to 0.39887, saving model to DeepFM.h5\n",
      "Epoch 897/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4093 - auc: 0.657 - ETA: 0s - loss: 0.3980 - auc: 0.682 - 0s 21us/step - loss: 0.3969 - auc: 0.6918 - val_loss: 0.3988 - val_auc: 0.6742\n",
      "\n",
      "Epoch 00897: val_loss improved from 0.39887 to 0.39880, saving model to DeepFM.h5\n",
      "Epoch 898/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3773 - auc: 0.670 - ETA: 0s - loss: 0.3909 - auc: 0.693 - 0s 22us/step - loss: 0.3968 - auc: 0.6903 - val_loss: 0.3987 - val_auc: 0.6745\n",
      "\n",
      "Epoch 00898: val_loss improved from 0.39880 to 0.39874, saving model to DeepFM.h5\n",
      "Epoch 899/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4480 - auc: 0.625 - ETA: 0s - loss: 0.3967 - auc: 0.688 - 0s 23us/step - loss: 0.3948 - auc: 0.7047 - val_loss: 0.3987 - val_auc: 0.6745\n",
      "\n",
      "Epoch 00899: val_loss improved from 0.39874 to 0.39868, saving model to DeepFM.h5\n",
      "Epoch 900/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3904 - auc: 0.686 - ETA: 0s - loss: 0.3919 - auc: 0.696 - 0s 21us/step - loss: 0.3946 - auc: 0.7038 - val_loss: 0.3986 - val_auc: 0.6752\n",
      "\n",
      "Epoch 00900: val_loss improved from 0.39868 to 0.39862, saving model to DeepFM.h5\n",
      "Epoch 901/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3554 - auc: 0.638 - ETA: 0s - loss: 0.3939 - auc: 0.693 - 0s 23us/step - loss: 0.3964 - auc: 0.6950 - val_loss: 0.3986 - val_auc: 0.6755\n",
      "\n",
      "Epoch 00901: val_loss improved from 0.39862 to 0.39857, saving model to DeepFM.h5\n",
      "Epoch 902/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4418 - auc: 0.710 - ETA: 0s - loss: 0.3922 - auc: 0.691 - 0s 21us/step - loss: 0.3963 - auc: 0.6964 - val_loss: 0.3985 - val_auc: 0.6753\n",
      "\n",
      "Epoch 00902: val_loss improved from 0.39857 to 0.39849, saving model to DeepFM.h5\n",
      "Epoch 903/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3667 - auc: 0.703 - ETA: 0s - loss: 0.3872 - auc: 0.692 - 0s 21us/step - loss: 0.3954 - auc: 0.6979 - val_loss: 0.3984 - val_auc: 0.6768\n",
      "\n",
      "Epoch 00903: val_loss improved from 0.39849 to 0.39841, saving model to DeepFM.h5\n",
      "Epoch 904/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3888 - auc: 0.719 - ETA: 0s - loss: 0.3899 - auc: 0.698 - 0s 21us/step - loss: 0.3945 - auc: 0.6979 - val_loss: 0.3983 - val_auc: 0.6764\n",
      "\n",
      "Epoch 00904: val_loss improved from 0.39841 to 0.39835, saving model to DeepFM.h5\n",
      "Epoch 905/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3598 - auc: 0.702 - ETA: 0s - loss: 0.4000 - auc: 0.690 - 0s 21us/step - loss: 0.3948 - auc: 0.6988 - val_loss: 0.3983 - val_auc: 0.6766\n",
      "\n",
      "Epoch 00905: val_loss improved from 0.39835 to 0.39829, saving model to DeepFM.h5\n",
      "Epoch 906/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4299 - auc: 0.714 - ETA: 0s - loss: 0.3947 - auc: 0.712 - 0s 20us/step - loss: 0.3953 - auc: 0.7018 - val_loss: 0.3982 - val_auc: 0.6763\n",
      "\n",
      "Epoch 00906: val_loss improved from 0.39829 to 0.39823, saving model to DeepFM.h5\n",
      "Epoch 907/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3945 - auc: 0.720 - ETA: 0s - loss: 0.3922 - auc: 0.710 - 0s 22us/step - loss: 0.3939 - auc: 0.7064 - val_loss: 0.3982 - val_auc: 0.6773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00907: val_loss improved from 0.39823 to 0.39819, saving model to DeepFM.h5\n",
      "Epoch 908/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3440 - auc: 0.712 - ETA: 0s - loss: 0.3997 - auc: 0.694 - 0s 21us/step - loss: 0.3945 - auc: 0.7024 - val_loss: 0.3981 - val_auc: 0.6768\n",
      "\n",
      "Epoch 00908: val_loss improved from 0.39819 to 0.39813, saving model to DeepFM.h5\n",
      "Epoch 909/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4146 - auc: 0.727 - ETA: 0s - loss: 0.3952 - auc: 0.707 - 0s 22us/step - loss: 0.3946 - auc: 0.7047 - val_loss: 0.3980 - val_auc: 0.6779\n",
      "\n",
      "Epoch 00909: val_loss improved from 0.39813 to 0.39804, saving model to DeepFM.h5\n",
      "Epoch 910/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3223 - auc: 0.800 - ETA: 0s - loss: 0.3934 - auc: 0.708 - 0s 22us/step - loss: 0.3927 - auc: 0.7075 - val_loss: 0.3980 - val_auc: 0.6775\n",
      "\n",
      "Epoch 00910: val_loss improved from 0.39804 to 0.39797, saving model to DeepFM.h5\n",
      "Epoch 911/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3603 - auc: 0.728 - ETA: 0s - loss: 0.3919 - auc: 0.716 - 0s 23us/step - loss: 0.3915 - auc: 0.7185 - val_loss: 0.3979 - val_auc: 0.6786\n",
      "\n",
      "Epoch 00911: val_loss improved from 0.39797 to 0.39793, saving model to DeepFM.h5\n",
      "Epoch 912/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3924 - auc: 0.685 - ETA: 0s - loss: 0.3842 - auc: 0.719 - 0s 21us/step - loss: 0.3938 - auc: 0.7012 - val_loss: 0.3979 - val_auc: 0.6777\n",
      "\n",
      "Epoch 00912: val_loss improved from 0.39793 to 0.39788, saving model to DeepFM.h5\n",
      "Epoch 913/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3764 - auc: 0.668 - ETA: 0s - loss: 0.3844 - auc: 0.710 - 0s 23us/step - loss: 0.3947 - auc: 0.6987 - val_loss: 0.3978 - val_auc: 0.6778\n",
      "\n",
      "Epoch 00913: val_loss improved from 0.39788 to 0.39781, saving model to DeepFM.h5\n",
      "Epoch 914/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3995 - auc: 0.710 - ETA: 0s - loss: 0.3939 - auc: 0.715 - 0s 21us/step - loss: 0.3928 - auc: 0.7142 - val_loss: 0.3978 - val_auc: 0.6786\n",
      "\n",
      "Epoch 00914: val_loss improved from 0.39781 to 0.39775, saving model to DeepFM.h5\n",
      "Epoch 915/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3349 - auc: 0.797 - ETA: 0s - loss: 0.4086 - auc: 0.686 - 0s 22us/step - loss: 0.3968 - auc: 0.6864 - val_loss: 0.3977 - val_auc: 0.6787\n",
      "\n",
      "Epoch 00915: val_loss improved from 0.39775 to 0.39770, saving model to DeepFM.h5\n",
      "Epoch 916/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3798 - auc: 0.736 - ETA: 0s - loss: 0.3831 - auc: 0.678 - 0s 21us/step - loss: 0.3956 - auc: 0.6951 - val_loss: 0.3976 - val_auc: 0.6793\n",
      "\n",
      "Epoch 00916: val_loss improved from 0.39770 to 0.39759, saving model to DeepFM.h5\n",
      "Epoch 917/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.650 - ETA: 0s - loss: 0.3783 - auc: 0.698 - 0s 23us/step - loss: 0.3963 - auc: 0.6959 - val_loss: 0.3975 - val_auc: 0.6783\n",
      "\n",
      "Epoch 00917: val_loss improved from 0.39759 to 0.39750, saving model to DeepFM.h5\n",
      "Epoch 918/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3956 - auc: 0.724 - ETA: 0s - loss: 0.3924 - auc: 0.707 - 0s 20us/step - loss: 0.3950 - auc: 0.7012 - val_loss: 0.3974 - val_auc: 0.6793\n",
      "\n",
      "Epoch 00918: val_loss improved from 0.39750 to 0.39743, saving model to DeepFM.h5\n",
      "Epoch 919/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3771 - auc: 0.686 - ETA: 0s - loss: 0.3924 - auc: 0.704 - 0s 20us/step - loss: 0.3928 - auc: 0.7066 - val_loss: 0.3974 - val_auc: 0.6788\n",
      "\n",
      "Epoch 00919: val_loss improved from 0.39743 to 0.39736, saving model to DeepFM.h5\n",
      "Epoch 920/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3931 - auc: 0.666 - ETA: 0s - loss: 0.3916 - auc: 0.693 - 0s 21us/step - loss: 0.3916 - auc: 0.7096 - val_loss: 0.3973 - val_auc: 0.6792\n",
      "\n",
      "Epoch 00920: val_loss improved from 0.39736 to 0.39732, saving model to DeepFM.h5\n",
      "Epoch 921/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4196 - auc: 0.648 - ETA: 0s - loss: 0.3982 - auc: 0.698 - 0s 20us/step - loss: 0.3934 - auc: 0.7033 - val_loss: 0.3973 - val_auc: 0.6799\n",
      "\n",
      "Epoch 00921: val_loss improved from 0.39732 to 0.39727, saving model to DeepFM.h5\n",
      "Epoch 922/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3923 - auc: 0.772 - ETA: 0s - loss: 0.3867 - auc: 0.702 - 0s 21us/step - loss: 0.3949 - auc: 0.7024 - val_loss: 0.3972 - val_auc: 0.6793\n",
      "\n",
      "Epoch 00922: val_loss improved from 0.39727 to 0.39721, saving model to DeepFM.h5\n",
      "Epoch 923/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3914 - auc: 0.661 - ETA: 0s - loss: 0.3950 - auc: 0.703 - 0s 20us/step - loss: 0.3942 - auc: 0.7053 - val_loss: 0.3971 - val_auc: 0.6792\n",
      "\n",
      "Epoch 00923: val_loss improved from 0.39721 to 0.39714, saving model to DeepFM.h5\n",
      "Epoch 924/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4701 - auc: 0.662 - ETA: 0s - loss: 0.3938 - auc: 0.700 - 0s 21us/step - loss: 0.3953 - auc: 0.6923 - val_loss: 0.3971 - val_auc: 0.6793\n",
      "\n",
      "Epoch 00924: val_loss improved from 0.39714 to 0.39706, saving model to DeepFM.h5\n",
      "Epoch 925/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4125 - auc: 0.665 - ETA: 0s - loss: 0.3933 - auc: 0.715 - 0s 22us/step - loss: 0.3947 - auc: 0.7018 - val_loss: 0.3970 - val_auc: 0.6799\n",
      "\n",
      "Epoch 00925: val_loss improved from 0.39706 to 0.39701, saving model to DeepFM.h5\n",
      "Epoch 926/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3591 - auc: 0.688 - ETA: 0s - loss: 0.3990 - auc: 0.693 - 0s 22us/step - loss: 0.3919 - auc: 0.7063 - val_loss: 0.3969 - val_auc: 0.6809\n",
      "\n",
      "Epoch 00926: val_loss improved from 0.39701 to 0.39695, saving model to DeepFM.h5\n",
      "Epoch 927/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3970 - auc: 0.709 - ETA: 0s - loss: 0.3893 - auc: 0.723 - 0s 21us/step - loss: 0.3917 - auc: 0.7140 - val_loss: 0.3969 - val_auc: 0.6817\n",
      "\n",
      "Epoch 00927: val_loss improved from 0.39695 to 0.39688, saving model to DeepFM.h5\n",
      "Epoch 928/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4691 - auc: 0.666 - ETA: 0s - loss: 0.3993 - auc: 0.713 - 0s 20us/step - loss: 0.3962 - auc: 0.6976 - val_loss: 0.3968 - val_auc: 0.6815\n",
      "\n",
      "Epoch 00928: val_loss improved from 0.39688 to 0.39681, saving model to DeepFM.h5\n",
      "Epoch 929/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5025 - auc: 0.616 - ETA: 0s - loss: 0.3924 - auc: 0.676 - 0s 22us/step - loss: 0.3946 - auc: 0.7000 - val_loss: 0.3967 - val_auc: 0.6816\n",
      "\n",
      "Epoch 00929: val_loss improved from 0.39681 to 0.39673, saving model to DeepFM.h5\n",
      "Epoch 930/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3831 - auc: 0.689 - ETA: 0s - loss: 0.3938 - auc: 0.699 - 0s 21us/step - loss: 0.3941 - auc: 0.7015 - val_loss: 0.3967 - val_auc: 0.6830\n",
      "\n",
      "Epoch 00930: val_loss improved from 0.39673 to 0.39668, saving model to DeepFM.h5\n",
      "Epoch 931/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3752 - auc: 0.745 - ETA: 0s - loss: 0.3968 - auc: 0.683 - 0s 21us/step - loss: 0.3951 - auc: 0.6951 - val_loss: 0.3966 - val_auc: 0.6837\n",
      "\n",
      "Epoch 00931: val_loss improved from 0.39668 to 0.39662, saving model to DeepFM.h5\n",
      "Epoch 932/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4180 - auc: 0.731 - ETA: 0s - loss: 0.3884 - auc: 0.726 - 0s 21us/step - loss: 0.3894 - auc: 0.7221 - val_loss: 0.3965 - val_auc: 0.6824\n",
      "\n",
      "Epoch 00932: val_loss improved from 0.39662 to 0.39650, saving model to DeepFM.h5\n",
      "Epoch 933/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3870 - auc: 0.745 - ETA: 0s - loss: 0.4036 - auc: 0.705 - 0s 21us/step - loss: 0.3917 - auc: 0.7085 - val_loss: 0.3964 - val_auc: 0.6829\n",
      "\n",
      "Epoch 00933: val_loss improved from 0.39650 to 0.39644, saving model to DeepFM.h5\n",
      "Epoch 934/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3895 - auc: 0.730 - ETA: 0s - loss: 0.3945 - auc: 0.706 - 0s 21us/step - loss: 0.3937 - auc: 0.7063 - val_loss: 0.3964 - val_auc: 0.6831\n",
      "\n",
      "Epoch 00934: val_loss improved from 0.39644 to 0.39636, saving model to DeepFM.h5\n",
      "Epoch 935/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3496 - auc: 0.663 - ETA: 0s - loss: 0.3764 - auc: 0.713 - 0s 22us/step - loss: 0.3910 - auc: 0.7176 - val_loss: 0.3963 - val_auc: 0.6824\n",
      "\n",
      "Epoch 00935: val_loss improved from 0.39636 to 0.39627, saving model to DeepFM.h5\n",
      "Epoch 936/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4322 - auc: 0.700 - ETA: 0s - loss: 0.3878 - auc: 0.715 - 0s 20us/step - loss: 0.3908 - auc: 0.7121 - val_loss: 0.3962 - val_auc: 0.6835\n",
      "\n",
      "Epoch 00936: val_loss improved from 0.39627 to 0.39620, saving model to DeepFM.h5\n",
      "Epoch 937/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4056 - auc: 0.663 - ETA: 0s - loss: 0.3925 - auc: 0.708 - 0s 20us/step - loss: 0.3928 - auc: 0.7100 - val_loss: 0.3961 - val_auc: 0.6842\n",
      "\n",
      "Epoch 00937: val_loss improved from 0.39620 to 0.39614, saving model to DeepFM.h5\n",
      "Epoch 938/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3724 - auc: 0.705 - ETA: 0s - loss: 0.3885 - auc: 0.702 - 0s 19us/step - loss: 0.3931 - auc: 0.7055 - val_loss: 0.3961 - val_auc: 0.6840\n",
      "\n",
      "Epoch 00938: val_loss improved from 0.39614 to 0.39606, saving model to DeepFM.h5\n",
      "Epoch 939/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3534 - auc: 0.786 - ETA: 0s - loss: 0.3895 - auc: 0.717 - 0s 20us/step - loss: 0.3908 - auc: 0.7134 - val_loss: 0.3960 - val_auc: 0.6842\n",
      "\n",
      "Epoch 00939: val_loss improved from 0.39606 to 0.39598, saving model to DeepFM.h5\n",
      "Epoch 940/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4102 - auc: 0.683 - ETA: 0s - loss: 0.3896 - auc: 0.721 - 0s 24us/step - loss: 0.3924 - auc: 0.7089 - val_loss: 0.3959 - val_auc: 0.6838\n",
      "\n",
      "Epoch 00940: val_loss improved from 0.39598 to 0.39592, saving model to DeepFM.h5\n",
      "Epoch 941/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3667 - auc: 0.682 - ETA: 0s - loss: 0.3830 - auc: 0.712 - 0s 23us/step - loss: 0.3912 - auc: 0.7123 - val_loss: 0.3958 - val_auc: 0.6848\n",
      "\n",
      "Epoch 00941: val_loss improved from 0.39592 to 0.39583, saving model to DeepFM.h5\n",
      "Epoch 942/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4185 - auc: 0.644 - ETA: 0s - loss: 0.3939 - auc: 0.710 - 0s 21us/step - loss: 0.3926 - auc: 0.7078 - val_loss: 0.3958 - val_auc: 0.6845\n",
      "\n",
      "Epoch 00942: val_loss improved from 0.39583 to 0.39580, saving model to DeepFM.h5\n",
      "Epoch 943/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4193 - auc: 0.733 - ETA: 0s - loss: 0.3902 - auc: 0.709 - 0s 21us/step - loss: 0.3918 - auc: 0.7142 - val_loss: 0.3957 - val_auc: 0.6853\n",
      "\n",
      "Epoch 00943: val_loss improved from 0.39580 to 0.39571, saving model to DeepFM.h5\n",
      "Epoch 944/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4166 - auc: 0.718 - ETA: 0s - loss: 0.3909 - auc: 0.719 - 0s 21us/step - loss: 0.3912 - auc: 0.7158 - val_loss: 0.3957 - val_auc: 0.6848\n",
      "\n",
      "Epoch 00944: val_loss improved from 0.39571 to 0.39566, saving model to DeepFM.h5\n",
      "Epoch 945/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3829 - auc: 0.705 - ETA: 0s - loss: 0.3792 - auc: 0.697 - 0s 25us/step - loss: 0.3893 - auc: 0.7192 - val_loss: 0.3956 - val_auc: 0.6846\n",
      "\n",
      "Epoch 00945: val_loss improved from 0.39566 to 0.39560, saving model to DeepFM.h5\n",
      "Epoch 946/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3835 - auc: 0.708 - ETA: 0s - loss: 0.3911 - auc: 0.715 - 0s 22us/step - loss: 0.3907 - auc: 0.7152 - val_loss: 0.3955 - val_auc: 0.6846\n",
      "\n",
      "Epoch 00946: val_loss improved from 0.39560 to 0.39553, saving model to DeepFM.h5\n",
      "Epoch 947/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4461 - auc: 0.754 - ETA: 0s - loss: 0.3774 - auc: 0.735 - 0s 21us/step - loss: 0.3891 - auc: 0.7244 - val_loss: 0.3954 - val_auc: 0.6864\n",
      "\n",
      "Epoch 00947: val_loss improved from 0.39553 to 0.39542, saving model to DeepFM.h5\n",
      "Epoch 948/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3540 - auc: 0.714 - ETA: 0s - loss: 0.3840 - auc: 0.721 - 0s 23us/step - loss: 0.3901 - auc: 0.7168 - val_loss: 0.3953 - val_auc: 0.6863\n",
      "\n",
      "Epoch 00948: val_loss improved from 0.39542 to 0.39534, saving model to DeepFM.h5\n",
      "Epoch 949/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3869 - auc: 0.737 - ETA: 0s - loss: 0.3863 - auc: 0.716 - 0s 23us/step - loss: 0.3891 - auc: 0.7187 - val_loss: 0.3953 - val_auc: 0.6870\n",
      "\n",
      "Epoch 00949: val_loss improved from 0.39534 to 0.39530, saving model to DeepFM.h5\n",
      "Epoch 950/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3364 - auc: 0.744 - ETA: 0s - loss: 0.3813 - auc: 0.715 - 0s 22us/step - loss: 0.3908 - auc: 0.7154 - val_loss: 0.3952 - val_auc: 0.6875\n",
      "\n",
      "Epoch 00950: val_loss improved from 0.39530 to 0.39522, saving model to DeepFM.h5\n",
      "Epoch 951/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.728 - ETA: 0s - loss: 0.3929 - auc: 0.713 - 0s 19us/step - loss: 0.3902 - auc: 0.7177 - val_loss: 0.3951 - val_auc: 0.6871\n",
      "\n",
      "Epoch 00951: val_loss improved from 0.39522 to 0.39515, saving model to DeepFM.h5\n",
      "Epoch 952/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3636 - auc: 0.723 - ETA: 0s - loss: 0.3949 - auc: 0.719 - 0s 19us/step - loss: 0.3882 - auc: 0.7221 - val_loss: 0.3951 - val_auc: 0.6886\n",
      "\n",
      "Epoch 00952: val_loss improved from 0.39515 to 0.39506, saving model to DeepFM.h5\n",
      "Epoch 953/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3701 - auc: 0.702 - ETA: 0s - loss: 0.3828 - auc: 0.715 - 0s 20us/step - loss: 0.3887 - auc: 0.7198 - val_loss: 0.3950 - val_auc: 0.6876\n",
      "\n",
      "Epoch 00953: val_loss improved from 0.39506 to 0.39500, saving model to DeepFM.h5\n",
      "Epoch 954/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3983 - auc: 0.747 - ETA: 0s - loss: 0.3855 - auc: 0.715 - 0s 20us/step - loss: 0.3900 - auc: 0.7171 - val_loss: 0.3949 - val_auc: 0.6878\n",
      "\n",
      "Epoch 00954: val_loss improved from 0.39500 to 0.39493, saving model to DeepFM.h5\n",
      "Epoch 955/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3827 - auc: 0.660 - ETA: 0s - loss: 0.3961 - auc: 0.721 - 0s 21us/step - loss: 0.3900 - auc: 0.7206 - val_loss: 0.3949 - val_auc: 0.6884\n",
      "\n",
      "Epoch 00955: val_loss improved from 0.39493 to 0.39487, saving model to DeepFM.h5\n",
      "Epoch 956/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4074 - auc: 0.771 - ETA: 0s - loss: 0.3797 - auc: 0.722 - 0s 21us/step - loss: 0.3894 - auc: 0.7178 - val_loss: 0.3948 - val_auc: 0.6888\n",
      "\n",
      "Epoch 00956: val_loss improved from 0.39487 to 0.39476, saving model to DeepFM.h5\n",
      "Epoch 957/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3784 - auc: 0.755 - ETA: 0s - loss: 0.3926 - auc: 0.729 - 0s 20us/step - loss: 0.3895 - auc: 0.7198 - val_loss: 0.3947 - val_auc: 0.6893\n",
      "\n",
      "Epoch 00957: val_loss improved from 0.39476 to 0.39467, saving model to DeepFM.h5\n",
      "Epoch 958/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4169 - auc: 0.679 - ETA: 0s - loss: 0.3862 - auc: 0.726 - 0s 20us/step - loss: 0.3886 - auc: 0.7200 - val_loss: 0.3946 - val_auc: 0.6889\n",
      "\n",
      "Epoch 00958: val_loss improved from 0.39467 to 0.39462, saving model to DeepFM.h5\n",
      "Epoch 959/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3521 - auc: 0.643 - ETA: 0s - loss: 0.3898 - auc: 0.697 - 0s 22us/step - loss: 0.3914 - auc: 0.7094 - val_loss: 0.3945 - val_auc: 0.6891\n",
      "\n",
      "Epoch 00959: val_loss improved from 0.39462 to 0.39454, saving model to DeepFM.h5\n",
      "Epoch 960/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3744 - auc: 0.758 - ETA: 0s - loss: 0.3823 - auc: 0.715 - 0s 21us/step - loss: 0.3890 - auc: 0.7205 - val_loss: 0.3944 - val_auc: 0.6899\n",
      "\n",
      "Epoch 00960: val_loss improved from 0.39454 to 0.39445, saving model to DeepFM.h5\n",
      "Epoch 961/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4061 - auc: 0.714 - ETA: 0s - loss: 0.3850 - auc: 0.725 - 0s 21us/step - loss: 0.3892 - auc: 0.7197 - val_loss: 0.3944 - val_auc: 0.6897\n",
      "\n",
      "Epoch 00961: val_loss improved from 0.39445 to 0.39438, saving model to DeepFM.h5\n",
      "Epoch 962/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3041 - auc: 0.772 - ETA: 0s - loss: 0.3965 - auc: 0.710 - 0s 21us/step - loss: 0.3889 - auc: 0.7191 - val_loss: 0.3943 - val_auc: 0.6902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00962: val_loss improved from 0.39438 to 0.39432, saving model to DeepFM.h5\n",
      "Epoch 963/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3444 - auc: 0.790 - ETA: 0s - loss: 0.3780 - auc: 0.713 - 0s 21us/step - loss: 0.3896 - auc: 0.7126 - val_loss: 0.3942 - val_auc: 0.6899\n",
      "\n",
      "Epoch 00963: val_loss improved from 0.39432 to 0.39425, saving model to DeepFM.h5\n",
      "Epoch 964/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4259 - auc: 0.625 - ETA: 0s - loss: 0.3777 - auc: 0.733 - 0s 22us/step - loss: 0.3881 - auc: 0.7229 - val_loss: 0.3942 - val_auc: 0.6903\n",
      "\n",
      "Epoch 00964: val_loss improved from 0.39425 to 0.39416, saving model to DeepFM.h5\n",
      "Epoch 965/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4027 - auc: 0.765 - ETA: 0s - loss: 0.3964 - auc: 0.717 - 0s 20us/step - loss: 0.3885 - auc: 0.7179 - val_loss: 0.3941 - val_auc: 0.6908\n",
      "\n",
      "Epoch 00965: val_loss improved from 0.39416 to 0.39412, saving model to DeepFM.h5\n",
      "Epoch 966/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4674 - auc: 0.695 - ETA: 0s - loss: 0.3894 - auc: 0.718 - 0s 20us/step - loss: 0.3892 - auc: 0.7209 - val_loss: 0.3941 - val_auc: 0.6916\n",
      "\n",
      "Epoch 00966: val_loss improved from 0.39412 to 0.39408, saving model to DeepFM.h5\n",
      "Epoch 967/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3610 - auc: 0.762 - ETA: 0s - loss: 0.3829 - auc: 0.721 - 0s 23us/step - loss: 0.3881 - auc: 0.7243 - val_loss: 0.3940 - val_auc: 0.6921\n",
      "\n",
      "Epoch 00967: val_loss improved from 0.39408 to 0.39399, saving model to DeepFM.h5\n",
      "Epoch 968/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4342 - auc: 0.644 - ETA: 0s - loss: 0.3940 - auc: 0.693 - 0s 22us/step - loss: 0.3909 - auc: 0.7107 - val_loss: 0.3939 - val_auc: 0.6912\n",
      "\n",
      "Epoch 00968: val_loss improved from 0.39399 to 0.39389, saving model to DeepFM.h5\n",
      "Epoch 969/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4773 - auc: 0.729 - ETA: 0s - loss: 0.3881 - auc: 0.735 - 0s 21us/step - loss: 0.3888 - auc: 0.7206 - val_loss: 0.3939 - val_auc: 0.6931\n",
      "\n",
      "Epoch 00969: val_loss improved from 0.39389 to 0.39385, saving model to DeepFM.h5\n",
      "Epoch 970/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4634 - auc: 0.720 - ETA: 0s - loss: 0.3903 - auc: 0.730 - 0s 21us/step - loss: 0.3865 - auc: 0.7276 - val_loss: 0.3938 - val_auc: 0.6938\n",
      "\n",
      "Epoch 00970: val_loss improved from 0.39385 to 0.39377, saving model to DeepFM.h5\n",
      "Epoch 971/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4020 - auc: 0.678 - ETA: 0s - loss: 0.3947 - auc: 0.729 - 0s 21us/step - loss: 0.3883 - auc: 0.7184 - val_loss: 0.3937 - val_auc: 0.6931\n",
      "\n",
      "Epoch 00971: val_loss improved from 0.39377 to 0.39371, saving model to DeepFM.h5\n",
      "Epoch 972/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3634 - auc: 0.612 - ETA: 0s - loss: 0.3781 - auc: 0.715 - 0s 24us/step - loss: 0.3902 - auc: 0.7168 - val_loss: 0.3936 - val_auc: 0.6926\n",
      "\n",
      "Epoch 00972: val_loss improved from 0.39371 to 0.39357, saving model to DeepFM.h5\n",
      "Epoch 973/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3809 - auc: 0.725 - ETA: 0s - loss: 0.3958 - auc: 0.704 - 0s 23us/step - loss: 0.3895 - auc: 0.7143 - val_loss: 0.3935 - val_auc: 0.6936\n",
      "\n",
      "Epoch 00973: val_loss improved from 0.39357 to 0.39353, saving model to DeepFM.h5\n",
      "Epoch 974/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4239 - auc: 0.723 - ETA: 0s - loss: 0.3935 - auc: 0.719 - 0s 22us/step - loss: 0.3863 - auc: 0.7283 - val_loss: 0.3934 - val_auc: 0.6934\n",
      "\n",
      "Epoch 00974: val_loss improved from 0.39353 to 0.39344, saving model to DeepFM.h5\n",
      "Epoch 975/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4152 - auc: 0.695 - ETA: 0s - loss: 0.3709 - auc: 0.735 - 0s 20us/step - loss: 0.3876 - auc: 0.7228 - val_loss: 0.3933 - val_auc: 0.6939\n",
      "\n",
      "Epoch 00975: val_loss improved from 0.39344 to 0.39334, saving model to DeepFM.h5\n",
      "Epoch 976/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4029 - auc: 0.730 - ETA: 0s - loss: 0.3856 - auc: 0.731 - 0s 20us/step - loss: 0.3874 - auc: 0.7277 - val_loss: 0.3933 - val_auc: 0.6947\n",
      "\n",
      "Epoch 00976: val_loss improved from 0.39334 to 0.39329, saving model to DeepFM.h5\n",
      "Epoch 977/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4240 - auc: 0.707 - ETA: 0s - loss: 0.3791 - auc: 0.720 - 0s 22us/step - loss: 0.3867 - auc: 0.7260 - val_loss: 0.3932 - val_auc: 0.6943\n",
      "\n",
      "Epoch 00977: val_loss improved from 0.39329 to 0.39324, saving model to DeepFM.h5\n",
      "Epoch 978/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3673 - auc: 0.663 - ETA: 0s - loss: 0.3861 - auc: 0.711 - 0s 20us/step - loss: 0.3881 - auc: 0.7219 - val_loss: 0.3931 - val_auc: 0.6954\n",
      "\n",
      "Epoch 00978: val_loss improved from 0.39324 to 0.39312, saving model to DeepFM.h5\n",
      "Epoch 979/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3823 - auc: 0.735 - ETA: 0s - loss: 0.3922 - auc: 0.726 - 0s 19us/step - loss: 0.3886 - auc: 0.7168 - val_loss: 0.3931 - val_auc: 0.6964\n",
      "\n",
      "Epoch 00979: val_loss improved from 0.39312 to 0.39306, saving model to DeepFM.h5\n",
      "Epoch 980/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3406 - auc: 0.813 - ETA: 0s - loss: 0.3853 - auc: 0.735 - 0s 24us/step - loss: 0.3863 - auc: 0.7275 - val_loss: 0.3930 - val_auc: 0.6963\n",
      "\n",
      "Epoch 00980: val_loss improved from 0.39306 to 0.39298, saving model to DeepFM.h5\n",
      "Epoch 981/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4004 - auc: 0.793 - ETA: 0s - loss: 0.3966 - auc: 0.728 - 0s 22us/step - loss: 0.3871 - auc: 0.7250 - val_loss: 0.3929 - val_auc: 0.6974\n",
      "\n",
      "Epoch 00981: val_loss improved from 0.39298 to 0.39288, saving model to DeepFM.h5\n",
      "Epoch 982/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4477 - auc: 0.707 - ETA: 0s - loss: 0.3862 - auc: 0.706 - 0s 23us/step - loss: 0.3901 - auc: 0.7115 - val_loss: 0.3928 - val_auc: 0.6967\n",
      "\n",
      "Epoch 00982: val_loss improved from 0.39288 to 0.39279, saving model to DeepFM.h5\n",
      "Epoch 983/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4628 - auc: 0.675 - ETA: 0s - loss: 0.3852 - auc: 0.737 - 0s 24us/step - loss: 0.3871 - auc: 0.7289 - val_loss: 0.3927 - val_auc: 0.6972\n",
      "\n",
      "Epoch 00983: val_loss improved from 0.39279 to 0.39271, saving model to DeepFM.h5\n",
      "Epoch 984/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3817 - auc: 0.706 - ETA: 0s - loss: 0.3901 - auc: 0.712 - 0s 23us/step - loss: 0.3914 - auc: 0.7061 - val_loss: 0.3927 - val_auc: 0.6978\n",
      "\n",
      "Epoch 00984: val_loss improved from 0.39271 to 0.39266, saving model to DeepFM.h5\n",
      "Epoch 985/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3486 - auc: 0.760 - ETA: 0s - loss: 0.3864 - auc: 0.726 - 0s 21us/step - loss: 0.3858 - auc: 0.7285 - val_loss: 0.3925 - val_auc: 0.6974\n",
      "\n",
      "Epoch 00985: val_loss improved from 0.39266 to 0.39255, saving model to DeepFM.h5\n",
      "Epoch 986/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4102 - auc: 0.666 - ETA: 0s - loss: 0.3855 - auc: 0.717 - 0s 21us/step - loss: 0.3873 - auc: 0.7213 - val_loss: 0.3925 - val_auc: 0.6977\n",
      "\n",
      "Epoch 00986: val_loss improved from 0.39255 to 0.39248, saving model to DeepFM.h5\n",
      "Epoch 987/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3436 - auc: 0.699 - ETA: 0s - loss: 0.3869 - auc: 0.729 - 0s 23us/step - loss: 0.3858 - auc: 0.7291 - val_loss: 0.3924 - val_auc: 0.6970\n",
      "\n",
      "Epoch 00987: val_loss improved from 0.39248 to 0.39242, saving model to DeepFM.h5\n",
      "Epoch 988/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4316 - auc: 0.715 - ETA: 0s - loss: 0.3943 - auc: 0.735 - 0s 22us/step - loss: 0.3875 - auc: 0.7262 - val_loss: 0.3924 - val_auc: 0.6970\n",
      "\n",
      "Epoch 00988: val_loss improved from 0.39242 to 0.39237, saving model to DeepFM.h5\n",
      "Epoch 989/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4249 - auc: 0.809 - ETA: 0s - loss: 0.3786 - auc: 0.735 - 0s 26us/step - loss: 0.3857 - auc: 0.7322 - val_loss: 0.3923 - val_auc: 0.6973\n",
      "\n",
      "Epoch 00989: val_loss improved from 0.39237 to 0.39228, saving model to DeepFM.h5\n",
      "Epoch 990/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4040 - auc: 0.749 - ETA: 0s - loss: 0.3803 - auc: 0.739 - 0s 23us/step - loss: 0.3863 - auc: 0.7317 - val_loss: 0.3922 - val_auc: 0.6971\n",
      "\n",
      "Epoch 00990: val_loss improved from 0.39228 to 0.39219, saving model to DeepFM.h5\n",
      "Epoch 991/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3884 - auc: 0.709 - ETA: 0s - loss: 0.3797 - auc: 0.738 - 0s 22us/step - loss: 0.3844 - auc: 0.7324 - val_loss: 0.3921 - val_auc: 0.6968\n",
      "\n",
      "Epoch 00991: val_loss improved from 0.39219 to 0.39212, saving model to DeepFM.h5\n",
      "Epoch 992/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4783 - auc: 0.698 - ETA: 0s - loss: 0.3793 - auc: 0.751 - 0s 21us/step - loss: 0.3817 - auc: 0.7459 - val_loss: 0.3920 - val_auc: 0.6976\n",
      "\n",
      "Epoch 00992: val_loss improved from 0.39212 to 0.39203, saving model to DeepFM.h5\n",
      "Epoch 993/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3857 - auc: 0.683 - ETA: 0s - loss: 0.3913 - auc: 0.726 - 0s 20us/step - loss: 0.3852 - auc: 0.7308 - val_loss: 0.3919 - val_auc: 0.6977\n",
      "\n",
      "Epoch 00993: val_loss improved from 0.39203 to 0.39195, saving model to DeepFM.h5\n",
      "Epoch 994/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3674 - auc: 0.755 - ETA: 0s - loss: 0.3893 - auc: 0.742 - 0s 20us/step - loss: 0.3849 - auc: 0.7357 - val_loss: 0.3919 - val_auc: 0.6981\n",
      "\n",
      "Epoch 00994: val_loss improved from 0.39195 to 0.39190, saving model to DeepFM.h5\n",
      "Epoch 995/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3869 - auc: 0.793 - ETA: 0s - loss: 0.3892 - auc: 0.726 - 0s 20us/step - loss: 0.3857 - auc: 0.7298 - val_loss: 0.3918 - val_auc: 0.6988\n",
      "\n",
      "Epoch 00995: val_loss improved from 0.39190 to 0.39178, saving model to DeepFM.h5\n",
      "Epoch 996/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4086 - auc: 0.776 - ETA: 0s - loss: 0.3796 - auc: 0.741 - 0s 23us/step - loss: 0.3839 - auc: 0.7362 - val_loss: 0.3917 - val_auc: 0.6989\n",
      "\n",
      "Epoch 00996: val_loss improved from 0.39178 to 0.39169, saving model to DeepFM.h5\n",
      "Epoch 997/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4141 - auc: 0.699 - ETA: 0s - loss: 0.3895 - auc: 0.718 - 0s 20us/step - loss: 0.3876 - auc: 0.7225 - val_loss: 0.3916 - val_auc: 0.6987\n",
      "\n",
      "Epoch 00997: val_loss improved from 0.39169 to 0.39160, saving model to DeepFM.h5\n",
      "Epoch 998/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3395 - auc: 0.676 - ETA: 0s - loss: 0.3762 - auc: 0.728 - 0s 19us/step - loss: 0.3819 - auc: 0.7393 - val_loss: 0.3915 - val_auc: 0.7001\n",
      "\n",
      "Epoch 00998: val_loss improved from 0.39160 to 0.39150, saving model to DeepFM.h5\n",
      "Epoch 999/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3905 - auc: 0.718 - ETA: 0s - loss: 0.3755 - auc: 0.740 - 0s 21us/step - loss: 0.3851 - auc: 0.7357 - val_loss: 0.3914 - val_auc: 0.7000\n",
      "\n",
      "Epoch 00999: val_loss improved from 0.39150 to 0.39143, saving model to DeepFM.h5\n",
      "Epoch 1000/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3819 - auc: 0.714 - ETA: 0s - loss: 0.3776 - auc: 0.739 - 0s 22us/step - loss: 0.3839 - auc: 0.7341 - val_loss: 0.3914 - val_auc: 0.6996\n",
      "\n",
      "Epoch 01000: val_loss improved from 0.39143 to 0.39135, saving model to DeepFM.h5\n",
      "Epoch 1001/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3461 - auc: 0.762 - ETA: 0s - loss: 0.3852 - auc: 0.742 - 0s 22us/step - loss: 0.3827 - auc: 0.7398 - val_loss: 0.3913 - val_auc: 0.7001\n",
      "\n",
      "Epoch 01001: val_loss improved from 0.39135 to 0.39128, saving model to DeepFM.h5\n",
      "Epoch 1002/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3671 - auc: 0.629 - ETA: 0s - loss: 0.3868 - auc: 0.725 - 0s 22us/step - loss: 0.3845 - auc: 0.7326 - val_loss: 0.3912 - val_auc: 0.7008\n",
      "\n",
      "Epoch 01002: val_loss improved from 0.39128 to 0.39123, saving model to DeepFM.h5\n",
      "Epoch 1003/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4114 - auc: 0.691 - ETA: 0s - loss: 0.3798 - auc: 0.720 - 0s 21us/step - loss: 0.3843 - auc: 0.7304 - val_loss: 0.3912 - val_auc: 0.7016\n",
      "\n",
      "Epoch 01003: val_loss improved from 0.39123 to 0.39117, saving model to DeepFM.h5\n",
      "Epoch 1004/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4281 - auc: 0.748 - ETA: 0s - loss: 0.3773 - auc: 0.737 - 0s 22us/step - loss: 0.3840 - auc: 0.7381 - val_loss: 0.3911 - val_auc: 0.7018\n",
      "\n",
      "Epoch 01004: val_loss improved from 0.39117 to 0.39107, saving model to DeepFM.h5\n",
      "Epoch 1005/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3945 - auc: 0.686 - ETA: 0s - loss: 0.3858 - auc: 0.727 - 0s 22us/step - loss: 0.3845 - auc: 0.7292 - val_loss: 0.3910 - val_auc: 0.7016\n",
      "\n",
      "Epoch 01005: val_loss improved from 0.39107 to 0.39100, saving model to DeepFM.h5\n",
      "Epoch 1006/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3834 - auc: 0.756 - ETA: 0s - loss: 0.3708 - auc: 0.742 - 0s 21us/step - loss: 0.3839 - auc: 0.7325 - val_loss: 0.3909 - val_auc: 0.7019\n",
      "\n",
      "Epoch 01006: val_loss improved from 0.39100 to 0.39090, saving model to DeepFM.h5\n",
      "Epoch 1007/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4039 - auc: 0.695 - ETA: 0s - loss: 0.3949 - auc: 0.731 - 0s 20us/step - loss: 0.3823 - auc: 0.7364 - val_loss: 0.3908 - val_auc: 0.7038\n",
      "\n",
      "Epoch 01007: val_loss improved from 0.39090 to 0.39085, saving model to DeepFM.h5\n",
      "Epoch 1008/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4389 - auc: 0.719 - ETA: 0s - loss: 0.3846 - auc: 0.730 - 0s 20us/step - loss: 0.3852 - auc: 0.7294 - val_loss: 0.3907 - val_auc: 0.7028\n",
      "\n",
      "Epoch 01008: val_loss improved from 0.39085 to 0.39072, saving model to DeepFM.h5\n",
      "Epoch 1009/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3623 - auc: 0.684 - ETA: 0s - loss: 0.3876 - auc: 0.726 - 0s 21us/step - loss: 0.3846 - auc: 0.7288 - val_loss: 0.3907 - val_auc: 0.7042\n",
      "\n",
      "Epoch 01009: val_loss improved from 0.39072 to 0.39069, saving model to DeepFM.h5\n",
      "Epoch 1010/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3855 - auc: 0.753 - ETA: 0s - loss: 0.3775 - auc: 0.735 - 0s 21us/step - loss: 0.3836 - auc: 0.7344 - val_loss: 0.3906 - val_auc: 0.7034\n",
      "\n",
      "Epoch 01010: val_loss improved from 0.39069 to 0.39059, saving model to DeepFM.h5\n",
      "Epoch 1011/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4131 - auc: 0.738 - ETA: 0s - loss: 0.3946 - auc: 0.725 - 0s 20us/step - loss: 0.3824 - auc: 0.7363 - val_loss: 0.3905 - val_auc: 0.7045\n",
      "\n",
      "Epoch 01011: val_loss improved from 0.39059 to 0.39052, saving model to DeepFM.h5\n",
      "Epoch 1012/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3456 - auc: 0.783 - ETA: 0s - loss: 0.3804 - auc: 0.732 - 0s 22us/step - loss: 0.3840 - auc: 0.7308 - val_loss: 0.3904 - val_auc: 0.7043\n",
      "\n",
      "Epoch 01012: val_loss improved from 0.39052 to 0.39044, saving model to DeepFM.h5\n",
      "Epoch 1013/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3997 - auc: 0.726 - ETA: 0s - loss: 0.3874 - auc: 0.741 - 0s 21us/step - loss: 0.3847 - auc: 0.7314 - val_loss: 0.3903 - val_auc: 0.7036\n",
      "\n",
      "Epoch 01013: val_loss improved from 0.39044 to 0.39034, saving model to DeepFM.h5\n",
      "Epoch 1014/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3912 - auc: 0.658 - ETA: 0s - loss: 0.3915 - auc: 0.720 - 0s 21us/step - loss: 0.3845 - auc: 0.7307 - val_loss: 0.3903 - val_auc: 0.7048\n",
      "\n",
      "Epoch 01014: val_loss improved from 0.39034 to 0.39033, saving model to DeepFM.h5\n",
      "Epoch 1015/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3249 - auc: 0.717 - ETA: 0s - loss: 0.3750 - auc: 0.729 - 0s 21us/step - loss: 0.3850 - auc: 0.7262 - val_loss: 0.3902 - val_auc: 0.7044\n",
      "\n",
      "Epoch 01015: val_loss improved from 0.39033 to 0.39023, saving model to DeepFM.h5\n",
      "Epoch 1016/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3042 - auc: 0.822 - ETA: 0s - loss: 0.3856 - auc: 0.741 - 0s 20us/step - loss: 0.3800 - auc: 0.7453 - val_loss: 0.3901 - val_auc: 0.7037\n",
      "\n",
      "Epoch 01016: val_loss improved from 0.39023 to 0.39015, saving model to DeepFM.h5\n",
      "Epoch 1017/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3706 - auc: 0.755 - ETA: 0s - loss: 0.3880 - auc: 0.739 - 0s 22us/step - loss: 0.3823 - auc: 0.7404 - val_loss: 0.3901 - val_auc: 0.7055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01017: val_loss improved from 0.39015 to 0.39012, saving model to DeepFM.h5\n",
      "Epoch 1018/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3585 - auc: 0.752 - ETA: 0s - loss: 0.3824 - auc: 0.720 - 0s 24us/step - loss: 0.3860 - auc: 0.7273 - val_loss: 0.3900 - val_auc: 0.7048\n",
      "\n",
      "Epoch 01018: val_loss improved from 0.39012 to 0.39001, saving model to DeepFM.h5\n",
      "Epoch 1019/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3185 - auc: 0.830 - ETA: 0s - loss: 0.3888 - auc: 0.744 - 0s 21us/step - loss: 0.3809 - auc: 0.7449 - val_loss: 0.3899 - val_auc: 0.7051\n",
      "\n",
      "Epoch 01019: val_loss improved from 0.39001 to 0.38992, saving model to DeepFM.h5\n",
      "Epoch 1020/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3735 - auc: 0.739 - ETA: 0s - loss: 0.3855 - auc: 0.742 - 0s 23us/step - loss: 0.3814 - auc: 0.7375 - val_loss: 0.3899 - val_auc: 0.7077\n",
      "\n",
      "Epoch 01020: val_loss improved from 0.38992 to 0.38989, saving model to DeepFM.h5\n",
      "Epoch 1021/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3997 - auc: 0.676 - ETA: 0s - loss: 0.3780 - auc: 0.729 - 0s 21us/step - loss: 0.3841 - auc: 0.7337 - val_loss: 0.3897 - val_auc: 0.7057\n",
      "\n",
      "Epoch 01021: val_loss improved from 0.38989 to 0.38974, saving model to DeepFM.h5\n",
      "Epoch 1022/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2955 - auc: 0.740 - ETA: 0s - loss: 0.3806 - auc: 0.743 - 0s 21us/step - loss: 0.3811 - auc: 0.7388 - val_loss: 0.3897 - val_auc: 0.7053\n",
      "\n",
      "Epoch 01022: val_loss improved from 0.38974 to 0.38969, saving model to DeepFM.h5\n",
      "Epoch 1023/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3481 - auc: 0.760 - ETA: 0s - loss: 0.3838 - auc: 0.731 - 0s 21us/step - loss: 0.3820 - auc: 0.7387 - val_loss: 0.3896 - val_auc: 0.7076\n",
      "\n",
      "Epoch 01023: val_loss improved from 0.38969 to 0.38959, saving model to DeepFM.h5\n",
      "Epoch 1024/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3807 - auc: 0.722 - ETA: 0s - loss: 0.3866 - auc: 0.722 - 0s 21us/step - loss: 0.3866 - auc: 0.7196 - val_loss: 0.3895 - val_auc: 0.7078\n",
      "\n",
      "Epoch 01024: val_loss improved from 0.38959 to 0.38949, saving model to DeepFM.h5\n",
      "Epoch 1025/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3612 - auc: 0.762 - ETA: 0s - loss: 0.3846 - auc: 0.751 - 0s 21us/step - loss: 0.3801 - auc: 0.7436 - val_loss: 0.3894 - val_auc: 0.7079\n",
      "\n",
      "Epoch 01025: val_loss improved from 0.38949 to 0.38944, saving model to DeepFM.h5\n",
      "Epoch 1026/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4150 - auc: 0.793 - ETA: 0s - loss: 0.3814 - auc: 0.751 - 0s 21us/step - loss: 0.3794 - auc: 0.7500 - val_loss: 0.3894 - val_auc: 0.7070\n",
      "\n",
      "Epoch 01026: val_loss improved from 0.38944 to 0.38938, saving model to DeepFM.h5\n",
      "Epoch 1027/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3270 - auc: 0.787 - ETA: 0s - loss: 0.3883 - auc: 0.738 - 0s 22us/step - loss: 0.3814 - auc: 0.7373 - val_loss: 0.3893 - val_auc: 0.7086\n",
      "\n",
      "Epoch 01027: val_loss improved from 0.38938 to 0.38934, saving model to DeepFM.h5\n",
      "Epoch 1028/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3536 - auc: 0.789 - ETA: 0s - loss: 0.3839 - auc: 0.745 - 0s 21us/step - loss: 0.3812 - auc: 0.7367 - val_loss: 0.3893 - val_auc: 0.7083\n",
      "\n",
      "Epoch 01028: val_loss improved from 0.38934 to 0.38931, saving model to DeepFM.h5\n",
      "Epoch 1029/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3781 - auc: 0.800 - ETA: 0s - loss: 0.3842 - auc: 0.733 - 0s 23us/step - loss: 0.3818 - auc: 0.7345 - val_loss: 0.3892 - val_auc: 0.7093\n",
      "\n",
      "Epoch 01029: val_loss improved from 0.38931 to 0.38921, saving model to DeepFM.h5\n",
      "Epoch 1030/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4163 - auc: 0.722 - ETA: 0s - loss: 0.3890 - auc: 0.736 - 0s 21us/step - loss: 0.3829 - auc: 0.7371 - val_loss: 0.3891 - val_auc: 0.7094\n",
      "\n",
      "Epoch 01030: val_loss improved from 0.38921 to 0.38911, saving model to DeepFM.h5\n",
      "Epoch 1031/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3570 - auc: 0.766 - ETA: 0s - loss: 0.3801 - auc: 0.734 - 0s 22us/step - loss: 0.3796 - auc: 0.7423 - val_loss: 0.3890 - val_auc: 0.7094\n",
      "\n",
      "Epoch 01031: val_loss improved from 0.38911 to 0.38904, saving model to DeepFM.h5\n",
      "Epoch 1032/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4786 - auc: 0.666 - ETA: 0s - loss: 0.3715 - auc: 0.738 - 0s 21us/step - loss: 0.3812 - auc: 0.7382 - val_loss: 0.3889 - val_auc: 0.7099\n",
      "\n",
      "Epoch 01032: val_loss improved from 0.38904 to 0.38890, saving model to DeepFM.h5\n",
      "Epoch 1033/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4128 - auc: 0.766 - ETA: 0s - loss: 0.3813 - auc: 0.730 - 0s 21us/step - loss: 0.3815 - auc: 0.7349 - val_loss: 0.3888 - val_auc: 0.7101\n",
      "\n",
      "Epoch 01033: val_loss improved from 0.38890 to 0.38878, saving model to DeepFM.h5\n",
      "Epoch 1034/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3613 - auc: 0.739 - ETA: 0s - loss: 0.3915 - auc: 0.718 - 0s 20us/step - loss: 0.3834 - auc: 0.7314 - val_loss: 0.3887 - val_auc: 0.7105\n",
      "\n",
      "Epoch 01034: val_loss improved from 0.38878 to 0.38871, saving model to DeepFM.h5\n",
      "Epoch 1035/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3871 - auc: 0.751 - ETA: 0s - loss: 0.3823 - auc: 0.722 - 0s 21us/step - loss: 0.3820 - auc: 0.7364 - val_loss: 0.3887 - val_auc: 0.7105\n",
      "\n",
      "Epoch 01035: val_loss improved from 0.38871 to 0.38866, saving model to DeepFM.h5\n",
      "Epoch 1036/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3983 - auc: 0.773 - ETA: 0s - loss: 0.3813 - auc: 0.729 - 0s 22us/step - loss: 0.3823 - auc: 0.7337 - val_loss: 0.3886 - val_auc: 0.7101\n",
      "\n",
      "Epoch 01036: val_loss improved from 0.38866 to 0.38863, saving model to DeepFM.h5\n",
      "Epoch 1037/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3777 - auc: 0.729 - ETA: 0s - loss: 0.3851 - auc: 0.715 - 0s 23us/step - loss: 0.3821 - auc: 0.7367 - val_loss: 0.3885 - val_auc: 0.7112\n",
      "\n",
      "Epoch 01037: val_loss improved from 0.38863 to 0.38853, saving model to DeepFM.h5\n",
      "Epoch 1038/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3543 - auc: 0.715 - ETA: 0s - loss: 0.3860 - auc: 0.732 - 0s 22us/step - loss: 0.3797 - auc: 0.7460 - val_loss: 0.3884 - val_auc: 0.7114\n",
      "\n",
      "Epoch 01038: val_loss improved from 0.38853 to 0.38842, saving model to DeepFM.h5\n",
      "Epoch 1039/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3303 - auc: 0.760 - ETA: 0s - loss: 0.3800 - auc: 0.740 - 0s 22us/step - loss: 0.3799 - auc: 0.7411 - val_loss: 0.3884 - val_auc: 0.7112\n",
      "\n",
      "Epoch 01039: val_loss improved from 0.38842 to 0.38838, saving model to DeepFM.h5\n",
      "Epoch 1040/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3750 - auc: 0.751 - ETA: 0s - loss: 0.3769 - auc: 0.753 - 0s 21us/step - loss: 0.3821 - auc: 0.7416 - val_loss: 0.3883 - val_auc: 0.7114\n",
      "\n",
      "Epoch 01040: val_loss improved from 0.38838 to 0.38827, saving model to DeepFM.h5\n",
      "Epoch 1041/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3669 - auc: 0.771 - ETA: 0s - loss: 0.3789 - auc: 0.740 - 0s 21us/step - loss: 0.3810 - auc: 0.7379 - val_loss: 0.3882 - val_auc: 0.7123\n",
      "\n",
      "Epoch 01041: val_loss improved from 0.38827 to 0.38820, saving model to DeepFM.h5\n",
      "Epoch 1042/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3756 - auc: 0.750 - ETA: 0s - loss: 0.3838 - auc: 0.740 - 0s 21us/step - loss: 0.3817 - auc: 0.7365 - val_loss: 0.3881 - val_auc: 0.7119\n",
      "\n",
      "Epoch 01042: val_loss improved from 0.38820 to 0.38808, saving model to DeepFM.h5\n",
      "Epoch 1043/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4037 - auc: 0.771 - ETA: 0s - loss: 0.3784 - auc: 0.751 - 0s 21us/step - loss: 0.3787 - auc: 0.7485 - val_loss: 0.3880 - val_auc: 0.7121\n",
      "\n",
      "Epoch 01043: val_loss improved from 0.38808 to 0.38804, saving model to DeepFM.h5\n",
      "Epoch 1044/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3651 - auc: 0.720 - ETA: 0s - loss: 0.3783 - auc: 0.745 - 0s 22us/step - loss: 0.3795 - auc: 0.7435 - val_loss: 0.3879 - val_auc: 0.7123\n",
      "\n",
      "Epoch 01044: val_loss improved from 0.38804 to 0.38794, saving model to DeepFM.h5\n",
      "Epoch 1045/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4066 - auc: 0.757 - ETA: 0s - loss: 0.3801 - auc: 0.744 - 0s 21us/step - loss: 0.3808 - auc: 0.7397 - val_loss: 0.3879 - val_auc: 0.7124\n",
      "\n",
      "Epoch 01045: val_loss improved from 0.38794 to 0.38788, saving model to DeepFM.h5\n",
      "Epoch 1046/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4213 - auc: 0.690 - ETA: 0s - loss: 0.3854 - auc: 0.736 - 0s 21us/step - loss: 0.3817 - auc: 0.7374 - val_loss: 0.3878 - val_auc: 0.7128\n",
      "\n",
      "Epoch 01046: val_loss improved from 0.38788 to 0.38782, saving model to DeepFM.h5\n",
      "Epoch 1047/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3475 - auc: 0.791 - ETA: 0s - loss: 0.3720 - auc: 0.767 - 0s 22us/step - loss: 0.3781 - auc: 0.7566 - val_loss: 0.3877 - val_auc: 0.7133\n",
      "\n",
      "Epoch 01047: val_loss improved from 0.38782 to 0.38772, saving model to DeepFM.h5\n",
      "Epoch 1048/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3941 - auc: 0.676 - ETA: 0s - loss: 0.3860 - auc: 0.726 - 0s 21us/step - loss: 0.3815 - auc: 0.7360 - val_loss: 0.3876 - val_auc: 0.7142\n",
      "\n",
      "Epoch 01048: val_loss improved from 0.38772 to 0.38764, saving model to DeepFM.h5\n",
      "Epoch 1049/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4052 - auc: 0.717 - ETA: 0s - loss: 0.3805 - auc: 0.726 - 0s 21us/step - loss: 0.3807 - auc: 0.7372 - val_loss: 0.3876 - val_auc: 0.7141\n",
      "\n",
      "Epoch 01049: val_loss improved from 0.38764 to 0.38758, saving model to DeepFM.h5\n",
      "Epoch 1050/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3334 - auc: 0.809 - ETA: 0s - loss: 0.3825 - auc: 0.730 - 0s 20us/step - loss: 0.3825 - auc: 0.7302 - val_loss: 0.3874 - val_auc: 0.7141\n",
      "\n",
      "Epoch 01050: val_loss improved from 0.38758 to 0.38744, saving model to DeepFM.h5\n",
      "Epoch 1051/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3551 - auc: 0.702 - ETA: 0s - loss: 0.3819 - auc: 0.738 - 0s 24us/step - loss: 0.3811 - auc: 0.7360 - val_loss: 0.3874 - val_auc: 0.7140\n",
      "\n",
      "Epoch 01051: val_loss improved from 0.38744 to 0.38739, saving model to DeepFM.h5\n",
      "Epoch 1052/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3354 - auc: 0.827 - ETA: 0s - loss: 0.3767 - auc: 0.746 - 0s 22us/step - loss: 0.3811 - auc: 0.7395 - val_loss: 0.3873 - val_auc: 0.7142\n",
      "\n",
      "Epoch 01052: val_loss improved from 0.38739 to 0.38734, saving model to DeepFM.h5\n",
      "Epoch 1053/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3905 - auc: 0.681 - ETA: 0s - loss: 0.3653 - auc: 0.762 - 0s 22us/step - loss: 0.3768 - auc: 0.7530 - val_loss: 0.3873 - val_auc: 0.7155\n",
      "\n",
      "Epoch 01053: val_loss improved from 0.38734 to 0.38728, saving model to DeepFM.h5\n",
      "Epoch 1054/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3449 - auc: 0.727 - ETA: 0s - loss: 0.3727 - auc: 0.758 - 0s 22us/step - loss: 0.3809 - auc: 0.7423 - val_loss: 0.3872 - val_auc: 0.7155\n",
      "\n",
      "Epoch 01054: val_loss improved from 0.38728 to 0.38721, saving model to DeepFM.h5\n",
      "Epoch 1055/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3594 - auc: 0.759 - ETA: 0s - loss: 0.3762 - auc: 0.766 - 0s 21us/step - loss: 0.3751 - auc: 0.7627 - val_loss: 0.3871 - val_auc: 0.7161\n",
      "\n",
      "Epoch 01055: val_loss improved from 0.38721 to 0.38712, saving model to DeepFM.h5\n",
      "Epoch 1056/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3654 - auc: 0.654 - ETA: 0s - loss: 0.3777 - auc: 0.737 - 0s 21us/step - loss: 0.3831 - auc: 0.7320 - val_loss: 0.3870 - val_auc: 0.7152\n",
      "\n",
      "Epoch 01056: val_loss improved from 0.38712 to 0.38697, saving model to DeepFM.h5\n",
      "Epoch 1057/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3366 - auc: 0.809 - ETA: 0s - loss: 0.3789 - auc: 0.747 - 0s 22us/step - loss: 0.3779 - auc: 0.7491 - val_loss: 0.3869 - val_auc: 0.7147\n",
      "\n",
      "Epoch 01057: val_loss improved from 0.38697 to 0.38691, saving model to DeepFM.h5\n",
      "Epoch 1058/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3226 - auc: 0.782 - ETA: 0s - loss: 0.3784 - auc: 0.748 - 0s 21us/step - loss: 0.3790 - auc: 0.7502 - val_loss: 0.3868 - val_auc: 0.7145\n",
      "\n",
      "Epoch 01058: val_loss improved from 0.38691 to 0.38680, saving model to DeepFM.h5\n",
      "Epoch 1059/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3887 - auc: 0.736 - ETA: 0s - loss: 0.3702 - auc: 0.754 - 0s 21us/step - loss: 0.3777 - auc: 0.7504 - val_loss: 0.3867 - val_auc: 0.7151\n",
      "\n",
      "Epoch 01059: val_loss improved from 0.38680 to 0.38673, saving model to DeepFM.h5\n",
      "Epoch 1060/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3612 - auc: 0.687 - ETA: 0s - loss: 0.3713 - auc: 0.742 - 0s 22us/step - loss: 0.3760 - auc: 0.7552 - val_loss: 0.3866 - val_auc: 0.7167\n",
      "\n",
      "Epoch 01060: val_loss improved from 0.38673 to 0.38661, saving model to DeepFM.h5\n",
      "Epoch 1061/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4026 - auc: 0.722 - ETA: 0s - loss: 0.3796 - auc: 0.730 - 0s 21us/step - loss: 0.3795 - auc: 0.7396 - val_loss: 0.3865 - val_auc: 0.7166\n",
      "\n",
      "Epoch 01061: val_loss improved from 0.38661 to 0.38655, saving model to DeepFM.h5\n",
      "Epoch 1062/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4213 - auc: 0.742 - ETA: 0s - loss: 0.3810 - auc: 0.765 - 0s 22us/step - loss: 0.3754 - auc: 0.7578 - val_loss: 0.3865 - val_auc: 0.7177\n",
      "\n",
      "Epoch 01062: val_loss improved from 0.38655 to 0.38652, saving model to DeepFM.h5\n",
      "Epoch 1063/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4361 - auc: 0.773 - ETA: 0s - loss: 0.3728 - auc: 0.749 - 0s 22us/step - loss: 0.3752 - auc: 0.7499 - val_loss: 0.3865 - val_auc: 0.7175\n",
      "\n",
      "Epoch 01063: val_loss improved from 0.38652 to 0.38649, saving model to DeepFM.h5\n",
      "Epoch 1064/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3738 - auc: 0.765 - ETA: 0s - loss: 0.3850 - auc: 0.746 - 0s 22us/step - loss: 0.3794 - auc: 0.7455 - val_loss: 0.3864 - val_auc: 0.7174\n",
      "\n",
      "Epoch 01064: val_loss improved from 0.38649 to 0.38644, saving model to DeepFM.h5\n",
      "Epoch 1065/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3463 - auc: 0.760 - ETA: 0s - loss: 0.3781 - auc: 0.745 - 0s 22us/step - loss: 0.3770 - auc: 0.7484 - val_loss: 0.3863 - val_auc: 0.7180\n",
      "\n",
      "Epoch 01065: val_loss improved from 0.38644 to 0.38629, saving model to DeepFM.h5\n",
      "Epoch 1066/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3642 - auc: 0.849 - ETA: 0s - loss: 0.3746 - auc: 0.752 - 0s 21us/step - loss: 0.3754 - auc: 0.7535 - val_loss: 0.3862 - val_auc: 0.7191\n",
      "\n",
      "Epoch 01066: val_loss improved from 0.38629 to 0.38620, saving model to DeepFM.h5\n",
      "Epoch 1067/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4011 - auc: 0.747 - ETA: 0s - loss: 0.3749 - auc: 0.762 - 0s 21us/step - loss: 0.3754 - auc: 0.7570 - val_loss: 0.3861 - val_auc: 0.7182\n",
      "\n",
      "Epoch 01067: val_loss improved from 0.38620 to 0.38611, saving model to DeepFM.h5\n",
      "Epoch 1068/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4340 - auc: 0.744 - ETA: 0s - loss: 0.3833 - auc: 0.745 - 0s 21us/step - loss: 0.3787 - auc: 0.7456 - val_loss: 0.3861 - val_auc: 0.7185\n",
      "\n",
      "Epoch 01068: val_loss improved from 0.38611 to 0.38609, saving model to DeepFM.h5\n",
      "Epoch 1069/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3060 - auc: 0.851 - ETA: 0s - loss: 0.3829 - auc: 0.768 - 0s 22us/step - loss: 0.3700 - auc: 0.7744 - val_loss: 0.3860 - val_auc: 0.7194\n",
      "\n",
      "Epoch 01069: val_loss improved from 0.38609 to 0.38604, saving model to DeepFM.h5\n",
      "Epoch 1070/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3476 - auc: 0.733 - ETA: 0s - loss: 0.3774 - auc: 0.734 - 0s 22us/step - loss: 0.3782 - auc: 0.7458 - val_loss: 0.3858 - val_auc: 0.7186\n",
      "\n",
      "Epoch 01070: val_loss improved from 0.38604 to 0.38585, saving model to DeepFM.h5\n",
      "Epoch 1071/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3243 - auc: 0.783 - ETA: 0s - loss: 0.3794 - auc: 0.757 - 0s 21us/step - loss: 0.3777 - auc: 0.7520 - val_loss: 0.3858 - val_auc: 0.7186\n",
      "\n",
      "Epoch 01071: val_loss improved from 0.38585 to 0.38578, saving model to DeepFM.h5\n",
      "Epoch 1072/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4207 - auc: 0.661 - ETA: 0s - loss: 0.3862 - auc: 0.721 - 0s 23us/step - loss: 0.3796 - auc: 0.7398 - val_loss: 0.3858 - val_auc: 0.7202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01072: val_loss did not improve from 0.38578\n",
      "Epoch 1073/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3705 - auc: 0.777 - ETA: 0s - loss: 0.3703 - auc: 0.763 - 0s 21us/step - loss: 0.3758 - auc: 0.7536 - val_loss: 0.3857 - val_auc: 0.7185\n",
      "\n",
      "Epoch 01073: val_loss improved from 0.38578 to 0.38567, saving model to DeepFM.h5\n",
      "Epoch 1074/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3739 - auc: 0.700 - ETA: 0s - loss: 0.3631 - auc: 0.780 - 0s 22us/step - loss: 0.3746 - auc: 0.7617 - val_loss: 0.3856 - val_auc: 0.7188\n",
      "\n",
      "Epoch 01074: val_loss improved from 0.38567 to 0.38559, saving model to DeepFM.h5\n",
      "Epoch 1075/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.5009 - auc: 0.661 - ETA: 0s - loss: 0.3919 - auc: 0.736 - 0s 21us/step - loss: 0.3798 - auc: 0.7443 - val_loss: 0.3856 - val_auc: 0.7194\n",
      "\n",
      "Epoch 01075: val_loss improved from 0.38559 to 0.38556, saving model to DeepFM.h5\n",
      "Epoch 1076/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3794 - auc: 0.754 - ETA: 0s - loss: 0.3755 - auc: 0.769 - 0s 21us/step - loss: 0.3728 - auc: 0.7669 - val_loss: 0.3856 - val_auc: 0.7202\n",
      "\n",
      "Epoch 01076: val_loss improved from 0.38556 to 0.38556, saving model to DeepFM.h5\n",
      "Epoch 1077/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3148 - auc: 0.780 - ETA: 0s - loss: 0.3718 - auc: 0.766 - 0s 20us/step - loss: 0.3746 - auc: 0.7590 - val_loss: 0.3854 - val_auc: 0.7207\n",
      "\n",
      "Epoch 01077: val_loss improved from 0.38556 to 0.38545, saving model to DeepFM.h5\n",
      "Epoch 1078/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4029 - auc: 0.804 - ETA: 0s - loss: 0.3736 - auc: 0.749 - 0s 23us/step - loss: 0.3767 - auc: 0.7493 - val_loss: 0.3853 - val_auc: 0.7196\n",
      "\n",
      "Epoch 01078: val_loss improved from 0.38545 to 0.38531, saving model to DeepFM.h5\n",
      "Epoch 1079/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4334 - auc: 0.712 - ETA: 0s - loss: 0.3763 - auc: 0.751 - 0s 21us/step - loss: 0.3769 - auc: 0.7508 - val_loss: 0.3851 - val_auc: 0.7204\n",
      "\n",
      "Epoch 01079: val_loss improved from 0.38531 to 0.38514, saving model to DeepFM.h5\n",
      "Epoch 1080/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3402 - auc: 0.804 - ETA: 0s - loss: 0.3730 - auc: 0.753 - 0s 22us/step - loss: 0.3764 - auc: 0.7505 - val_loss: 0.3851 - val_auc: 0.7198\n",
      "\n",
      "Epoch 01080: val_loss improved from 0.38514 to 0.38510, saving model to DeepFM.h5\n",
      "Epoch 1081/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3660 - auc: 0.818 - ETA: 0s - loss: 0.3719 - auc: 0.759 - 0s 20us/step - loss: 0.3724 - auc: 0.7609 - val_loss: 0.3851 - val_auc: 0.7214\n",
      "\n",
      "Epoch 01081: val_loss improved from 0.38510 to 0.38508, saving model to DeepFM.h5\n",
      "Epoch 1082/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3934 - auc: 0.684 - ETA: 0s - loss: 0.3842 - auc: 0.742 - 0s 20us/step - loss: 0.3808 - auc: 0.7374 - val_loss: 0.3850 - val_auc: 0.7207\n",
      "\n",
      "Epoch 01082: val_loss improved from 0.38508 to 0.38503, saving model to DeepFM.h5\n",
      "Epoch 1083/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3054 - auc: 0.774 - ETA: 0s - loss: 0.3787 - auc: 0.763 - 0s 20us/step - loss: 0.3721 - auc: 0.7654 - val_loss: 0.3849 - val_auc: 0.7214\n",
      "\n",
      "Epoch 01083: val_loss improved from 0.38503 to 0.38494, saving model to DeepFM.h5\n",
      "Epoch 1084/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3613 - auc: 0.732 - ETA: 0s - loss: 0.3764 - auc: 0.774 - 0s 21us/step - loss: 0.3703 - auc: 0.7733 - val_loss: 0.3849 - val_auc: 0.7206\n",
      "\n",
      "Epoch 01084: val_loss improved from 0.38494 to 0.38493, saving model to DeepFM.h5\n",
      "Epoch 1085/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3993 - auc: 0.717 - ETA: 0s - loss: 0.3688 - auc: 0.754 - 0s 22us/step - loss: 0.3763 - auc: 0.7499 - val_loss: 0.3848 - val_auc: 0.7221\n",
      "\n",
      "Epoch 01085: val_loss improved from 0.38493 to 0.38478, saving model to DeepFM.h5\n",
      "Epoch 1086/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4111 - auc: 0.704 - ETA: 0s - loss: 0.3796 - auc: 0.748 - 0s 23us/step - loss: 0.3764 - auc: 0.7479 - val_loss: 0.3847 - val_auc: 0.7208\n",
      "\n",
      "Epoch 01086: val_loss improved from 0.38478 to 0.38469, saving model to DeepFM.h5\n",
      "Epoch 1087/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3477 - auc: 0.730 - ETA: 0s - loss: 0.3731 - auc: 0.777 - 0s 23us/step - loss: 0.3720 - auc: 0.7672 - val_loss: 0.3847 - val_auc: 0.7214\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.38469\n",
      "Epoch 1088/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4188 - auc: 0.739 - ETA: 0s - loss: 0.3700 - auc: 0.757 - 0s 21us/step - loss: 0.3744 - auc: 0.7577 - val_loss: 0.3846 - val_auc: 0.7221\n",
      "\n",
      "Epoch 01088: val_loss improved from 0.38469 to 0.38458, saving model to DeepFM.h5\n",
      "Epoch 1089/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3841 - auc: 0.730 - ETA: 0s - loss: 0.3716 - auc: 0.763 - 0s 20us/step - loss: 0.3707 - auc: 0.7672 - val_loss: 0.3846 - val_auc: 0.7228\n",
      "\n",
      "Epoch 01089: val_loss improved from 0.38458 to 0.38457, saving model to DeepFM.h5\n",
      "Epoch 1090/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3569 - auc: 0.792 - ETA: 0s - loss: 0.3618 - auc: 0.756 - 0s 21us/step - loss: 0.3726 - auc: 0.7536 - val_loss: 0.3845 - val_auc: 0.7230\n",
      "\n",
      "Epoch 01090: val_loss improved from 0.38457 to 0.38445, saving model to DeepFM.h5\n",
      "Epoch 1091/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3760 - auc: 0.760 - ETA: 0s - loss: 0.3687 - auc: 0.769 - 0s 22us/step - loss: 0.3718 - auc: 0.7618 - val_loss: 0.3844 - val_auc: 0.7231\n",
      "\n",
      "Epoch 01091: val_loss improved from 0.38445 to 0.38439, saving model to DeepFM.h5\n",
      "Epoch 1092/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4010 - auc: 0.789 - ETA: 0s - loss: 0.3624 - auc: 0.776 - 0s 22us/step - loss: 0.3721 - auc: 0.7654 - val_loss: 0.3843 - val_auc: 0.7234\n",
      "\n",
      "Epoch 01092: val_loss improved from 0.38439 to 0.38427, saving model to DeepFM.h5\n",
      "Epoch 1093/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4592 - auc: 0.699 - ETA: 0s - loss: 0.3743 - auc: 0.759 - 0s 22us/step - loss: 0.3742 - auc: 0.7565 - val_loss: 0.3842 - val_auc: 0.7233\n",
      "\n",
      "Epoch 01093: val_loss improved from 0.38427 to 0.38422, saving model to DeepFM.h5\n",
      "Epoch 1094/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3673 - auc: 0.775 - ETA: 0s - loss: 0.3780 - auc: 0.750 - 0s 21us/step - loss: 0.3749 - auc: 0.7567 - val_loss: 0.3841 - val_auc: 0.7229\n",
      "\n",
      "Epoch 01094: val_loss improved from 0.38422 to 0.38415, saving model to DeepFM.h5\n",
      "Epoch 1095/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4406 - auc: 0.703 - ETA: 0s - loss: 0.3874 - auc: 0.737 - 0s 23us/step - loss: 0.3731 - auc: 0.7602 - val_loss: 0.3841 - val_auc: 0.7235\n",
      "\n",
      "Epoch 01095: val_loss improved from 0.38415 to 0.38410, saving model to DeepFM.h5\n",
      "Epoch 1096/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3720 - auc: 0.712 - ETA: 0s - loss: 0.3654 - auc: 0.768 - 0s 21us/step - loss: 0.3706 - auc: 0.7661 - val_loss: 0.3840 - val_auc: 0.7233\n",
      "\n",
      "Epoch 01096: val_loss improved from 0.38410 to 0.38400, saving model to DeepFM.h5\n",
      "Epoch 1097/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3544 - auc: 0.771 - ETA: 0s - loss: 0.3887 - auc: 0.757 - 0s 22us/step - loss: 0.3731 - auc: 0.7573 - val_loss: 0.3840 - val_auc: 0.7238\n",
      "\n",
      "Epoch 01097: val_loss improved from 0.38400 to 0.38398, saving model to DeepFM.h5\n",
      "Epoch 1098/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3506 - auc: 0.736 - ETA: 0s - loss: 0.3635 - auc: 0.762 - 0s 22us/step - loss: 0.3705 - auc: 0.7675 - val_loss: 0.3839 - val_auc: 0.7230\n",
      "\n",
      "Epoch 01098: val_loss improved from 0.38398 to 0.38386, saving model to DeepFM.h5\n",
      "Epoch 1099/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3500 - auc: 0.817 - ETA: 0s - loss: 0.3705 - auc: 0.763 - 0s 20us/step - loss: 0.3721 - auc: 0.7601 - val_loss: 0.3838 - val_auc: 0.7239\n",
      "\n",
      "Epoch 01099: val_loss improved from 0.38386 to 0.38382, saving model to DeepFM.h5\n",
      "Epoch 1100/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3943 - auc: 0.784 - ETA: 0s - loss: 0.3721 - auc: 0.754 - 0s 21us/step - loss: 0.3739 - auc: 0.7574 - val_loss: 0.3837 - val_auc: 0.7235\n",
      "\n",
      "Epoch 01100: val_loss improved from 0.38382 to 0.38367, saving model to DeepFM.h5\n",
      "Epoch 1101/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4141 - auc: 0.760 - ETA: 0s - loss: 0.3642 - auc: 0.755 - 0s 23us/step - loss: 0.3727 - auc: 0.7549 - val_loss: 0.3835 - val_auc: 0.7239\n",
      "\n",
      "Epoch 01101: val_loss improved from 0.38367 to 0.38355, saving model to DeepFM.h5\n",
      "Epoch 1102/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4324 - auc: 0.711 - ETA: 0s - loss: 0.3673 - auc: 0.745 - 0s 22us/step - loss: 0.3746 - auc: 0.7498 - val_loss: 0.3835 - val_auc: 0.7234\n",
      "\n",
      "Epoch 01102: val_loss improved from 0.38355 to 0.38348, saving model to DeepFM.h5\n",
      "Epoch 1103/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3691 - auc: 0.787 - ETA: 0s - loss: 0.3776 - auc: 0.768 - 0s 21us/step - loss: 0.3734 - auc: 0.7597 - val_loss: 0.3834 - val_auc: 0.7236\n",
      "\n",
      "Epoch 01103: val_loss improved from 0.38348 to 0.38345, saving model to DeepFM.h5\n",
      "Epoch 1104/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4243 - auc: 0.785 - ETA: 0s - loss: 0.3676 - auc: 0.763 - 0s 20us/step - loss: 0.3722 - auc: 0.7573 - val_loss: 0.3834 - val_auc: 0.7239\n",
      "\n",
      "Epoch 01104: val_loss improved from 0.38345 to 0.38339, saving model to DeepFM.h5\n",
      "Epoch 1105/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3030 - auc: 0.845 - ETA: 0s - loss: 0.3690 - auc: 0.757 - 0s 21us/step - loss: 0.3732 - auc: 0.7556 - val_loss: 0.3834 - val_auc: 0.7245\n",
      "\n",
      "Epoch 01105: val_loss improved from 0.38339 to 0.38336, saving model to DeepFM.h5\n",
      "Epoch 1106/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3082 - auc: 0.864 - ETA: 0s - loss: 0.3571 - auc: 0.772 - 0s 22us/step - loss: 0.3712 - auc: 0.7649 - val_loss: 0.3832 - val_auc: 0.7242\n",
      "\n",
      "Epoch 01106: val_loss improved from 0.38336 to 0.38320, saving model to DeepFM.h5\n",
      "Epoch 1107/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4225 - auc: 0.748 - ETA: 0s - loss: 0.3733 - auc: 0.768 - 0s 20us/step - loss: 0.3689 - auc: 0.7721 - val_loss: 0.3831 - val_auc: 0.7247\n",
      "\n",
      "Epoch 01107: val_loss improved from 0.38320 to 0.38312, saving model to DeepFM.h5\n",
      "Epoch 1108/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3844 - auc: 0.801 - ETA: 0s - loss: 0.3633 - auc: 0.777 - 0s 20us/step - loss: 0.3666 - auc: 0.7793 - val_loss: 0.3831 - val_auc: 0.7260\n",
      "\n",
      "Epoch 01108: val_loss improved from 0.38312 to 0.38308, saving model to DeepFM.h5\n",
      "Epoch 1109/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3898 - auc: 0.770 - ETA: 0s - loss: 0.3684 - auc: 0.763 - 0s 20us/step - loss: 0.3716 - auc: 0.7615 - val_loss: 0.3830 - val_auc: 0.7251\n",
      "\n",
      "Epoch 01109: val_loss improved from 0.38308 to 0.38297, saving model to DeepFM.h5\n",
      "Epoch 1110/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4034 - auc: 0.741 - ETA: 0s - loss: 0.3745 - auc: 0.761 - 0s 20us/step - loss: 0.3694 - auc: 0.7697 - val_loss: 0.3830 - val_auc: 0.7279\n",
      "\n",
      "Epoch 01110: val_loss improved from 0.38297 to 0.38295, saving model to DeepFM.h5\n",
      "Epoch 1111/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4207 - auc: 0.732 - ETA: 0s - loss: 0.3698 - auc: 0.768 - 0s 21us/step - loss: 0.3722 - auc: 0.7598 - val_loss: 0.3829 - val_auc: 0.7279\n",
      "\n",
      "Epoch 01111: val_loss improved from 0.38295 to 0.38286, saving model to DeepFM.h5\n",
      "Epoch 1112/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4410 - auc: 0.767 - ETA: 0s - loss: 0.3775 - auc: 0.749 - 0s 21us/step - loss: 0.3738 - auc: 0.7589 - val_loss: 0.3827 - val_auc: 0.7275\n",
      "\n",
      "Epoch 01112: val_loss improved from 0.38286 to 0.38275, saving model to DeepFM.h5\n",
      "Epoch 1113/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3969 - auc: 0.713 - ETA: 0s - loss: 0.3693 - auc: 0.757 - 0s 20us/step - loss: 0.3728 - auc: 0.7568 - val_loss: 0.3826 - val_auc: 0.7274\n",
      "\n",
      "Epoch 01113: val_loss improved from 0.38275 to 0.38264, saving model to DeepFM.h5\n",
      "Epoch 1114/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4096 - auc: 0.761 - ETA: 0s - loss: 0.3670 - auc: 0.765 - 0s 21us/step - loss: 0.3723 - auc: 0.7583 - val_loss: 0.3825 - val_auc: 0.7263\n",
      "\n",
      "Epoch 01114: val_loss improved from 0.38264 to 0.38250, saving model to DeepFM.h5\n",
      "Epoch 1115/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3278 - auc: 0.809 - ETA: 0s - loss: 0.3723 - auc: 0.769 - 0s 19us/step - loss: 0.3711 - auc: 0.7689 - val_loss: 0.3824 - val_auc: 0.7264\n",
      "\n",
      "Epoch 01115: val_loss improved from 0.38250 to 0.38243, saving model to DeepFM.h5\n",
      "Epoch 1116/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3514 - auc: 0.797 - ETA: 0s - loss: 0.3728 - auc: 0.777 - 0s 22us/step - loss: 0.3696 - auc: 0.7639 - val_loss: 0.3824 - val_auc: 0.7273\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.38243\n",
      "Epoch 1117/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3822 - auc: 0.709 - ETA: 0s - loss: 0.3672 - auc: 0.772 - 0s 22us/step - loss: 0.3694 - auc: 0.7680 - val_loss: 0.3824 - val_auc: 0.7286\n",
      "\n",
      "Epoch 01117: val_loss improved from 0.38243 to 0.38238, saving model to DeepFM.h5\n",
      "Epoch 1118/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4167 - auc: 0.791 - ETA: 0s - loss: 0.3740 - auc: 0.770 - 0s 21us/step - loss: 0.3702 - auc: 0.7642 - val_loss: 0.3824 - val_auc: 0.7283\n",
      "\n",
      "Epoch 01118: val_loss improved from 0.38238 to 0.38236, saving model to DeepFM.h5\n",
      "Epoch 1119/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3669 - auc: 0.728 - ETA: 0s - loss: 0.3658 - auc: 0.769 - 0s 20us/step - loss: 0.3680 - auc: 0.7726 - val_loss: 0.3824 - val_auc: 0.7274\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.38236\n",
      "Epoch 1120/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3264 - auc: 0.752 - ETA: 0s - loss: 0.3708 - auc: 0.770 - 0s 22us/step - loss: 0.3670 - auc: 0.7733 - val_loss: 0.3823 - val_auc: 0.7279\n",
      "\n",
      "Epoch 01120: val_loss improved from 0.38236 to 0.38232, saving model to DeepFM.h5\n",
      "Epoch 1121/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3688 - auc: 0.699 - ETA: 0s - loss: 0.3669 - auc: 0.769 - 0s 20us/step - loss: 0.3664 - auc: 0.7776 - val_loss: 0.3822 - val_auc: 0.7270\n",
      "\n",
      "Epoch 01121: val_loss improved from 0.38232 to 0.38220, saving model to DeepFM.h5\n",
      "Epoch 1122/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3447 - auc: 0.818 - ETA: 0s - loss: 0.3598 - auc: 0.780 - 0s 21us/step - loss: 0.3693 - auc: 0.7682 - val_loss: 0.3821 - val_auc: 0.7268\n",
      "\n",
      "Epoch 01122: val_loss improved from 0.38220 to 0.38212, saving model to DeepFM.h5\n",
      "Epoch 1123/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3441 - auc: 0.758 - ETA: 0s - loss: 0.3803 - auc: 0.760 - 0s 20us/step - loss: 0.3705 - auc: 0.7646 - val_loss: 0.3820 - val_auc: 0.7293\n",
      "\n",
      "Epoch 01123: val_loss improved from 0.38212 to 0.38197, saving model to DeepFM.h5\n",
      "Epoch 1124/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4473 - auc: 0.767 - ETA: 0s - loss: 0.3686 - auc: 0.774 - 0s 20us/step - loss: 0.3672 - auc: 0.7721 - val_loss: 0.3820 - val_auc: 0.7272\n",
      "\n",
      "Epoch 01124: val_loss improved from 0.38197 to 0.38196, saving model to DeepFM.h5\n",
      "Epoch 1125/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4265 - auc: 0.716 - ETA: 0s - loss: 0.3661 - auc: 0.767 - 0s 20us/step - loss: 0.3680 - auc: 0.7660 - val_loss: 0.3819 - val_auc: 0.7275\n",
      "\n",
      "Epoch 01125: val_loss improved from 0.38196 to 0.38189, saving model to DeepFM.h5\n",
      "Epoch 1126/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3901 - auc: 0.698 - ETA: 0s - loss: 0.3617 - auc: 0.756 - 0s 21us/step - loss: 0.3695 - auc: 0.7672 - val_loss: 0.3817 - val_auc: 0.7286\n",
      "\n",
      "Epoch 01126: val_loss improved from 0.38189 to 0.38173, saving model to DeepFM.h5\n",
      "Epoch 1127/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3389 - auc: 0.802 - ETA: 0s - loss: 0.3632 - auc: 0.780 - 0s 21us/step - loss: 0.3680 - auc: 0.7720 - val_loss: 0.3816 - val_auc: 0.7299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01127: val_loss improved from 0.38173 to 0.38160, saving model to DeepFM.h5\n",
      "Epoch 1128/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3966 - auc: 0.767 - ETA: 0s - loss: 0.3721 - auc: 0.765 - 0s 21us/step - loss: 0.3700 - auc: 0.7698 - val_loss: 0.3816 - val_auc: 0.7286\n",
      "\n",
      "Epoch 01128: val_loss improved from 0.38160 to 0.38155, saving model to DeepFM.h5\n",
      "Epoch 1129/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3082 - auc: 0.737 - ETA: 0s - loss: 0.3655 - auc: 0.779 - 0s 19us/step - loss: 0.3646 - auc: 0.7832 - val_loss: 0.3815 - val_auc: 0.7300\n",
      "\n",
      "Epoch 01129: val_loss improved from 0.38155 to 0.38147, saving model to DeepFM.h5\n",
      "Epoch 1130/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4196 - auc: 0.733 - ETA: 0s - loss: 0.3750 - auc: 0.759 - 0s 20us/step - loss: 0.3694 - auc: 0.7685 - val_loss: 0.3815 - val_auc: 0.7295\n",
      "\n",
      "Epoch 01130: val_loss improved from 0.38147 to 0.38145, saving model to DeepFM.h5\n",
      "Epoch 1131/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3422 - auc: 0.819 - ETA: 0s - loss: 0.3617 - auc: 0.767 - 0s 21us/step - loss: 0.3707 - auc: 0.7599 - val_loss: 0.3813 - val_auc: 0.7306\n",
      "\n",
      "Epoch 01131: val_loss improved from 0.38145 to 0.38131, saving model to DeepFM.h5\n",
      "Epoch 1132/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3702 - auc: 0.824 - ETA: 0s - loss: 0.3623 - auc: 0.778 - 0s 21us/step - loss: 0.3670 - auc: 0.7720 - val_loss: 0.3813 - val_auc: 0.7302\n",
      "\n",
      "Epoch 01132: val_loss improved from 0.38131 to 0.38128, saving model to DeepFM.h5\n",
      "Epoch 1133/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3193 - auc: 0.791 - ETA: 0s - loss: 0.3602 - auc: 0.780 - 0s 21us/step - loss: 0.3685 - auc: 0.7725 - val_loss: 0.3812 - val_auc: 0.7310\n",
      "\n",
      "Epoch 01133: val_loss improved from 0.38128 to 0.38122, saving model to DeepFM.h5\n",
      "Epoch 1134/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3632 - auc: 0.757 - ETA: 0s - loss: 0.3672 - auc: 0.769 - 0s 21us/step - loss: 0.3665 - auc: 0.7791 - val_loss: 0.3813 - val_auc: 0.7305\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.38122\n",
      "Epoch 1135/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3694 - auc: 0.750 - ETA: 0s - loss: 0.3707 - auc: 0.758 - 0s 22us/step - loss: 0.3698 - auc: 0.7630 - val_loss: 0.3812 - val_auc: 0.7307\n",
      "\n",
      "Epoch 01135: val_loss improved from 0.38122 to 0.38120, saving model to DeepFM.h5\n",
      "Epoch 1136/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3883 - auc: 0.759 - ETA: 0s - loss: 0.3610 - auc: 0.773 - 0s 21us/step - loss: 0.3646 - auc: 0.7820 - val_loss: 0.3811 - val_auc: 0.7312\n",
      "\n",
      "Epoch 01136: val_loss improved from 0.38120 to 0.38109, saving model to DeepFM.h5\n",
      "Epoch 1137/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3514 - auc: 0.761 - ETA: 0s - loss: 0.3649 - auc: 0.770 - 0s 21us/step - loss: 0.3693 - auc: 0.7651 - val_loss: 0.3809 - val_auc: 0.7311\n",
      "\n",
      "Epoch 01137: val_loss improved from 0.38109 to 0.38094, saving model to DeepFM.h5\n",
      "Epoch 1138/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3850 - auc: 0.729 - ETA: 0s - loss: 0.3719 - auc: 0.766 - 0s 20us/step - loss: 0.3676 - auc: 0.7699 - val_loss: 0.3809 - val_auc: 0.7318\n",
      "\n",
      "Epoch 01138: val_loss improved from 0.38094 to 0.38086, saving model to DeepFM.h5\n",
      "Epoch 1139/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3522 - auc: 0.748 - ETA: 0s - loss: 0.3621 - auc: 0.771 - 0s 20us/step - loss: 0.3686 - auc: 0.7663 - val_loss: 0.3807 - val_auc: 0.7327\n",
      "\n",
      "Epoch 01139: val_loss improved from 0.38086 to 0.38071, saving model to DeepFM.h5\n",
      "Epoch 1140/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3453 - auc: 0.766 - ETA: 0s - loss: 0.3757 - auc: 0.773 - 0s 23us/step - loss: 0.3673 - auc: 0.7687 - val_loss: 0.3807 - val_auc: 0.7315\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.38071\n",
      "Epoch 1141/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3783 - auc: 0.825 - ETA: 0s - loss: 0.3595 - auc: 0.765 - 0s 23us/step - loss: 0.3669 - auc: 0.7681 - val_loss: 0.3808 - val_auc: 0.7322\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.38071\n",
      "Epoch 1142/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3252 - auc: 0.802 - ETA: 0s - loss: 0.3712 - auc: 0.779 - 0s 23us/step - loss: 0.3683 - auc: 0.7719 - val_loss: 0.3806 - val_auc: 0.7323\n",
      "\n",
      "Epoch 01142: val_loss improved from 0.38071 to 0.38064, saving model to DeepFM.h5\n",
      "Epoch 1143/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4153 - auc: 0.745 - ETA: 0s - loss: 0.3629 - auc: 0.764 - 0s 20us/step - loss: 0.3703 - auc: 0.7640 - val_loss: 0.3805 - val_auc: 0.7326\n",
      "\n",
      "Epoch 01143: val_loss improved from 0.38064 to 0.38054, saving model to DeepFM.h5\n",
      "Epoch 1144/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3573 - auc: 0.781 - ETA: 0s - loss: 0.3679 - auc: 0.775 - 0s 20us/step - loss: 0.3674 - auc: 0.7720 - val_loss: 0.3806 - val_auc: 0.7319\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.38054\n",
      "Epoch 1145/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4408 - auc: 0.762 - ETA: 0s - loss: 0.3732 - auc: 0.767 - 0s 20us/step - loss: 0.3680 - auc: 0.7697 - val_loss: 0.3806 - val_auc: 0.7327\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.38054\n",
      "Epoch 1146/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3438 - auc: 0.783 - ETA: 0s - loss: 0.3673 - auc: 0.771 - 0s 21us/step - loss: 0.3689 - auc: 0.7618 - val_loss: 0.3805 - val_auc: 0.7333\n",
      "\n",
      "Epoch 01146: val_loss improved from 0.38054 to 0.38052, saving model to DeepFM.h5\n",
      "Epoch 1147/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4690 - auc: 0.741 - ETA: 0s - loss: 0.3658 - auc: 0.769 - 0s 20us/step - loss: 0.3685 - auc: 0.7663 - val_loss: 0.3805 - val_auc: 0.7326\n",
      "\n",
      "Epoch 01147: val_loss improved from 0.38052 to 0.38051, saving model to DeepFM.h5\n",
      "Epoch 1148/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3351 - auc: 0.786 - ETA: 0s - loss: 0.3734 - auc: 0.757 - 0s 20us/step - loss: 0.3669 - auc: 0.7724 - val_loss: 0.3804 - val_auc: 0.7330\n",
      "\n",
      "Epoch 01148: val_loss improved from 0.38051 to 0.38044, saving model to DeepFM.h5\n",
      "Epoch 1149/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4095 - auc: 0.785 - ETA: 0s - loss: 0.3604 - auc: 0.779 - 0s 20us/step - loss: 0.3681 - auc: 0.7660 - val_loss: 0.3804 - val_auc: 0.7339\n",
      "\n",
      "Epoch 01149: val_loss improved from 0.38044 to 0.38042, saving model to DeepFM.h5\n",
      "Epoch 1150/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2891 - auc: 0.821 - ETA: 0s - loss: 0.3580 - auc: 0.775 - 0s 22us/step - loss: 0.3674 - auc: 0.7756 - val_loss: 0.3802 - val_auc: 0.7339\n",
      "\n",
      "Epoch 01150: val_loss improved from 0.38042 to 0.38018, saving model to DeepFM.h5\n",
      "Epoch 1151/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3275 - auc: 0.770 - ETA: 0s - loss: 0.3762 - auc: 0.767 - 0s 20us/step - loss: 0.3664 - auc: 0.7760 - val_loss: 0.3802 - val_auc: 0.7339\n",
      "\n",
      "Epoch 01151: val_loss improved from 0.38018 to 0.38018, saving model to DeepFM.h5\n",
      "Epoch 1152/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3425 - auc: 0.795 - ETA: 0s - loss: 0.3682 - auc: 0.774 - 0s 20us/step - loss: 0.3672 - auc: 0.7743 - val_loss: 0.3800 - val_auc: 0.7343\n",
      "\n",
      "Epoch 01152: val_loss improved from 0.38018 to 0.38005, saving model to DeepFM.h5\n",
      "Epoch 1153/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4043 - auc: 0.752 - ETA: 0s - loss: 0.3753 - auc: 0.768 - 0s 21us/step - loss: 0.3653 - auc: 0.7777 - val_loss: 0.3800 - val_auc: 0.7335\n",
      "\n",
      "Epoch 01153: val_loss improved from 0.38005 to 0.38004, saving model to DeepFM.h5\n",
      "Epoch 1154/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3727 - auc: 0.796 - ETA: 0s - loss: 0.3612 - auc: 0.783 - 0s 22us/step - loss: 0.3632 - auc: 0.7830 - val_loss: 0.3799 - val_auc: 0.7341\n",
      "\n",
      "Epoch 01154: val_loss improved from 0.38004 to 0.37994, saving model to DeepFM.h5\n",
      "Epoch 1155/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3135 - auc: 0.840 - ETA: 0s - loss: 0.3629 - auc: 0.764 - 0s 20us/step - loss: 0.3693 - auc: 0.7653 - val_loss: 0.3798 - val_auc: 0.7336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01155: val_loss improved from 0.37994 to 0.37982, saving model to DeepFM.h5\n",
      "Epoch 1156/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3858 - auc: 0.715 - ETA: 0s - loss: 0.3673 - auc: 0.783 - 0s 20us/step - loss: 0.3645 - auc: 0.7786 - val_loss: 0.3798 - val_auc: 0.7347\n",
      "\n",
      "Epoch 01156: val_loss improved from 0.37982 to 0.37981, saving model to DeepFM.h5\n",
      "Epoch 1157/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3681 - auc: 0.727 - ETA: 0s - loss: 0.3688 - auc: 0.776 - 0s 21us/step - loss: 0.3644 - auc: 0.7780 - val_loss: 0.3798 - val_auc: 0.7357\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.37981\n",
      "Epoch 1158/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3679 - auc: 0.830 - ETA: 0s - loss: 0.3673 - auc: 0.769 - 0s 20us/step - loss: 0.3656 - auc: 0.7697 - val_loss: 0.3797 - val_auc: 0.7356\n",
      "\n",
      "Epoch 01158: val_loss improved from 0.37981 to 0.37969, saving model to DeepFM.h5\n",
      "Epoch 1159/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3280 - auc: 0.843 - ETA: 0s - loss: 0.3514 - auc: 0.784 - 0s 20us/step - loss: 0.3610 - auc: 0.7821 - val_loss: 0.3796 - val_auc: 0.7348\n",
      "\n",
      "Epoch 01159: val_loss improved from 0.37969 to 0.37958, saving model to DeepFM.h5\n",
      "Epoch 1160/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3974 - auc: 0.736 - ETA: 0s - loss: 0.3547 - auc: 0.801 - 0s 20us/step - loss: 0.3592 - auc: 0.7959 - val_loss: 0.3796 - val_auc: 0.7356\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.37958\n",
      "Epoch 1161/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3710 - auc: 0.827 - ETA: 0s - loss: 0.3499 - auc: 0.794 - 0s 22us/step - loss: 0.3648 - auc: 0.7753 - val_loss: 0.3795 - val_auc: 0.7367\n",
      "\n",
      "Epoch 01161: val_loss improved from 0.37958 to 0.37953, saving model to DeepFM.h5\n",
      "Epoch 1162/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3540 - auc: 0.765 - ETA: 0s - loss: 0.3646 - auc: 0.774 - 0s 21us/step - loss: 0.3639 - auc: 0.7796 - val_loss: 0.3795 - val_auc: 0.7362\n",
      "\n",
      "Epoch 01162: val_loss improved from 0.37953 to 0.37947, saving model to DeepFM.h5\n",
      "Epoch 1163/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3956 - auc: 0.671 - ETA: 0s - loss: 0.3672 - auc: 0.778 - 0s 20us/step - loss: 0.3641 - auc: 0.7771 - val_loss: 0.3794 - val_auc: 0.7366\n",
      "\n",
      "Epoch 01163: val_loss improved from 0.37947 to 0.37942, saving model to DeepFM.h5\n",
      "Epoch 1164/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3974 - auc: 0.754 - ETA: 0s - loss: 0.3593 - auc: 0.778 - 0s 22us/step - loss: 0.3645 - auc: 0.7807 - val_loss: 0.3793 - val_auc: 0.7369\n",
      "\n",
      "Epoch 01164: val_loss improved from 0.37942 to 0.37934, saving model to DeepFM.h5\n",
      "Epoch 1165/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3303 - auc: 0.784 - ETA: 0s - loss: 0.3648 - auc: 0.787 - 0s 22us/step - loss: 0.3599 - auc: 0.7892 - val_loss: 0.3793 - val_auc: 0.7366\n",
      "\n",
      "Epoch 01165: val_loss improved from 0.37934 to 0.37928, saving model to DeepFM.h5\n",
      "Epoch 1166/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4097 - auc: 0.776 - ETA: 0s - loss: 0.3610 - auc: 0.766 - 0s 22us/step - loss: 0.3626 - auc: 0.7808 - val_loss: 0.3791 - val_auc: 0.7374\n",
      "\n",
      "Epoch 01166: val_loss improved from 0.37928 to 0.37911, saving model to DeepFM.h5\n",
      "Epoch 1167/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3961 - auc: 0.738 - ETA: 0s - loss: 0.3609 - auc: 0.770 - 0s 22us/step - loss: 0.3650 - auc: 0.7748 - val_loss: 0.3790 - val_auc: 0.7376\n",
      "\n",
      "Epoch 01167: val_loss improved from 0.37911 to 0.37904, saving model to DeepFM.h5\n",
      "Epoch 1168/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3739 - auc: 0.740 - ETA: 0s - loss: 0.3554 - auc: 0.778 - 0s 22us/step - loss: 0.3633 - auc: 0.7765 - val_loss: 0.3789 - val_auc: 0.7365\n",
      "\n",
      "Epoch 01168: val_loss improved from 0.37904 to 0.37895, saving model to DeepFM.h5\n",
      "Epoch 1169/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3417 - auc: 0.756 - ETA: 0s - loss: 0.3640 - auc: 0.767 - 0s 20us/step - loss: 0.3601 - auc: 0.7847 - val_loss: 0.3790 - val_auc: 0.7371\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.37895\n",
      "Epoch 1170/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4193 - auc: 0.792 - ETA: 0s - loss: 0.3650 - auc: 0.780 - 0s 21us/step - loss: 0.3645 - auc: 0.7748 - val_loss: 0.3789 - val_auc: 0.7373\n",
      "\n",
      "Epoch 01170: val_loss improved from 0.37895 to 0.37888, saving model to DeepFM.h5\n",
      "Epoch 1171/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3636 - auc: 0.650 - ETA: 0s - loss: 0.3575 - auc: 0.785 - 0s 21us/step - loss: 0.3620 - auc: 0.7857 - val_loss: 0.3789 - val_auc: 0.7370\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.37888\n",
      "Epoch 1172/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3442 - auc: 0.834 - ETA: 0s - loss: 0.3556 - auc: 0.791 - 0s 23us/step - loss: 0.3611 - auc: 0.7877 - val_loss: 0.3788 - val_auc: 0.7372\n",
      "\n",
      "Epoch 01172: val_loss improved from 0.37888 to 0.37879, saving model to DeepFM.h5\n",
      "Epoch 1173/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3406 - auc: 0.852 - ETA: 0s - loss: 0.3661 - auc: 0.764 - 0s 21us/step - loss: 0.3635 - auc: 0.7770 - val_loss: 0.3787 - val_auc: 0.7373\n",
      "\n",
      "Epoch 01173: val_loss improved from 0.37879 to 0.37867, saving model to DeepFM.h5\n",
      "Epoch 1174/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3609 - auc: 0.803 - ETA: 0s - loss: 0.3571 - auc: 0.774 - 0s 22us/step - loss: 0.3630 - auc: 0.7814 - val_loss: 0.3787 - val_auc: 0.7374\n",
      "\n",
      "Epoch 01174: val_loss improved from 0.37867 to 0.37865, saving model to DeepFM.h5\n",
      "Epoch 1175/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3751 - auc: 0.814 - ETA: 0s - loss: 0.3559 - auc: 0.790 - 0s 20us/step - loss: 0.3628 - auc: 0.7851 - val_loss: 0.3785 - val_auc: 0.7370\n",
      "\n",
      "Epoch 01175: val_loss improved from 0.37865 to 0.37850, saving model to DeepFM.h5\n",
      "Epoch 1176/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3213 - auc: 0.770 - ETA: 0s - loss: 0.3569 - auc: 0.780 - 0s 21us/step - loss: 0.3596 - auc: 0.7890 - val_loss: 0.3784 - val_auc: 0.7374\n",
      "\n",
      "Epoch 01176: val_loss improved from 0.37850 to 0.37839, saving model to DeepFM.h5\n",
      "Epoch 1177/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3512 - auc: 0.797 - ETA: 0s - loss: 0.3674 - auc: 0.780 - 0s 21us/step - loss: 0.3595 - auc: 0.7866 - val_loss: 0.3785 - val_auc: 0.7366\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.37839\n",
      "Epoch 1178/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3087 - auc: 0.788 - ETA: 0s - loss: 0.3687 - auc: 0.781 - 0s 22us/step - loss: 0.3624 - auc: 0.7839 - val_loss: 0.3784 - val_auc: 0.7372\n",
      "\n",
      "Epoch 01178: val_loss improved from 0.37839 to 0.37837, saving model to DeepFM.h5\n",
      "Epoch 1179/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3423 - auc: 0.820 - ETA: 0s - loss: 0.3589 - auc: 0.799 - 0s 22us/step - loss: 0.3616 - auc: 0.7882 - val_loss: 0.3783 - val_auc: 0.7377\n",
      "\n",
      "Epoch 01179: val_loss improved from 0.37837 to 0.37829, saving model to DeepFM.h5\n",
      "Epoch 1180/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3602 - auc: 0.689 - ETA: 0s - loss: 0.3529 - auc: 0.789 - 0s 22us/step - loss: 0.3598 - auc: 0.7906 - val_loss: 0.3782 - val_auc: 0.7376\n",
      "\n",
      "Epoch 01180: val_loss improved from 0.37829 to 0.37817, saving model to DeepFM.h5\n",
      "Epoch 1181/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3653 - auc: 0.767 - ETA: 0s - loss: 0.3609 - auc: 0.765 - 0s 21us/step - loss: 0.3610 - auc: 0.7842 - val_loss: 0.3781 - val_auc: 0.7379\n",
      "\n",
      "Epoch 01181: val_loss improved from 0.37817 to 0.37807, saving model to DeepFM.h5\n",
      "Epoch 1182/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3987 - auc: 0.785 - ETA: 0s - loss: 0.3620 - auc: 0.787 - 0s 21us/step - loss: 0.3606 - auc: 0.7890 - val_loss: 0.3780 - val_auc: 0.7383\n",
      "\n",
      "Epoch 01182: val_loss improved from 0.37807 to 0.37802, saving model to DeepFM.h5\n",
      "Epoch 1183/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3081 - auc: 0.822 - ETA: 0s - loss: 0.3513 - auc: 0.785 - 0s 22us/step - loss: 0.3572 - auc: 0.7922 - val_loss: 0.3781 - val_auc: 0.7382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01183: val_loss did not improve from 0.37802\n",
      "Epoch 1184/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3793 - auc: 0.764 - ETA: 0s - loss: 0.3652 - auc: 0.775 - 0s 21us/step - loss: 0.3634 - auc: 0.7817 - val_loss: 0.3779 - val_auc: 0.7385\n",
      "\n",
      "Epoch 01184: val_loss improved from 0.37802 to 0.37793, saving model to DeepFM.h5\n",
      "Epoch 1185/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3641 - auc: 0.770 - ETA: 0s - loss: 0.3639 - auc: 0.769 - 0s 21us/step - loss: 0.3621 - auc: 0.7777 - val_loss: 0.3778 - val_auc: 0.7396\n",
      "\n",
      "Epoch 01185: val_loss improved from 0.37793 to 0.37784, saving model to DeepFM.h5\n",
      "Epoch 1186/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3859 - auc: 0.791 - ETA: 0s - loss: 0.3747 - auc: 0.767 - 0s 21us/step - loss: 0.3598 - auc: 0.7865 - val_loss: 0.3777 - val_auc: 0.7390\n",
      "\n",
      "Epoch 01186: val_loss improved from 0.37784 to 0.37774, saving model to DeepFM.h5\n",
      "Epoch 1187/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3946 - auc: 0.818 - ETA: 0s - loss: 0.3510 - auc: 0.790 - 0s 23us/step - loss: 0.3584 - auc: 0.7893 - val_loss: 0.3778 - val_auc: 0.7387\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.37774\n",
      "Epoch 1188/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3690 - auc: 0.790 - ETA: 0s - loss: 0.3525 - auc: 0.796 - 0s 21us/step - loss: 0.3565 - auc: 0.7938 - val_loss: 0.3777 - val_auc: 0.7395\n",
      "\n",
      "Epoch 01188: val_loss improved from 0.37774 to 0.37767, saving model to DeepFM.h5\n",
      "Epoch 1189/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4211 - auc: 0.722 - ETA: 0s - loss: 0.3601 - auc: 0.780 - 0s 21us/step - loss: 0.3595 - auc: 0.7858 - val_loss: 0.3777 - val_auc: 0.7390\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.37767\n",
      "Epoch 1190/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3652 - auc: 0.783 - ETA: 0s - loss: 0.3688 - auc: 0.781 - 0s 20us/step - loss: 0.3627 - auc: 0.7832 - val_loss: 0.3777 - val_auc: 0.7392\n",
      "\n",
      "Epoch 01190: val_loss improved from 0.37767 to 0.37766, saving model to DeepFM.h5\n",
      "Epoch 1191/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3272 - auc: 0.802 - ETA: 0s - loss: 0.3532 - auc: 0.772 - 0s 21us/step - loss: 0.3630 - auc: 0.7776 - val_loss: 0.3774 - val_auc: 0.7400\n",
      "\n",
      "Epoch 01191: val_loss improved from 0.37766 to 0.37738, saving model to DeepFM.h5\n",
      "Epoch 1192/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3653 - auc: 0.745 - ETA: 0s - loss: 0.3622 - auc: 0.779 - 0s 23us/step - loss: 0.3606 - auc: 0.7813 - val_loss: 0.3773 - val_auc: 0.7404\n",
      "\n",
      "Epoch 01192: val_loss improved from 0.37738 to 0.37735, saving model to DeepFM.h5\n",
      "Epoch 1193/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3784 - auc: 0.851 - ETA: 0s - loss: 0.3629 - auc: 0.791 - 0s 22us/step - loss: 0.3576 - auc: 0.7919 - val_loss: 0.3774 - val_auc: 0.7397\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.37735\n",
      "Epoch 1194/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3758 - auc: 0.790 - ETA: 0s - loss: 0.3621 - auc: 0.784 - 0s 21us/step - loss: 0.3578 - auc: 0.7885 - val_loss: 0.3775 - val_auc: 0.7398\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 0.37735\n",
      "Epoch 1195/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3627 - auc: 0.788 - ETA: 0s - loss: 0.3522 - auc: 0.780 - 0s 21us/step - loss: 0.3603 - auc: 0.7837 - val_loss: 0.3773 - val_auc: 0.7405\n",
      "\n",
      "Epoch 01195: val_loss improved from 0.37735 to 0.37730, saving model to DeepFM.h5\n",
      "Epoch 1196/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3719 - auc: 0.808 - ETA: 0s - loss: 0.3690 - auc: 0.779 - 0s 19us/step - loss: 0.3609 - auc: 0.7840 - val_loss: 0.3772 - val_auc: 0.7402\n",
      "\n",
      "Epoch 01196: val_loss improved from 0.37730 to 0.37717, saving model to DeepFM.h5\n",
      "Epoch 1197/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3588 - auc: 0.662 - ETA: 0s - loss: 0.3679 - auc: 0.772 - 0s 22us/step - loss: 0.3607 - auc: 0.7827 - val_loss: 0.3772 - val_auc: 0.7411\n",
      "\n",
      "Epoch 01197: val_loss improved from 0.37717 to 0.37716, saving model to DeepFM.h5\n",
      "Epoch 1198/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3553 - auc: 0.749 - ETA: 0s - loss: 0.3602 - auc: 0.784 - 0s 20us/step - loss: 0.3593 - auc: 0.7894 - val_loss: 0.3770 - val_auc: 0.7405\n",
      "\n",
      "Epoch 01198: val_loss improved from 0.37716 to 0.37703, saving model to DeepFM.h5\n",
      "Epoch 1199/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3850 - auc: 0.816 - ETA: 0s - loss: 0.3552 - auc: 0.797 - 0s 21us/step - loss: 0.3567 - auc: 0.7948 - val_loss: 0.3771 - val_auc: 0.7413\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.37703\n",
      "Epoch 1200/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4227 - auc: 0.787 - ETA: 0s - loss: 0.3672 - auc: 0.790 - 0s 21us/step - loss: 0.3582 - auc: 0.7891 - val_loss: 0.3770 - val_auc: 0.7412\n",
      "\n",
      "Epoch 01200: val_loss improved from 0.37703 to 0.37700, saving model to DeepFM.h5\n",
      "Epoch 1201/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3760 - auc: 0.803 - ETA: 0s - loss: 0.3601 - auc: 0.790 - 0s 24us/step - loss: 0.3569 - auc: 0.7902 - val_loss: 0.3768 - val_auc: 0.7396\n",
      "\n",
      "Epoch 01201: val_loss improved from 0.37700 to 0.37682, saving model to DeepFM.h5\n",
      "Epoch 1202/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3279 - auc: 0.831 - ETA: 0s - loss: 0.3464 - auc: 0.800 - 0s 20us/step - loss: 0.3598 - auc: 0.7891 - val_loss: 0.3767 - val_auc: 0.7407\n",
      "\n",
      "Epoch 01202: val_loss improved from 0.37682 to 0.37671, saving model to DeepFM.h5\n",
      "Epoch 1203/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4061 - auc: 0.723 - ETA: 0s - loss: 0.3554 - auc: 0.793 - 0s 22us/step - loss: 0.3571 - auc: 0.7859 - val_loss: 0.3769 - val_auc: 0.7401\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.37671\n",
      "Epoch 1204/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3387 - auc: 0.809 - ETA: 0s - loss: 0.3542 - auc: 0.796 - 0s 22us/step - loss: 0.3600 - auc: 0.7890 - val_loss: 0.3768 - val_auc: 0.7396\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.37671\n",
      "Epoch 1205/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3051 - auc: 0.778 - ETA: 0s - loss: 0.3697 - auc: 0.783 - 0s 23us/step - loss: 0.3600 - auc: 0.7882 - val_loss: 0.3766 - val_auc: 0.7412\n",
      "\n",
      "Epoch 01205: val_loss improved from 0.37671 to 0.37662, saving model to DeepFM.h5\n",
      "Epoch 1206/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3174 - auc: 0.797 - ETA: 0s - loss: 0.3486 - auc: 0.785 - 0s 24us/step - loss: 0.3559 - auc: 0.7963 - val_loss: 0.3766 - val_auc: 0.7408\n",
      "\n",
      "Epoch 01206: val_loss improved from 0.37662 to 0.37660, saving model to DeepFM.h5\n",
      "Epoch 1207/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3565 - auc: 0.783 - ETA: 0s - loss: 0.3503 - auc: 0.790 - 0s 21us/step - loss: 0.3557 - auc: 0.7963 - val_loss: 0.3766 - val_auc: 0.7394\n",
      "\n",
      "Epoch 01207: val_loss improved from 0.37660 to 0.37660, saving model to DeepFM.h5\n",
      "Epoch 1208/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3524 - auc: 0.763 - ETA: 0s - loss: 0.3625 - auc: 0.784 - 0s 22us/step - loss: 0.3607 - auc: 0.7859 - val_loss: 0.3766 - val_auc: 0.7402\n",
      "\n",
      "Epoch 01208: val_loss improved from 0.37660 to 0.37656, saving model to DeepFM.h5\n",
      "Epoch 1209/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3503 - auc: 0.779 - ETA: 0s - loss: 0.3472 - auc: 0.802 - 0s 22us/step - loss: 0.3581 - auc: 0.7879 - val_loss: 0.3765 - val_auc: 0.7404\n",
      "\n",
      "Epoch 01209: val_loss improved from 0.37656 to 0.37652, saving model to DeepFM.h5\n",
      "Epoch 1210/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3478 - auc: 0.757 - ETA: 0s - loss: 0.3623 - auc: 0.778 - 0s 21us/step - loss: 0.3604 - auc: 0.7806 - val_loss: 0.3765 - val_auc: 0.7401\n",
      "\n",
      "Epoch 01210: val_loss improved from 0.37652 to 0.37649, saving model to DeepFM.h5\n",
      "Epoch 1211/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3745 - auc: 0.793 - ETA: 0s - loss: 0.3534 - auc: 0.787 - 0s 24us/step - loss: 0.3563 - auc: 0.7947 - val_loss: 0.3764 - val_auc: 0.7408\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01211: val_loss improved from 0.37649 to 0.37639, saving model to DeepFM.h5\n",
      "Epoch 1212/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4006 - auc: 0.804 - ETA: 0s - loss: 0.3567 - auc: 0.794 - 0s 20us/step - loss: 0.3517 - auc: 0.8007 - val_loss: 0.3764 - val_auc: 0.7412\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.37639\n",
      "Epoch 1213/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3932 - auc: 0.774 - ETA: 0s - loss: 0.3534 - auc: 0.810 - 0s 21us/step - loss: 0.3515 - auc: 0.8085 - val_loss: 0.3764 - val_auc: 0.7419\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.37639\n",
      "Epoch 1214/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3294 - auc: 0.814 - ETA: 0s - loss: 0.3620 - auc: 0.787 - 0s 22us/step - loss: 0.3585 - auc: 0.7873 - val_loss: 0.3764 - val_auc: 0.7422\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.37639\n",
      "Epoch 1215/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3563 - auc: 0.801 - ETA: 0s - loss: 0.3535 - auc: 0.813 - 0s 22us/step - loss: 0.3549 - auc: 0.7973 - val_loss: 0.3763 - val_auc: 0.7426\n",
      "\n",
      "Epoch 01215: val_loss improved from 0.37639 to 0.37634, saving model to DeepFM.h5\n",
      "Epoch 1216/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4235 - auc: 0.741 - ETA: 0s - loss: 0.3499 - auc: 0.797 - 0s 21us/step - loss: 0.3567 - auc: 0.7875 - val_loss: 0.3763 - val_auc: 0.7420\n",
      "\n",
      "Epoch 01216: val_loss improved from 0.37634 to 0.37634, saving model to DeepFM.h5\n",
      "Epoch 1217/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4049 - auc: 0.770 - ETA: 0s - loss: 0.3572 - auc: 0.795 - 0s 23us/step - loss: 0.3535 - auc: 0.7962 - val_loss: 0.3762 - val_auc: 0.7428\n",
      "\n",
      "Epoch 01217: val_loss improved from 0.37634 to 0.37622, saving model to DeepFM.h5\n",
      "Epoch 1218/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4217 - auc: 0.748 - ETA: 0s - loss: 0.3603 - auc: 0.791 - 0s 21us/step - loss: 0.3574 - auc: 0.7890 - val_loss: 0.3763 - val_auc: 0.7418\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.37622\n",
      "Epoch 1219/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4199 - auc: 0.785 - ETA: 0s - loss: 0.3493 - auc: 0.814 - 0s 22us/step - loss: 0.3509 - auc: 0.8055 - val_loss: 0.3763 - val_auc: 0.7422\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 0.37622\n",
      "Epoch 1220/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3303 - auc: 0.829 - ETA: 0s - loss: 0.3487 - auc: 0.783 - 0s 21us/step - loss: 0.3571 - auc: 0.7861 - val_loss: 0.3760 - val_auc: 0.7430\n",
      "\n",
      "Epoch 01220: val_loss improved from 0.37622 to 0.37598, saving model to DeepFM.h5\n",
      "Epoch 1221/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3676 - auc: 0.812 - ETA: 0s - loss: 0.3699 - auc: 0.783 - 0s 21us/step - loss: 0.3572 - auc: 0.7927 - val_loss: 0.3761 - val_auc: 0.7420\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 0.37598\n",
      "Epoch 1222/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3072 - auc: 0.861 - ETA: 0s - loss: 0.3538 - auc: 0.798 - 0s 20us/step - loss: 0.3549 - auc: 0.7935 - val_loss: 0.3760 - val_auc: 0.7420\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.37598\n",
      "Epoch 1223/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3374 - auc: 0.804 - ETA: 0s - loss: 0.3496 - auc: 0.794 - 0s 21us/step - loss: 0.3552 - auc: 0.7909 - val_loss: 0.3759 - val_auc: 0.7431\n",
      "\n",
      "Epoch 01223: val_loss improved from 0.37598 to 0.37594, saving model to DeepFM.h5\n",
      "Epoch 1224/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3689 - auc: 0.798 - ETA: 0s - loss: 0.3532 - auc: 0.797 - 0s 21us/step - loss: 0.3538 - auc: 0.7983 - val_loss: 0.3758 - val_auc: 0.7436\n",
      "\n",
      "Epoch 01224: val_loss improved from 0.37594 to 0.37583, saving model to DeepFM.h5\n",
      "Epoch 1225/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3913 - auc: 0.724 - ETA: 0s - loss: 0.3618 - auc: 0.774 - 0s 21us/step - loss: 0.3634 - auc: 0.7763 - val_loss: 0.3758 - val_auc: 0.7434\n",
      "\n",
      "Epoch 01225: val_loss improved from 0.37583 to 0.37581, saving model to DeepFM.h5\n",
      "Epoch 1226/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3707 - auc: 0.750 - ETA: 0s - loss: 0.3441 - auc: 0.796 - 0s 22us/step - loss: 0.3566 - auc: 0.7917 - val_loss: 0.3756 - val_auc: 0.7431\n",
      "\n",
      "Epoch 01226: val_loss improved from 0.37581 to 0.37556, saving model to DeepFM.h5\n",
      "Epoch 1227/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3897 - auc: 0.749 - ETA: 0s - loss: 0.3583 - auc: 0.786 - 0s 22us/step - loss: 0.3531 - auc: 0.7994 - val_loss: 0.3756 - val_auc: 0.7436\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.37556\n",
      "Epoch 1228/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3564 - auc: 0.822 - ETA: 0s - loss: 0.3551 - auc: 0.793 - 0s 20us/step - loss: 0.3565 - auc: 0.7907 - val_loss: 0.3756 - val_auc: 0.7430\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.37556\n",
      "Epoch 1229/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3359 - auc: 0.792 - ETA: 0s - loss: 0.3617 - auc: 0.785 - 0s 21us/step - loss: 0.3586 - auc: 0.7859 - val_loss: 0.3755 - val_auc: 0.7433\n",
      "\n",
      "Epoch 01229: val_loss improved from 0.37556 to 0.37551, saving model to DeepFM.h5\n",
      "Epoch 1230/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3103 - auc: 0.831 - ETA: 0s - loss: 0.3514 - auc: 0.788 - 0s 23us/step - loss: 0.3560 - auc: 0.7909 - val_loss: 0.3755 - val_auc: 0.7439\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.37551\n",
      "Epoch 1231/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3248 - auc: 0.824 - ETA: 0s - loss: 0.3587 - auc: 0.789 - 0s 20us/step - loss: 0.3543 - auc: 0.7972 - val_loss: 0.3756 - val_auc: 0.7429\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.37551\n",
      "Epoch 1232/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3313 - auc: 0.827 - ETA: 0s - loss: 0.3564 - auc: 0.801 - 0s 22us/step - loss: 0.3548 - auc: 0.7952 - val_loss: 0.3758 - val_auc: 0.7439\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.37551\n",
      "Epoch 1233/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3071 - auc: 0.855 - ETA: 0s - loss: 0.3603 - auc: 0.789 - 0s 22us/step - loss: 0.3557 - auc: 0.7910 - val_loss: 0.3757 - val_auc: 0.7440\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.37551\n",
      "Epoch 1234/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4087 - auc: 0.767 - ETA: 0s - loss: 0.3543 - auc: 0.792 - 0s 22us/step - loss: 0.3581 - auc: 0.7860 - val_loss: 0.3755 - val_auc: 0.7434\n",
      "\n",
      "Epoch 01234: val_loss improved from 0.37551 to 0.37548, saving model to DeepFM.h5\n",
      "Epoch 1235/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3824 - auc: 0.807 - ETA: 0s - loss: 0.3475 - auc: 0.803 - 0s 23us/step - loss: 0.3513 - auc: 0.8057 - val_loss: 0.3753 - val_auc: 0.7425\n",
      "\n",
      "Epoch 01235: val_loss improved from 0.37548 to 0.37530, saving model to DeepFM.h5\n",
      "Epoch 1236/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3317 - auc: 0.853 - ETA: 0s - loss: 0.3549 - auc: 0.809 - 0s 22us/step - loss: 0.3545 - auc: 0.8000 - val_loss: 0.3755 - val_auc: 0.7447\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.37530\n",
      "Epoch 1237/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3837 - auc: 0.776 - ETA: 0s - loss: 0.3579 - auc: 0.787 - 0s 20us/step - loss: 0.3585 - auc: 0.7869 - val_loss: 0.3751 - val_auc: 0.7434\n",
      "\n",
      "Epoch 01237: val_loss improved from 0.37530 to 0.37507, saving model to DeepFM.h5\n",
      "Epoch 1238/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3263 - auc: 0.795 - ETA: 0s - loss: 0.3523 - auc: 0.808 - 0s 21us/step - loss: 0.3522 - auc: 0.7986 - val_loss: 0.3752 - val_auc: 0.7433\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.37507\n",
      "Epoch 1239/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2968 - auc: 0.844 - ETA: 0s - loss: 0.3466 - auc: 0.785 - 0s 22us/step - loss: 0.3535 - auc: 0.7974 - val_loss: 0.3749 - val_auc: 0.7436\n",
      "\n",
      "Epoch 01239: val_loss improved from 0.37507 to 0.37489, saving model to DeepFM.h5\n",
      "Epoch 1240/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3602 - auc: 0.832 - ETA: 0s - loss: 0.3458 - auc: 0.810 - 0s 22us/step - loss: 0.3524 - auc: 0.8002 - val_loss: 0.3751 - val_auc: 0.7443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01240: val_loss did not improve from 0.37489\n",
      "Epoch 1241/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4281 - auc: 0.769 - ETA: 0s - loss: 0.3519 - auc: 0.804 - 0s 21us/step - loss: 0.3547 - auc: 0.7972 - val_loss: 0.3751 - val_auc: 0.7448\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.37489\n",
      "Epoch 1242/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4052 - auc: 0.793 - ETA: 0s - loss: 0.3458 - auc: 0.801 - 0s 21us/step - loss: 0.3522 - auc: 0.8022 - val_loss: 0.3750 - val_auc: 0.7445\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 0.37489\n",
      "Epoch 1243/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3673 - auc: 0.820 - ETA: 0s - loss: 0.3570 - auc: 0.804 - 0s 22us/step - loss: 0.3537 - auc: 0.7992 - val_loss: 0.3751 - val_auc: 0.7441\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.37489\n",
      "Epoch 1244/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3243 - auc: 0.796 - ETA: 0s - loss: 0.3595 - auc: 0.791 - 0s 23us/step - loss: 0.3560 - auc: 0.7921 - val_loss: 0.3749 - val_auc: 0.7452\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.37489\n",
      "Epoch 1245/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2997 - auc: 0.845 - ETA: 0s - loss: 0.3563 - auc: 0.794 - 0s 21us/step - loss: 0.3550 - auc: 0.7934 - val_loss: 0.3747 - val_auc: 0.7441\n",
      "\n",
      "Epoch 01245: val_loss improved from 0.37489 to 0.37469, saving model to DeepFM.h5\n",
      "Epoch 1246/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3426 - auc: 0.792 - ETA: 0s - loss: 0.3410 - auc: 0.819 - 0s 21us/step - loss: 0.3498 - auc: 0.8066 - val_loss: 0.3746 - val_auc: 0.7437\n",
      "\n",
      "Epoch 01246: val_loss improved from 0.37469 to 0.37465, saving model to DeepFM.h5\n",
      "Epoch 1247/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3917 - auc: 0.757 - ETA: 0s - loss: 0.3535 - auc: 0.792 - 0s 22us/step - loss: 0.3545 - auc: 0.7946 - val_loss: 0.3745 - val_auc: 0.7443\n",
      "\n",
      "Epoch 01247: val_loss improved from 0.37465 to 0.37455, saving model to DeepFM.h5\n",
      "Epoch 1248/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3444 - auc: 0.794 - ETA: 0s - loss: 0.3526 - auc: 0.789 - 0s 22us/step - loss: 0.3554 - auc: 0.7868 - val_loss: 0.3746 - val_auc: 0.7453\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.37455\n",
      "Epoch 1249/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3049 - auc: 0.776 - ETA: 0s - loss: 0.3502 - auc: 0.801 - 0s 21us/step - loss: 0.3510 - auc: 0.8001 - val_loss: 0.3747 - val_auc: 0.7453\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.37455\n",
      "Epoch 1250/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3293 - auc: 0.864 - ETA: 0s - loss: 0.3431 - auc: 0.814 - 0s 22us/step - loss: 0.3485 - auc: 0.8106 - val_loss: 0.3746 - val_auc: 0.7443\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.37455\n",
      "Epoch 1251/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3369 - auc: 0.805 - ETA: 0s - loss: 0.3668 - auc: 0.783 - 0s 22us/step - loss: 0.3544 - auc: 0.7896 - val_loss: 0.3747 - val_auc: 0.7455\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.37455\n",
      "Epoch 1252/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3929 - auc: 0.741 - ETA: 0s - loss: 0.3533 - auc: 0.793 - 0s 22us/step - loss: 0.3549 - auc: 0.7945 - val_loss: 0.3747 - val_auc: 0.7457\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.37455\n",
      "Epoch 1253/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3575 - auc: 0.786 - ETA: 0s - loss: 0.3626 - auc: 0.792 - 0s 22us/step - loss: 0.3523 - auc: 0.7957 - val_loss: 0.3748 - val_auc: 0.7457\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.37455\n",
      "Epoch 1254/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4181 - auc: 0.783 - ETA: 0s - loss: 0.3515 - auc: 0.797 - 0s 22us/step - loss: 0.3505 - auc: 0.8044 - val_loss: 0.3746 - val_auc: 0.7452\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.37455\n",
      "Epoch 1255/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3351 - auc: 0.822 - ETA: 0s - loss: 0.3446 - auc: 0.807 - 0s 22us/step - loss: 0.3552 - auc: 0.7938 - val_loss: 0.3744 - val_auc: 0.7455\n",
      "\n",
      "Epoch 01255: val_loss improved from 0.37455 to 0.37436, saving model to DeepFM.h5\n",
      "Epoch 1256/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3515 - auc: 0.821 - ETA: 0s - loss: 0.3478 - auc: 0.790 - 0s 22us/step - loss: 0.3555 - auc: 0.7888 - val_loss: 0.3743 - val_auc: 0.7450\n",
      "\n",
      "Epoch 01256: val_loss improved from 0.37436 to 0.37425, saving model to DeepFM.h5\n",
      "Epoch 1257/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3698 - auc: 0.852 - ETA: 0s - loss: 0.3493 - auc: 0.799 - 0s 22us/step - loss: 0.3525 - auc: 0.7985 - val_loss: 0.3743 - val_auc: 0.7451\n",
      "\n",
      "Epoch 01257: val_loss improved from 0.37425 to 0.37425, saving model to DeepFM.h5\n",
      "Epoch 1258/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3469 - auc: 0.802 - ETA: 0s - loss: 0.3459 - auc: 0.806 - 0s 22us/step - loss: 0.3504 - auc: 0.8011 - val_loss: 0.3743 - val_auc: 0.7449\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.37425\n",
      "Epoch 1259/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3001 - auc: 0.899 - ETA: 0s - loss: 0.3542 - auc: 0.811 - 0s 23us/step - loss: 0.3461 - auc: 0.8164 - val_loss: 0.3744 - val_auc: 0.7448\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.37425\n",
      "Epoch 1260/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3740 - auc: 0.763 - ETA: 0s - loss: 0.3467 - auc: 0.796 - 0s 21us/step - loss: 0.3529 - auc: 0.7979 - val_loss: 0.3745 - val_auc: 0.7459\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.37425\n",
      "Epoch 1261/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3588 - auc: 0.685 - ETA: 0s - loss: 0.3525 - auc: 0.793 - 0s 21us/step - loss: 0.3574 - auc: 0.7892 - val_loss: 0.3742 - val_auc: 0.7450\n",
      "\n",
      "Epoch 01261: val_loss improved from 0.37425 to 0.37421, saving model to DeepFM.h5\n",
      "Epoch 1262/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3804 - auc: 0.754 - ETA: 0s - loss: 0.3615 - auc: 0.798 - 0s 21us/step - loss: 0.3512 - auc: 0.8014 - val_loss: 0.3743 - val_auc: 0.7450\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.37421\n",
      "Epoch 1263/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3247 - auc: 0.804 - ETA: 0s - loss: 0.3445 - auc: 0.807 - 0s 22us/step - loss: 0.3519 - auc: 0.7999 - val_loss: 0.3743 - val_auc: 0.7457\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.37421\n",
      "Epoch 1264/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3403 - auc: 0.830 - ETA: 0s - loss: 0.3518 - auc: 0.808 - 0s 22us/step - loss: 0.3488 - auc: 0.8053 - val_loss: 0.3743 - val_auc: 0.7452\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.37421\n",
      "Epoch 1265/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3116 - auc: 0.858 - ETA: 0s - loss: 0.3411 - auc: 0.812 - 0s 22us/step - loss: 0.3517 - auc: 0.8015 - val_loss: 0.3740 - val_auc: 0.7454\n",
      "\n",
      "Epoch 01265: val_loss improved from 0.37421 to 0.37402, saving model to DeepFM.h5\n",
      "Epoch 1266/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3869 - auc: 0.831 - ETA: 0s - loss: 0.3526 - auc: 0.803 - 0s 21us/step - loss: 0.3530 - auc: 0.7983 - val_loss: 0.3742 - val_auc: 0.7456\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.37402\n",
      "Epoch 1267/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3989 - auc: 0.800 - ETA: 0s - loss: 0.3497 - auc: 0.795 - 0s 20us/step - loss: 0.3520 - auc: 0.8011 - val_loss: 0.3742 - val_auc: 0.7460\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.37402\n",
      "Epoch 1268/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3343 - auc: 0.812 - ETA: 0s - loss: 0.3507 - auc: 0.801 - 0s 21us/step - loss: 0.3499 - auc: 0.8069 - val_loss: 0.3741 - val_auc: 0.7453\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.37402\n",
      "Epoch 1269/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3121 - auc: 0.818 - ETA: 0s - loss: 0.3552 - auc: 0.803 - 0s 21us/step - loss: 0.3534 - auc: 0.7930 - val_loss: 0.3741 - val_auc: 0.7452\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.37402\n",
      "Epoch 1270/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3268 - auc: 0.837 - ETA: 0s - loss: 0.3491 - auc: 0.805 - 0s 21us/step - loss: 0.3487 - auc: 0.8047 - val_loss: 0.3740 - val_auc: 0.7454\n",
      "\n",
      "Epoch 01270: val_loss improved from 0.37402 to 0.37401, saving model to DeepFM.h5\n",
      "Epoch 1271/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3498 - auc: 0.769 - ETA: 0s - loss: 0.3463 - auc: 0.811 - 0s 21us/step - loss: 0.3502 - auc: 0.8036 - val_loss: 0.3739 - val_auc: 0.7461\n",
      "\n",
      "Epoch 01271: val_loss improved from 0.37401 to 0.37385, saving model to DeepFM.h5\n",
      "Epoch 1272/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3203 - auc: 0.782 - ETA: 0s - loss: 0.3540 - auc: 0.813 - 0s 21us/step - loss: 0.3447 - auc: 0.8129 - val_loss: 0.3741 - val_auc: 0.7471\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.37385\n",
      "Epoch 1273/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3751 - auc: 0.740 - ETA: 0s - loss: 0.3418 - auc: 0.818 - 0s 20us/step - loss: 0.3465 - auc: 0.8140 - val_loss: 0.3740 - val_auc: 0.7462\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.37385\n",
      "Epoch 1274/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3102 - auc: 0.862 - ETA: 0s - loss: 0.3569 - auc: 0.780 - 0s 21us/step - loss: 0.3556 - auc: 0.7902 - val_loss: 0.3741 - val_auc: 0.7466\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.37385\n",
      "Epoch 1275/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3136 - auc: 0.822 - ETA: 0s - loss: 0.3541 - auc: 0.804 - 0s 22us/step - loss: 0.3493 - auc: 0.8047 - val_loss: 0.3741 - val_auc: 0.7459\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.37385\n",
      "Epoch 1276/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3267 - auc: 0.814 - ETA: 0s - loss: 0.3513 - auc: 0.806 - 0s 22us/step - loss: 0.3466 - auc: 0.8085 - val_loss: 0.3740 - val_auc: 0.7464\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.37385\n",
      "Epoch 1277/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3564 - auc: 0.802 - ETA: 0s - loss: 0.3557 - auc: 0.798 - 0s 22us/step - loss: 0.3534 - auc: 0.7998 - val_loss: 0.3738 - val_auc: 0.7466\n",
      "\n",
      "Epoch 01277: val_loss improved from 0.37385 to 0.37379, saving model to DeepFM.h5\n",
      "Epoch 1278/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3779 - auc: 0.800 - ETA: 0s - loss: 0.3474 - auc: 0.821 - 0s 22us/step - loss: 0.3450 - auc: 0.8146 - val_loss: 0.3739 - val_auc: 0.7461\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.37379\n",
      "Epoch 1279/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3335 - auc: 0.857 - ETA: 0s - loss: 0.3542 - auc: 0.791 - 0s 21us/step - loss: 0.3495 - auc: 0.7994 - val_loss: 0.3737 - val_auc: 0.7464\n",
      "\n",
      "Epoch 01279: val_loss improved from 0.37379 to 0.37374, saving model to DeepFM.h5\n",
      "Epoch 1280/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3414 - auc: 0.770 - ETA: 0s - loss: 0.3437 - auc: 0.805 - 0s 21us/step - loss: 0.3493 - auc: 0.8020 - val_loss: 0.3736 - val_auc: 0.7468\n",
      "\n",
      "Epoch 01280: val_loss improved from 0.37374 to 0.37361, saving model to DeepFM.h5\n",
      "Epoch 1281/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4312 - auc: 0.758 - ETA: 0s - loss: 0.3625 - auc: 0.791 - 0s 21us/step - loss: 0.3550 - auc: 0.7936 - val_loss: 0.3735 - val_auc: 0.7468\n",
      "\n",
      "Epoch 01281: val_loss improved from 0.37361 to 0.37353, saving model to DeepFM.h5\n",
      "Epoch 1282/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3089 - auc: 0.827 - ETA: 0s - loss: 0.3429 - auc: 0.804 - 0s 22us/step - loss: 0.3491 - auc: 0.8044 - val_loss: 0.3733 - val_auc: 0.7472\n",
      "\n",
      "Epoch 01282: val_loss improved from 0.37353 to 0.37329, saving model to DeepFM.h5\n",
      "Epoch 1283/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3737 - auc: 0.784 - ETA: 0s - loss: 0.3539 - auc: 0.792 - 0s 21us/step - loss: 0.3548 - auc: 0.7975 - val_loss: 0.3732 - val_auc: 0.7471\n",
      "\n",
      "Epoch 01283: val_loss improved from 0.37329 to 0.37320, saving model to DeepFM.h5\n",
      "Epoch 1284/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3505 - auc: 0.825 - ETA: 0s - loss: 0.3569 - auc: 0.796 - 0s 20us/step - loss: 0.3491 - auc: 0.8042 - val_loss: 0.3735 - val_auc: 0.7451\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.37320\n",
      "Epoch 1285/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3364 - auc: 0.842 - ETA: 0s - loss: 0.3475 - auc: 0.817 - 0s 22us/step - loss: 0.3463 - auc: 0.8098 - val_loss: 0.3735 - val_auc: 0.7457\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.37320\n",
      "Epoch 1286/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3477 - auc: 0.795 - ETA: 0s - loss: 0.3518 - auc: 0.814 - 0s 21us/step - loss: 0.3464 - auc: 0.8112 - val_loss: 0.3736 - val_auc: 0.7465\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.37320\n",
      "Epoch 1287/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2867 - auc: 0.867 - ETA: 0s - loss: 0.3463 - auc: 0.808 - 0s 21us/step - loss: 0.3460 - auc: 0.8104 - val_loss: 0.3734 - val_auc: 0.7468\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 0.37320\n",
      "Epoch 1288/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3416 - auc: 0.861 - ETA: 0s - loss: 0.3402 - auc: 0.809 - 0s 21us/step - loss: 0.3460 - auc: 0.8116 - val_loss: 0.3733 - val_auc: 0.7472\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 0.37320\n",
      "Epoch 1289/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3391 - auc: 0.839 - ETA: 0s - loss: 0.3450 - auc: 0.803 - 0s 21us/step - loss: 0.3492 - auc: 0.8028 - val_loss: 0.3735 - val_auc: 0.7464\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.37320\n",
      "Epoch 1290/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3434 - auc: 0.843 - ETA: 0s - loss: 0.3590 - auc: 0.789 - 0s 21us/step - loss: 0.3524 - auc: 0.7933 - val_loss: 0.3735 - val_auc: 0.7464\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.37320\n",
      "Epoch 1291/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3433 - auc: 0.842 - ETA: 0s - loss: 0.3482 - auc: 0.809 - 0s 20us/step - loss: 0.3488 - auc: 0.8069 - val_loss: 0.3735 - val_auc: 0.7460\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.37320\n",
      "Epoch 1292/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3517 - auc: 0.814 - ETA: 0s - loss: 0.3445 - auc: 0.809 - 0s 20us/step - loss: 0.3478 - auc: 0.8062 - val_loss: 0.3734 - val_auc: 0.7461\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.37320\n",
      "Epoch 1293/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3014 - auc: 0.853 - ETA: 0s - loss: 0.3535 - auc: 0.802 - 0s 20us/step - loss: 0.3470 - auc: 0.8045 - val_loss: 0.3736 - val_auc: 0.7471\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.37320\n",
      "Epoch 1294/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3272 - auc: 0.795 - ETA: 0s - loss: 0.3351 - auc: 0.815 - 0s 20us/step - loss: 0.3451 - auc: 0.8111 - val_loss: 0.3735 - val_auc: 0.7464\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.37320\n",
      "Epoch 1295/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4249 - auc: 0.712 - ETA: 0s - loss: 0.3462 - auc: 0.809 - 0s 21us/step - loss: 0.3461 - auc: 0.8115 - val_loss: 0.3734 - val_auc: 0.7466\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.37320\n",
      "Epoch 1296/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3988 - auc: 0.810 - ETA: 0s - loss: 0.3462 - auc: 0.826 - 0s 21us/step - loss: 0.3410 - auc: 0.8199 - val_loss: 0.3736 - val_auc: 0.7475\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.37320\n",
      "Epoch 1297/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3541 - auc: 0.724 - ETA: 0s - loss: 0.3480 - auc: 0.805 - 0s 21us/step - loss: 0.3462 - auc: 0.8078 - val_loss: 0.3735 - val_auc: 0.7480\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.37320\n",
      "Epoch 1298/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3102 - auc: 0.806 - ETA: 0s - loss: 0.3426 - auc: 0.820 - 0s 20us/step - loss: 0.3424 - auc: 0.8202 - val_loss: 0.3733 - val_auc: 0.7475\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.37320\n",
      "Epoch 1299/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3732 - auc: 0.782 - ETA: 0s - loss: 0.3478 - auc: 0.805 - 0s 21us/step - loss: 0.3459 - auc: 0.8068 - val_loss: 0.3733 - val_auc: 0.7477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01299: val_loss did not improve from 0.37320\n",
      "Epoch 1300/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3302 - auc: 0.798 - ETA: 0s - loss: 0.3398 - auc: 0.810 - 0s 21us/step - loss: 0.3500 - auc: 0.8008 - val_loss: 0.3731 - val_auc: 0.7466\n",
      "\n",
      "Epoch 01300: val_loss improved from 0.37320 to 0.37306, saving model to DeepFM.h5\n",
      "Epoch 1301/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3453 - auc: 0.817 - ETA: 0s - loss: 0.3412 - auc: 0.815 - 0s 20us/step - loss: 0.3456 - auc: 0.8110 - val_loss: 0.3732 - val_auc: 0.7478\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.37306\n",
      "Epoch 1302/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3248 - auc: 0.832 - ETA: 0s - loss: 0.3429 - auc: 0.812 - 0s 21us/step - loss: 0.3431 - auc: 0.8149 - val_loss: 0.3731 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.37306\n",
      "Epoch 1303/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3307 - auc: 0.866 - ETA: 0s - loss: 0.3468 - auc: 0.817 - 0s 20us/step - loss: 0.3409 - auc: 0.8194 - val_loss: 0.3733 - val_auc: 0.7487\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.37306\n",
      "Epoch 1304/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3223 - auc: 0.880 - ETA: 0s - loss: 0.3497 - auc: 0.810 - 0s 21us/step - loss: 0.3430 - auc: 0.8129 - val_loss: 0.3730 - val_auc: 0.7480\n",
      "\n",
      "Epoch 01304: val_loss improved from 0.37306 to 0.37303, saving model to DeepFM.h5\n",
      "Epoch 1305/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3882 - auc: 0.764 - ETA: 0s - loss: 0.3438 - auc: 0.812 - 0s 19us/step - loss: 0.3473 - auc: 0.8082 - val_loss: 0.3730 - val_auc: 0.7477\n",
      "\n",
      "Epoch 01305: val_loss improved from 0.37303 to 0.37297, saving model to DeepFM.h5\n",
      "Epoch 1306/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3466 - auc: 0.807 - ETA: 0s - loss: 0.3475 - auc: 0.811 - 0s 21us/step - loss: 0.3451 - auc: 0.8098 - val_loss: 0.3730 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.37297\n",
      "Epoch 1307/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3307 - auc: 0.796 - ETA: 0s - loss: 0.3503 - auc: 0.809 - 0s 20us/step - loss: 0.3455 - auc: 0.8084 - val_loss: 0.3731 - val_auc: 0.7488\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.37297\n",
      "Epoch 1308/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3696 - auc: 0.830 - ETA: 0s - loss: 0.3409 - auc: 0.814 - 0s 21us/step - loss: 0.3459 - auc: 0.8094 - val_loss: 0.3730 - val_auc: 0.7481\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.37297\n",
      "Epoch 1309/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3867 - auc: 0.762 - ETA: 0s - loss: 0.3426 - auc: 0.811 - 0s 20us/step - loss: 0.3441 - auc: 0.8138 - val_loss: 0.3730 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01309: val_loss improved from 0.37297 to 0.37295, saving model to DeepFM.h5\n",
      "Epoch 1310/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3607 - auc: 0.856 - ETA: 0s - loss: 0.3543 - auc: 0.811 - 0s 19us/step - loss: 0.3445 - auc: 0.8134 - val_loss: 0.3732 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.37295\n",
      "Epoch 1311/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3763 - auc: 0.806 - ETA: 0s - loss: 0.3544 - auc: 0.800 - 0s 21us/step - loss: 0.3461 - auc: 0.8092 - val_loss: 0.3732 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 0.37295\n",
      "Epoch 1312/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3603 - auc: 0.766 - ETA: 0s - loss: 0.3457 - auc: 0.805 - 0s 21us/step - loss: 0.3445 - auc: 0.8096 - val_loss: 0.3729 - val_auc: 0.7477\n",
      "\n",
      "Epoch 01312: val_loss improved from 0.37295 to 0.37286, saving model to DeepFM.h5\n",
      "Epoch 1313/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3422 - auc: 0.857 - ETA: 0s - loss: 0.3474 - auc: 0.819 - 0s 21us/step - loss: 0.3428 - auc: 0.8161 - val_loss: 0.3730 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.37286\n",
      "Epoch 1314/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2776 - auc: 0.875 - ETA: 0s - loss: 0.3330 - auc: 0.819 - 0s 21us/step - loss: 0.3429 - auc: 0.8107 - val_loss: 0.3731 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.37286\n",
      "Epoch 1315/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2816 - auc: 0.816 - ETA: 0s - loss: 0.3436 - auc: 0.808 - 0s 23us/step - loss: 0.3490 - auc: 0.8002 - val_loss: 0.3728 - val_auc: 0.7484\n",
      "\n",
      "Epoch 01315: val_loss improved from 0.37286 to 0.37284, saving model to DeepFM.h5\n",
      "Epoch 1316/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2846 - auc: 0.842 - ETA: 0s - loss: 0.3473 - auc: 0.799 - 0s 21us/step - loss: 0.3488 - auc: 0.8038 - val_loss: 0.3728 - val_auc: 0.7478\n",
      "\n",
      "Epoch 01316: val_loss improved from 0.37284 to 0.37277, saving model to DeepFM.h5\n",
      "Epoch 1317/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2985 - auc: 0.824 - ETA: 0s - loss: 0.3419 - auc: 0.805 - 0s 22us/step - loss: 0.3471 - auc: 0.8048 - val_loss: 0.3727 - val_auc: 0.7476\n",
      "\n",
      "Epoch 01317: val_loss improved from 0.37277 to 0.37269, saving model to DeepFM.h5\n",
      "Epoch 1318/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3124 - auc: 0.860 - ETA: 0s - loss: 0.3328 - auc: 0.827 - 0s 21us/step - loss: 0.3416 - auc: 0.8191 - val_loss: 0.3727 - val_auc: 0.7477\n",
      "\n",
      "Epoch 01318: val_loss improved from 0.37269 to 0.37268, saving model to DeepFM.h5\n",
      "Epoch 1319/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3867 - auc: 0.812 - ETA: 0s - loss: 0.3457 - auc: 0.819 - 0s 22us/step - loss: 0.3395 - auc: 0.8209 - val_loss: 0.3731 - val_auc: 0.7490\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 0.37268\n",
      "Epoch 1320/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3426 - auc: 0.853 - ETA: 0s - loss: 0.3415 - auc: 0.813 - 0s 21us/step - loss: 0.3435 - auc: 0.8153 - val_loss: 0.3728 - val_auc: 0.7491\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 0.37268\n",
      "Epoch 1321/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3622 - auc: 0.855 - ETA: 0s - loss: 0.3437 - auc: 0.816 - 0s 19us/step - loss: 0.3451 - auc: 0.8131 - val_loss: 0.3728 - val_auc: 0.7491\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.37268\n",
      "Epoch 1322/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3503 - auc: 0.830 - ETA: 0s - loss: 0.3501 - auc: 0.809 - 0s 22us/step - loss: 0.3468 - auc: 0.8096 - val_loss: 0.3727 - val_auc: 0.7488\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.37268\n",
      "Epoch 1323/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3167 - auc: 0.829 - ETA: 0s - loss: 0.3503 - auc: 0.807 - 0s 24us/step - loss: 0.3447 - auc: 0.8099 - val_loss: 0.3728 - val_auc: 0.7493\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.37268\n",
      "Epoch 1324/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3996 - auc: 0.793 - ETA: 0s - loss: 0.3470 - auc: 0.808 - 0s 24us/step - loss: 0.3435 - auc: 0.8161 - val_loss: 0.3726 - val_auc: 0.7492\n",
      "\n",
      "Epoch 01324: val_loss improved from 0.37268 to 0.37265, saving model to DeepFM.h5\n",
      "Epoch 1325/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3514 - auc: 0.800 - ETA: 0s - loss: 0.3396 - auc: 0.829 - 0s 24us/step - loss: 0.3442 - auc: 0.8189 - val_loss: 0.3726 - val_auc: 0.7488\n",
      "\n",
      "Epoch 01325: val_loss improved from 0.37265 to 0.37260, saving model to DeepFM.h5\n",
      "Epoch 1326/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3666 - auc: 0.864 - ETA: 0s - loss: 0.3560 - auc: 0.803 - 0s 22us/step - loss: 0.3504 - auc: 0.7978 - val_loss: 0.3728 - val_auc: 0.7499\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 0.37260\n",
      "Epoch 1327/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3202 - auc: 0.816 - ETA: 0s - loss: 0.3480 - auc: 0.807 - 0s 23us/step - loss: 0.3409 - auc: 0.8175 - val_loss: 0.3728 - val_auc: 0.7486\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 0.37260\n",
      "Epoch 1328/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3872 - auc: 0.823 - ETA: 0s - loss: 0.3348 - auc: 0.836 - 0s 23us/step - loss: 0.3377 - auc: 0.8268 - val_loss: 0.3727 - val_auc: 0.7500\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.37260\n",
      "Epoch 1329/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3640 - auc: 0.777 - ETA: 0s - loss: 0.3396 - auc: 0.818 - 0s 23us/step - loss: 0.3424 - auc: 0.8139 - val_loss: 0.3728 - val_auc: 0.7474\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 0.37260\n",
      "Epoch 1330/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3585 - auc: 0.783 - ETA: 0s - loss: 0.3543 - auc: 0.809 - 0s 23us/step - loss: 0.3436 - auc: 0.8133 - val_loss: 0.3728 - val_auc: 0.7480\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.37260\n",
      "Epoch 1331/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2888 - auc: 0.861 - ETA: 0s - loss: 0.3496 - auc: 0.803 - 0s 23us/step - loss: 0.3463 - auc: 0.8086 - val_loss: 0.3725 - val_auc: 0.7492\n",
      "\n",
      "Epoch 01331: val_loss improved from 0.37260 to 0.37245, saving model to DeepFM.h5\n",
      "Epoch 1332/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2827 - auc: 0.860 - ETA: 0s - loss: 0.3345 - auc: 0.822 - 0s 20us/step - loss: 0.3392 - auc: 0.8147 - val_loss: 0.3727 - val_auc: 0.7480\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.37245\n",
      "Epoch 1333/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3809 - auc: 0.813 - ETA: 0s - loss: 0.3517 - auc: 0.808 - 0s 23us/step - loss: 0.3428 - auc: 0.8134 - val_loss: 0.3728 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.37245\n",
      "Epoch 1334/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3813 - auc: 0.744 - ETA: 0s - loss: 0.3359 - auc: 0.817 - 0s 20us/step - loss: 0.3438 - auc: 0.8123 - val_loss: 0.3726 - val_auc: 0.7497\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 0.37245\n",
      "Epoch 1335/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3390 - auc: 0.813 - ETA: 0s - loss: 0.3402 - auc: 0.808 - 0s 20us/step - loss: 0.3453 - auc: 0.8059 - val_loss: 0.3724 - val_auc: 0.7488\n",
      "\n",
      "Epoch 01335: val_loss improved from 0.37245 to 0.37241, saving model to DeepFM.h5\n",
      "Epoch 1336/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3488 - auc: 0.865 - ETA: 0s - loss: 0.3456 - auc: 0.814 - 0s 23us/step - loss: 0.3414 - auc: 0.8203 - val_loss: 0.3726 - val_auc: 0.7486\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 0.37241\n",
      "Epoch 1337/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3473 - auc: 0.837 - ETA: 0s - loss: 0.3436 - auc: 0.814 - 0s 20us/step - loss: 0.3405 - auc: 0.8163 - val_loss: 0.3726 - val_auc: 0.7484\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.37241\n",
      "Epoch 1338/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3467 - auc: 0.777 - ETA: 0s - loss: 0.3546 - auc: 0.811 - 0s 22us/step - loss: 0.3461 - auc: 0.8087 - val_loss: 0.3727 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 0.37241\n",
      "Epoch 1339/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3032 - auc: 0.872 - ETA: 0s - loss: 0.3428 - auc: 0.827 - 0s 21us/step - loss: 0.3426 - auc: 0.8190 - val_loss: 0.3725 - val_auc: 0.7485\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 0.37241\n",
      "Epoch 1340/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3446 - auc: 0.810 - ETA: 0s - loss: 0.3484 - auc: 0.821 - 0s 21us/step - loss: 0.3423 - auc: 0.8134 - val_loss: 0.3728 - val_auc: 0.7481\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 0.37241\n",
      "Epoch 1341/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3667 - auc: 0.784 - ETA: 0s - loss: 0.3552 - auc: 0.811 - 0s 26us/step - loss: 0.3437 - auc: 0.8114 - val_loss: 0.3726 - val_auc: 0.7483\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 0.37241\n",
      "Epoch 1342/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3554 - auc: 0.807 - ETA: 0s - loss: 0.3394 - auc: 0.824 - 0s 26us/step - loss: 0.3416 - auc: 0.8175 - val_loss: 0.3727 - val_auc: 0.7482\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 0.37241\n",
      "Epoch 1343/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3275 - auc: 0.849 - ETA: 0s - loss: 0.3484 - auc: 0.819 - 0s 23us/step - loss: 0.3434 - auc: 0.8152 - val_loss: 0.3727 - val_auc: 0.7482\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.37241\n",
      "Epoch 1344/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3191 - auc: 0.852 - ETA: 0s - loss: 0.3417 - auc: 0.819 - 0s 21us/step - loss: 0.3366 - auc: 0.8232 - val_loss: 0.3728 - val_auc: 0.7484\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 0.37241\n",
      "Epoch 1345/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3541 - auc: 0.797 - ETA: 0s - loss: 0.3401 - auc: 0.815 - 0s 22us/step - loss: 0.3445 - auc: 0.8172 - val_loss: 0.3724 - val_auc: 0.7487\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 0.37241\n",
      "Epoch 1346/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3104 - auc: 0.897 - ETA: 0s - loss: 0.3393 - auc: 0.824 - 0s 21us/step - loss: 0.3457 - auc: 0.8085 - val_loss: 0.3725 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.37241\n",
      "Epoch 1347/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2920 - auc: 0.806 - ETA: 0s - loss: 0.3359 - auc: 0.820 - 0s 20us/step - loss: 0.3368 - auc: 0.8260 - val_loss: 0.3725 - val_auc: 0.7482\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 0.37241\n",
      "Epoch 1348/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4168 - auc: 0.783 - ETA: 0s - loss: 0.3408 - auc: 0.818 - 0s 22us/step - loss: 0.3413 - auc: 0.8180 - val_loss: 0.3725 - val_auc: 0.7478\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.37241\n",
      "Epoch 1349/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3992 - auc: 0.791 - ETA: 0s - loss: 0.3469 - auc: 0.814 - 0s 21us/step - loss: 0.3447 - auc: 0.8128 - val_loss: 0.3722 - val_auc: 0.7494\n",
      "\n",
      "Epoch 01349: val_loss improved from 0.37241 to 0.37223, saving model to DeepFM.h5\n",
      "Epoch 1350/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3489 - auc: 0.804 - ETA: 0s - loss: 0.3460 - auc: 0.810 - 0s 20us/step - loss: 0.3413 - auc: 0.8162 - val_loss: 0.3723 - val_auc: 0.7478\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 0.37223\n",
      "Epoch 1351/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.4148 - auc: 0.744 - ETA: 0s - loss: 0.3397 - auc: 0.820 - 0s 21us/step - loss: 0.3377 - auc: 0.8247 - val_loss: 0.3727 - val_auc: 0.7494\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 0.37223\n",
      "Epoch 1352/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3392 - auc: 0.825 - ETA: 0s - loss: 0.3311 - auc: 0.814 - 0s 27us/step - loss: 0.3415 - auc: 0.8146 - val_loss: 0.3722 - val_auc: 0.7479\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 0.37223\n",
      "Epoch 1353/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2983 - auc: 0.819 - ETA: 0s - loss: 0.3349 - auc: 0.826 - 0s 26us/step - loss: 0.3399 - auc: 0.8193 - val_loss: 0.3721 - val_auc: 0.7496\n",
      "\n",
      "Epoch 01353: val_loss improved from 0.37223 to 0.37209, saving model to DeepFM.h5\n",
      "Epoch 1354/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3024 - auc: 0.791 - ETA: 0s - loss: 0.3423 - auc: 0.808 - 0s 25us/step - loss: 0.3435 - auc: 0.8153 - val_loss: 0.3722 - val_auc: 0.7485\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.37209\n",
      "Epoch 1355/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3421 - auc: 0.846 - ETA: 0s - loss: 0.3548 - auc: 0.813 - 0s 25us/step - loss: 0.3427 - auc: 0.8120 - val_loss: 0.3723 - val_auc: 0.7484\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 0.37209\n",
      "Epoch 1356/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3039 - auc: 0.841 - ETA: 0s - loss: 0.3412 - auc: 0.817 - 0s 23us/step - loss: 0.3437 - auc: 0.8150 - val_loss: 0.3724 - val_auc: 0.7490\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 0.37209\n",
      "Epoch 1357/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2940 - auc: 0.851 - ETA: 0s - loss: 0.3403 - auc: 0.828 - 0s 24us/step - loss: 0.3364 - auc: 0.8252 - val_loss: 0.3726 - val_auc: 0.7496\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 0.37209\n",
      "Epoch 1358/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3740 - auc: 0.793 - ETA: 0s - loss: 0.3425 - auc: 0.807 - 0s 25us/step - loss: 0.3411 - auc: 0.8148 - val_loss: 0.3725 - val_auc: 0.7494\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 0.37209\n",
      "Epoch 1359/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3247 - auc: 0.822 - ETA: 0s - loss: 0.3405 - auc: 0.817 - 0s 24us/step - loss: 0.3319 - auc: 0.8319 - val_loss: 0.3725 - val_auc: 0.7498\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 0.37209\n",
      "Epoch 1360/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3301 - auc: 0.800 - ETA: 0s - loss: 0.3476 - auc: 0.804 - 0s 21us/step - loss: 0.3450 - auc: 0.8101 - val_loss: 0.3725 - val_auc: 0.7497\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 0.37209\n",
      "Epoch 1361/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3321 - auc: 0.835 - ETA: 0s - loss: 0.3424 - auc: 0.819 - 0s 20us/step - loss: 0.3386 - auc: 0.8175 - val_loss: 0.3726 - val_auc: 0.7499\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 0.37209\n",
      "Epoch 1362/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2860 - auc: 0.839 - ETA: 0s - loss: 0.3483 - auc: 0.825 - 0s 20us/step - loss: 0.3400 - auc: 0.8210 - val_loss: 0.3728 - val_auc: 0.7492\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 0.37209\n",
      "Epoch 1363/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3404 - auc: 0.813 - ETA: 0s - loss: 0.3422 - auc: 0.816 - 0s 20us/step - loss: 0.3382 - auc: 0.8217 - val_loss: 0.3725 - val_auc: 0.7501\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.37209\n",
      "Epoch 1364/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2926 - auc: 0.876 - ETA: 0s - loss: 0.3453 - auc: 0.816 - 0s 20us/step - loss: 0.3424 - auc: 0.8164 - val_loss: 0.3725 - val_auc: 0.7500\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.37209\n",
      "Epoch 1365/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3756 - auc: 0.781 - ETA: 0s - loss: 0.3362 - auc: 0.828 - 0s 21us/step - loss: 0.3375 - auc: 0.8214 - val_loss: 0.3726 - val_auc: 0.7504\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 0.37209\n",
      "Epoch 1366/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3140 - auc: 0.843 - ETA: 0s - loss: 0.3271 - auc: 0.831 - 0s 20us/step - loss: 0.3371 - auc: 0.8227 - val_loss: 0.3725 - val_auc: 0.7502\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 0.37209\n",
      "Epoch 1367/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2984 - auc: 0.812 - ETA: 0s - loss: 0.3395 - auc: 0.815 - 0s 20us/step - loss: 0.3403 - auc: 0.8177 - val_loss: 0.3725 - val_auc: 0.7495\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 0.37209\n",
      "Epoch 1368/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3155 - auc: 0.789 - ETA: 0s - loss: 0.3414 - auc: 0.810 - 0s 21us/step - loss: 0.3428 - auc: 0.8131 - val_loss: 0.3723 - val_auc: 0.7498\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.37209\n",
      "Epoch 1369/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2977 - auc: 0.824 - ETA: 0s - loss: 0.3365 - auc: 0.806 - 0s 21us/step - loss: 0.3432 - auc: 0.8111 - val_loss: 0.3723 - val_auc: 0.7496\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.37209\n",
      "Epoch 1370/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2983 - auc: 0.876 - ETA: 0s - loss: 0.3269 - auc: 0.832 - 0s 23us/step - loss: 0.3351 - auc: 0.8252 - val_loss: 0.3723 - val_auc: 0.7500\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 0.37209\n",
      "Epoch 1371/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3476 - auc: 0.849 - ETA: 0s - loss: 0.3491 - auc: 0.811 - 0s 20us/step - loss: 0.3432 - auc: 0.8152 - val_loss: 0.3723 - val_auc: 0.7506\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.37209\n",
      "Epoch 1372/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.3516 - auc: 0.812 - ETA: 0s - loss: 0.3367 - auc: 0.812 - 0s 21us/step - loss: 0.3423 - auc: 0.8124 - val_loss: 0.3722 - val_auc: 0.7505\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 0.37209\n",
      "Epoch 1373/4000\n",
      "4335/4335 [==============================] - ETA: 0s - loss: 0.2707 - auc: 0.892 - ETA: 0s - loss: 0.3419 - auc: 0.823 - 0s 21us/step - loss: 0.3400 - auc: 0.8199 - val_loss: 0.3723 - val_auc: 0.7494\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 0.37209\n"
     ]
    }
   ],
   "source": [
    "# model2 : DeepFM without matchTags \n",
    "model2 = DeepFM(k=18,deep_dim=48,flag=True)\n",
    "\n",
    "epochs = 4000\n",
    "batch_size = 256\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='DeepFM.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "train_input2 = [train_category[:,i] for i in range(train_category.shape[1])]+[train_continue]\n",
    "hist2 = model2.fit(train_input2,train_label,epochs = epochs, batch_size=batch_size,validation_split=0.15, callbacks=[checkpoint,early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plt_show(hist):\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    #acc_ax.plot(hist.history['auc'], 'b', label='train auc')\n",
    "    acc_ax.plot(hist.history['val_auc'], 'g', label='val auc')\n",
    "\n",
    "    #acc_ax.plot(hist.history['accuracy'], 'dodgerblue', label='train acc')\n",
    "    #acc_ax.plot(hist.history['val_accuracy'], 'springgreen', label='val acc')\n",
    "    acc_ax.set_ylabel('accuracy')\n",
    "    acc_ax.legend(loc='upper right')\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_show(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_show(hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100/5100 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 36us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31128618880814196, 0.8698626756668091]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_input,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100/5100 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 35us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3199136156428094, 0.8634520769119263]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(train_input2,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk(test):\n",
    "    cnt=0\n",
    "    for i in range(test.shape[0]):\n",
    "        if x_test.iloc[i].applied == test.iloc[i].pred:\n",
    "            cnt+=1\n",
    "    print(cnt/test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8711111111111111\n"
     ]
    }
   ],
   "source": [
    "#첫번째 모델 평가\n",
    "\n",
    "test_input = [test_category[:,i] for i in range(test_category.shape[1])]+[test_matchTags]+[test_continue]\n",
    "test_pred = model.predict(test_input)\n",
    "pred = pd.DataFrame(test_pred)\n",
    "pred.columns=['pred']\n",
    "pred['pred'] = pred['pred'].apply(lambda x: 1 if x>=.5 else 0)\n",
    "\n",
    "chk(pred)\n",
    "pred.to_csv('DeepFM_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8655555555555555\n"
     ]
    }
   ],
   "source": [
    "#두번째 모델 평가\n",
    "\n",
    "test_input2 = [test_category[:,i] for i in range(test_category.shape[1])]+[test_continue]\n",
    "test_pred2 = model2.predict(test_input2)\n",
    "pred = pd.DataFrame(test_pred2)\n",
    "pred.columns=['pred']\n",
    "pred['pred'] = pred['pred'].apply(lambda x: 1 if x>=.5 else 0)\n",
    "cnt = 0\n",
    "\n",
    "chk(pred)\n",
    "pred.to_csv('DeepFM_pred2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.vis_utils.pydot = pydot\n",
    "plot_model(model,to_file='model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
